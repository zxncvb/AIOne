# å¤§æ¨¡å‹å²—ä½é¢è¯•å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [åŸºç¡€æ¦‚å¿µ](#1-åŸºç¡€æ¦‚å¿µ)
2. [æ ¸å¿ƒæŠ€æœ¯](#2-æ ¸å¿ƒæŠ€æœ¯)
3. [å·¥ç¨‹å®è·µ](#3-å·¥ç¨‹å®è·µ)
4. [Agentç³»ç»Ÿ](#4-agentç³»ç»Ÿ)
5. [é¢è¯•æŠ€å·§](#5-é¢è¯•æŠ€å·§)

---

## 1. åŸºç¡€æ¦‚å¿µ

### 1.1 å¤§æ¨¡å‹æ ¸å¿ƒæ¦‚å¿µ

**Q: ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼Ÿæ ¸å¿ƒåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ**

**A:** å¤§è¯­è¨€æ¨¡å‹æ˜¯åŸºäºTransformeræ¶æ„çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé€šè¿‡å¤§è§„æ¨¡æ–‡æœ¬é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€è¡¨ç¤ºã€‚

**æ ¸å¿ƒåŸç†ï¼š**
- è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰
- ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰
- å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰
- å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰

### 1.2 Transformeræ¶æ„

**Q: è¯¦ç»†è§£é‡ŠTransformeræ¶æ„çš„å·¥ä½œåŸç†ï¼Ÿ**

**A:** Transformerç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œæ ¸å¿ƒæ˜¯æ³¨æ„åŠ›æœºåˆ¶ï¼š

```python
def attention(Q, K, V, mask=None):
    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(Q.size(-1))
    
    # åº”ç”¨mask
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    
    # Softmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)
    
    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, V)
    return output
```

### 1.3 é¢„è®­ç»ƒä¸å¾®è°ƒ

**Q: é¢„è®­ç»ƒå’Œå¾®è°ƒçš„åŒºåˆ«ï¼Ÿ**

**A:** 
- **é¢„è®­ç»ƒ**ï¼šåœ¨å¤§è§„æ¨¡æ— æ ‡ç­¾æ•°æ®ä¸Šè®­ç»ƒï¼Œå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º
- **å¾®è°ƒ**ï¼šåœ¨ç‰¹å®šä»»åŠ¡æ•°æ®ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼Œé€‚åº”å…·ä½“åº”ç”¨åœºæ™¯

**å¾®è°ƒæ–¹æ³•ï¼š**
- å…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰
- å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰ï¼šLoRAã€Adapterã€Prefix Tuning
- æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰

---

## 2. æ ¸å¿ƒæŠ€æœ¯

### 2.1 å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰

**Q: è§£é‡ŠRLHFçš„è®­ç»ƒæµç¨‹ï¼Ÿ**

**A:** RLHFåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼š

1. **ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰**ï¼šåœ¨é«˜è´¨é‡æ•°æ®ä¸Šå¾®è°ƒ
2. **å¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰**ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹
3. **å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰**ï¼šä½¿ç”¨PPOç®—æ³•ä¼˜åŒ–

```python
class RLHFTrainer:
    def __init__(self, policy_model, reward_model):
        self.policy_model = policy_model
        self.reward_model = reward_model
    
    def train_step(self, prompts, responses, rewards):
        # è®¡ç®—ç­–ç•¥æŸå¤±
        policy_loss = self.compute_policy_loss(prompts, responses, rewards)
        
        # è®¡ç®—KLæ•£åº¦æŸå¤±
        kl_loss = self.compute_kl_loss(prompts, responses)
        
        # æ€»æŸå¤±
        total_loss = policy_loss + self.kl_coef * kl_loss
        return total_loss
```

### 2.2 æ³¨æ„åŠ›æœºåˆ¶

**Q: æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹ï¼Ÿ**

**A:** æ³¨æ„åŠ›æœºåˆ¶é€šè¿‡Queryã€Keyã€Valueä¸‰ä¸ªçŸ©é˜µè®¡ç®—ï¼š

1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š`scores = Q * K^T / sqrt(d_k)`
2. åº”ç”¨Softmaxå½’ä¸€åŒ–
3. åŠ æƒæ±‚å’Œï¼š`output = attention_weights * V`

### 2.3 çŸ¥è¯†å¢å¼º

**Q: å¦‚ä½•å°†å¤–éƒ¨çŸ¥è¯†æ³¨å…¥å¤§æ¨¡å‹ï¼Ÿ**

**A:** 
- **æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰**ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œå¢å¼ºç”Ÿæˆ
- **çŸ¥è¯†å›¾è°±èåˆ**ï¼šå°†çŸ¥è¯†å›¾è°±ä¿¡æ¯æ³¨å…¥æ¨¡å‹
- **å·¥å…·è°ƒç”¨**ï¼šé€šè¿‡APIè°ƒç”¨å¤–éƒ¨å·¥å…·

```python
class RAGSystem:
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever
    
    def answer(self, question):
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.retriever.retrieve(question)
        
        # æ„å»ºå¢å¼ºæç¤º
        prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n{docs}\n\né—®é¢˜ï¼š{question}"
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.llm.generate(prompt)
        return answer
```

---

## 3. å·¥ç¨‹å®è·µ

### 3.1 æ¨¡å‹éƒ¨ç½²

**Q: å¤§æ¨¡å‹çš„éƒ¨ç½²ç­–ç•¥ï¼Ÿ**

**A:** 
- **æ¨¡å‹é‡åŒ–**ï¼šINT8ã€INT4é‡åŒ–å‡å°‘æ¨¡å‹å¤§å°
- **æ¨¡å‹å‰ªæ**ï¼šç§»é™¤ä¸é‡è¦çš„æƒé‡
- **æ¨¡å‹è’¸é¦**ï¼šè®­ç»ƒå°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹
- **åˆ†å¸ƒå¼éƒ¨ç½²**ï¼šæ¨¡å‹å¹¶è¡Œã€æ•°æ®å¹¶è¡Œ

```python
# åŠ¨æ€é‡åŒ–ç¤ºä¾‹
from torch.quantization import quantize_dynamic

quantized_model = quantize_dynamic(
    model,
    {nn.Linear, nn.LSTM},
    dtype=torch.qint8
)
```

### 3.2 æ€§èƒ½ä¼˜åŒ–

**Q: å¤§æ¨¡å‹çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•ï¼Ÿ**

**A:** 
- **æ¨ç†ä¼˜åŒ–**ï¼šKVç¼“å­˜ã€æ³¨æ„åŠ›ä¼˜åŒ–
- **å†…å­˜ä¼˜åŒ–**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ¿€æ´»é‡è®¡ç®—
- **è®¡ç®—ä¼˜åŒ–**ï¼šæ··åˆç²¾åº¦è®­ç»ƒã€ç®—å­èåˆ

### 3.3 ç›‘æ§ä¸è°ƒè¯•

**Q: å¦‚ä½•ç›‘æ§å¤§æ¨¡å‹ç³»ç»Ÿï¼Ÿ**

**A:** 
- **æ€§èƒ½æŒ‡æ ‡**ï¼šå»¶è¿Ÿã€ååé‡ã€å‡†ç¡®ç‡
- **èµ„æºç›‘æ§**ï¼šGPUä½¿ç”¨ç‡ã€å†…å­˜ä½¿ç”¨
- **è´¨é‡ç›‘æ§**ï¼šè¾“å‡ºè´¨é‡ã€å¼‚å¸¸æ£€æµ‹

---

## 4. Agentç³»ç»Ÿ

### 4.1 Agentæ¶æ„

**Q: Agentç³»ç»Ÿçš„æ¶æ„è®¾è®¡ï¼Ÿ**

**A:** Agentç³»ç»Ÿé€šå¸¸åŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š

- **å·¥å…·è°ƒç”¨**ï¼šæ‰§è¡Œå…·ä½“ä»»åŠ¡
- **è®°å¿†ç®¡ç†**ï¼šå­˜å‚¨å’Œæ£€ç´¢ä¿¡æ¯
- **è§„åˆ’å†³ç­–**ï¼šåˆ¶å®šæ‰§è¡Œè®¡åˆ’
- **é€šä¿¡åè°ƒ**ï¼šä¸å…¶ä»–Agentåä½œ

### 4.2 å·¥å…·è°ƒç”¨

**Q: Agentçš„å·¥å…·è°ƒç”¨æœºåˆ¶ï¼Ÿ**

**A:** 
```python
class ToolCallingAgent:
    def __init__(self, llm):
        self.llm = llm
        self.tools = {}
    
    def register_tool(self, name, func, description):
        self.tools[name] = {"function": func, "description": description}
    
    def execute(self, user_input):
        # åˆ†æç”¨æˆ·éœ€æ±‚
        analysis = self.llm.analyze(user_input)
        
        # é€‰æ‹©å·¥å…·
        selected_tools = self.select_tools(analysis)
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        results = []
        for tool_name, params in selected_tools:
            result = self.tools[tool_name]["function"](**params)
            results.append(result)
        
        return self.llm.integrate_results(results)
```

### 4.3 è®°å¿†æ¶æ„

**Q: Agentçš„è®°å¿†æ¶æ„è®¾è®¡ï¼Ÿ**

**A:** 
- **çŸ­æœŸè®°å¿†**ï¼šå¯¹è¯ä¸Šä¸‹æ–‡
- **é•¿æœŸè®°å¿†**ï¼šçŸ¥è¯†åº“ã€ç»éªŒ
- **å·¥ä½œè®°å¿†**ï¼šå½“å‰ä»»åŠ¡çŠ¶æ€

### 4.4 å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

**Q: å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„åä½œæœºåˆ¶ï¼Ÿ**

**A:** 
- **è§’è‰²åˆ†å·¥**ï¼šä¸åŒæ™ºèƒ½ä½“æ‰¿æ‹…ä¸åŒè§’è‰²
- **é€šä¿¡åè®®**ï¼šæ™ºèƒ½ä½“é—´çš„ä¿¡æ¯äº¤æ¢
- **åè°ƒæœºåˆ¶**ï¼šè§£å†³å†²çªå’Œè¾¾æˆå…±è¯†

---

## 5. é¢è¯•æŠ€å·§

### 5.1 æŠ€æœ¯é¢è¯•å‡†å¤‡

**å‡†å¤‡è¦ç‚¹ï¼š**
1. **åŸºç¡€çŸ¥è¯†**ï¼šæ·±åº¦å­¦ä¹ ã€NLPã€å¼ºåŒ–å­¦ä¹ 
2. **å®è·µç»éªŒ**ï¼šé¡¹ç›®ç»éªŒã€ä»£ç èƒ½åŠ›
3. **å‰æ²¿æŠ€æœ¯**ï¼šæœ€æ–°è®ºæ–‡ã€æŠ€æœ¯è¶‹åŠ¿
4. **ç³»ç»Ÿè®¾è®¡**ï¼šæ¶æ„è®¾è®¡ã€å·¥ç¨‹å®è·µ

### 5.2 å¸¸è§é¢è¯•é—®é¢˜

**æŠ€æœ¯é—®é¢˜ï¼š**
- è§£é‡ŠTransformeræ¶æ„
- å®ç°æ³¨æ„åŠ›æœºåˆ¶
- è®¾è®¡å¯¹è¯ç³»ç»Ÿ
- ä¼˜åŒ–æ¨¡å‹æ€§èƒ½

**é¡¹ç›®é—®é¢˜ï¼š**
- æè¿°ä½ çš„é¡¹ç›®ç»å†
- è§£å†³çš„æŠ€æœ¯æŒ‘æˆ˜
- æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ
- å›¢é˜Ÿåä½œç»éªŒ

### 5.3 å›ç­”æŠ€å·§

**STARæ–¹æ³•ï¼š**
- **Situation**ï¼šæƒ…å¢ƒ
- **Task**ï¼šä»»åŠ¡
- **Action**ï¼šè¡ŒåŠ¨
- **Result**ï¼šç»“æœ

**å‡†å¤‡å»ºè®®ï¼š**
1. å¤ä¹ åŸºç¡€çŸ¥è¯†
2. ç»ƒä¹ ç¼–ç¨‹
3. é¡¹ç›®å¤ç›˜
4. æ¨¡æ‹Ÿé¢è¯•

---

## ğŸ“š æ¨èèµ„æº

### ä¹¦ç±
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow
- ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ç»¼è®ºã€‹- Daniel Jurafsky
- ã€Šå¼ºåŒ–å­¦ä¹ ã€‹- Richard S. Sutton

### è®ºæ–‡
- Attention Is All You Need
- BERT: Pre-training of Deep Bidirectional Transformers
- GPT-3: Language Models are Few-Shot Learners

### è¯¾ç¨‹
- CS224n: Natural Language Processing
- CS285: Deep Reinforcement Learning
- Fast.ai: Practical Deep Learning

### å¹³å°
- Hugging Face
- OpenAI API
- LangChain
- Weights & Biases

---

## ğŸ¯ é¢è¯•å‡†å¤‡æ¸…å•

### æŠ€æœ¯å‡†å¤‡
- [ ] æ·±åº¦å­¦ä¹ åŸºç¡€
- [ ] Transformeræ¶æ„
- [ ] é¢„è®­ç»ƒæ¨¡å‹
- [ ] å¼ºåŒ–å­¦ä¹ 
- [ ] å¯¹è¯ç³»ç»Ÿ
- [ ] çŸ¥è¯†å›¾è°±
- [ ] å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

### å·¥ç¨‹å‡†å¤‡
- [ ] æ¨¡å‹éƒ¨ç½²
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] ç³»ç»Ÿè®¾è®¡
- [ ] å·¥å…·å¼€å‘
- [ ] ç›‘æ§è°ƒè¯•

### é¡¹ç›®å‡†å¤‡
- [ ] é¡¹ç›®ç»å†æ€»ç»“
- [ ] ä»£ç ç¤ºä¾‹å‡†å¤‡
- [ ] æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹
- [ ] å›¢é˜Ÿåä½œç»éªŒ

**ç¥æ‚¨é¢è¯•é¡ºåˆ©ï¼** ğŸš€
