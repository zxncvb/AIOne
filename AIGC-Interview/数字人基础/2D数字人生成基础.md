# 目录

## 第一章 可控数字人生成
- [1.2D数字人生成有什么方向?](#1.2D数字人生成有什么方向?)
- [2.如何基于一个图像生成模型扩展到视频?](#2.如何基于一个图像生成模型扩展到视频?)
- [3.人体驱动的方法有哪些?](#3.人体驱动的常用方法有哪些?)
- [4.可控人体生成的目的是什么，如何做到驱动?](#4.可控人体生成的目的是什么，如何做到驱动?)
- [5.如何提升人体驱动生成中脸部的ID相似度?](#5.如何提升人体驱动生成中脸部的ID相似度?)
- [6.Animate-Anyone的模型结构和原理](#6.Animate-Anyone的模型结构和原理)
- [7.ID保持图像生成和换脸的区别](#7.ID保持图像生成和换脸的区别)
- [8.有哪些专注人像生成的预训练模型?](#8.有哪些专注人像生成的预训练模型?)
- [9.介绍一下IP-Adapter](#9.介绍一下IP-Adapter)
- [10.介绍一下InstantID](#10.介绍一下InstantID)
- [11.介绍一下PuLID](#11.介绍一下PuLID)
- [12.介绍一下UniPortrait](#12.介绍一下UniPortrait)
- [13.介绍一下ConsistentID](#13.介绍一下ConsistentID)
- [14.介绍一下PhotoMaker](#14.介绍一下PhotoMaker)
- [15.介绍一下Hallo](#15.介绍一下Hallo)
- [16.介绍一下Hallo2](#16.介绍一下Hallo2)
- [17.介绍一下Hallo3](#17.介绍一下Hallo3)
- [18.介绍一下UniPortrait](#18.介绍一下UniPortrait)
- [19.介绍一下LivePortrait](#19.介绍一下LivePortrait)
- [20.介绍一下ConsisID](#20.介绍一下ConsisID)
- [21.什么是视频虚拟换衣](#21.什么是视频虚拟换衣)
- [22.视频换衣和图像换衣的区别](#22.视频换衣和图像换衣的区别)
- [23.视频虚拟换衣有哪些应用场景](#23.视频虚拟换衣有哪些应用场景)
- [24.视频换衣面临哪些技术挑战？如何解决？](#24.视频换衣面临哪些技术挑战？如何解决？)
- [25.如何解决视频换衣中服饰与动作的匹配冲突](#25.如何解决视频换衣中服饰与动作的匹配冲突)
- [26.如何设计一套成品模特试穿视频生成链路](#26.如何一套成品模特试穿视频生成链路)
- [27.介绍一下SimSwap](#27.介绍一下SimSwap)
- [28.介绍一下HifiFace](#28介绍一下HifiFace)
- [29.介绍一下Face-Adapter](#29.介绍一下Face-Adapter)
- [30.视频换脸有哪些挑战](#30视频换脸有哪些挑战)
- [31.介绍一下ReFace](#31.介绍一下ReFace)
- [32.换脸如何和3DMM人脸重建相结合](#32.换脸如何和3DMM人脸重建相结合)
- [33.介绍一下Phantom](#33.介绍一下Phantom)
- [34.介绍一下FantasyTalking](#34.介绍一下FantasyTalking)
- [35.介绍一下InfiniteYou](#35.介绍一下InfiniteYou)
- [36.介绍一下FlashFace](#36.介绍一下FlashFace)
- [37.介绍一下SkyReels-A1](#37.介绍一下SkyReels-A1)
- [38.介绍一下SkyReels-A2](#38.介绍一下SkyReels-A2)
- [39.介绍一下HiFiVFS](#39.介绍一下HiFiVFS)
- [40.介绍一下VividFace](#40.介绍一下VividFace)
- [41.介绍一下InstantCharacter](#41.介绍一下InstantCharacter)
- [42.介绍一下FuseAnyPart](#42.介绍一下FuseAnyPart)
- [43.介绍一下OmniHuman-1](#43.介绍一下OmniHuman-1)
- [44.介绍一下StableAnimator](#44.介绍一下StableAnimator)
- [45.介绍一下Sonic](#45.介绍一下Sonic)
- [46.如何对音频驱动人脸数据进行预处理](#46.如何对音频驱动人脸数据进行预处理)
- [47.如何将REPA思想应用在唇形对齐中](#47.如何将REPA思想应用在唇形对齐中)
- [48.如何实现精确的人脸条件控制](#48.如何实现精确的人脸条件控制)
- [49.视频人脸交换中的时间一致性如何保障](#49.视频人脸交换中的时间一致性如何保障)
- [50.如何实现高保真身份注入](#50.如何实现高保真身份注入)
- [51.如何解决长视频生成的闪烁问题](#51.如何解决长视频生成的闪烁问题)
- [52.在人脸或人体生成中如何避免训练时条件之间的信息冗余](#52.在人脸或人体生成中如何避免训练时条件之间的信息冗余)
- [53.背景处理如何解决源-目标人脸形状不匹配问题](#53.背景处理如何解决源-目标人脸形状不匹配问题)
- [54.虚拟换衣有哪些技术难点](#54.虚拟换衣有哪些技术难点)
- [55.虚拟换衣目前的技术路线有哪些](#55.虚拟换衣目前的技术路线有哪些)
- [56.Mask-based虚拟换衣方法中掩码引入的形状信息如何处理](#56.Mask-based虚拟换衣方法中掩码引入的形状信息如何处理)
- [57.虚拟换衣如何构造三元组数据](#57.虚拟换衣如何构造三元组数据)
- [58.虚拟换衣如何筛选优质的训练数据](#58.虚拟换衣如何筛选优质的训练数据)
- [59.虚拟换衣常用的数据集有哪些](#59.虚拟换衣常用的数据集有哪些)
- [60.如何通过文本更好的控制衣服尺寸](#60.如何通过文本更好的控制衣服尺寸)
- [61.如果训练数据里没有“卷袖子”的标注，怎么教会模型让生成人像的衣服卷袖子](#61.如果训练数据里没有“卷袖子”的标注，怎么教会模型让生成人像的衣服卷袖子)
- [62.模型在512×384上训练，线上要跑1024×768，如何防止衣服纹理糊掉](#62.模型在512×384上训练，线上要跑1024×768，如何防止衣服纹理糊掉)
- [63.发现生成人像的头发经常盖在新换的衣服上，怎么修复](#63.发现生成人像的头发经常盖在新换的衣服上，怎么修复)
- [64.如果只能用文字描述，想让一件紧身上衣在视觉上变宽松，该如何实现](#64.如果只能用文字描述，想让一件紧身上衣在视觉上变宽松，该如何实现)
- [65.如何评估文本-尺寸的忠实度](#65.如何评估文本-尺寸的忠实度)
- [66.音频驱动多人物视频生成有哪些难点](#66.音频驱动多人物视频生成有哪些难点)
- [67.MultiTalk的整体架构是什么](#67.MultiTalk的整体架构是什么？)
- [68.介绍一下L-RoPE(Label Rotary Position Embedding)](#68.介绍一下L-RoPE(Label Rotary Position Embedding))
- [69.如何实现自适应人物定位生成](#69.如何实现自适应人物定位生成)
- [70.多人音频驱动有哪些音频注入方式](#70.多人音频驱动有哪些音频注入方式)
- [71.音频驱动多人物视频生成有哪些应用场景](#71.音频驱动多人物视频生成有哪些应用场景)
- [72.Wan-S2V模型在数字人生成中如何处理手部动作的生成和优化](#72.Wan-S2V模型在数字人生成中如何处理手部动作的生成和优化)
- [73.Wan-S2V模型在数字人生成中如何解决长视频生成的稳定性问题](#73.Wan-S2V模型在数字人生成中如何解决长视频生成的稳定性问题)
- [74.Wan-S2V模型在音频驱动视频生成领域的主要创新点是什么](#74.Wan-S2V模型在音频驱动视频生成领域的主要创新点是什么)
- [75.Wan-Animate模型的核心思想和它试图解决的主要问题是什么](#75.Wan-Animate模型的核心思想和它试图解决的主要问题是什么)
- [76.Wan-Animate中提到的整体复制是如何实现的](#76.Wan-Animate中提到的整体复制是如何实现的)
- [77.如何设计训练数据和损失函数才能让模型既能处理人体驱动，又能处理人体替换](#77.如何设计训练数据和损失函数才能让模型既能处理人体驱动，又能处理人体替换)
- [78.Wan-Animate中的Face-Adapter是如何实现的](#78.Wan-Animate中的Face-Adapter是如何实现的)
- [79.Wan-Animate中的Body-Adapter是如何实现的](#79.Wan-Animate中的Body-Adapter是如何实现的)
- [80.Wan-Animate如何保证新角色与环境的一致性](#80.Wan-Animate如何保证新角色与环境的一致性)
- [81.Wan-Animate在推理时如何分别主体和环境](#81.Wan-Animate在推理时如何分别主体和环境)
- [82.Wan-Animate在推理时如何处理人体体型差异较大的问题](#82.Wan-Animate在推理时如何处理人体体型差异较大的问题)
- [83.什么是Copy-Paste现象](#83.什么是Copy-Paste现象)
- [84.如何收集多人ID数据集](#84.如何收集多人ID数据集)
- [85.HunyuanVideo-Avatar如何均衡视频的动态性和一致性](#85.HunyuanVideo-Avatar如何均衡视频的动态性和一致性)
- [86.HunyuanVideo-Avatar如何加强音频情感与面部表情对齐](#86.HunyuanVideo-Avatar如何加强音频情感与面部表情对齐)
- [87.HunyuanVideo-Avatar如何加强音频与特定ID绑定能力](#87.HunyuanVideo-Avatar如何加强音频与特定ID绑定能力)
- [88.HunyuanVideo-Avatar如何处理长视频生成](#88.HunyuanVideo-Avatar如何处理长视频生成)

## 第一章 可控数字人生成

<h2 id="1.2D数字人有什么方向">1.2D数字人有什么方向?</h2>

目前，2D数字人生成的方向包括：

1. 可控人体生成
- ‌**人体驱动** 
- **虚拟换衣**

2. 可控人脸生成
- **人脸属性编辑**
- **换脸**
- **目标人脸引导的人脸驱动生成**
- **音频引导的人脸驱动生成**

3. ID保持的人体图像/视频生成
- **视频写真**

<h2 id="2.如何基于一个图像人体或人脸生成模型扩展到视频?">2.如何基于一个图像生成模型扩展到视频?</h2>

  基于GAN的方案构造视频数据集抽帧进行训练即可，无需添加额外的帧间一致性模块，测试时就可以达到不错的帧间稳定性。由于扩散模型方案建模的多样性强，如果直接逐帧进行推理会导致帧间一致性较差，目前常用的解决方式是采用SD1.5或者SDXL基底模型的基础上，第一阶段使用人脸或人体数据集将底模调整到对应的domain，第二阶段插入一个类似AnimateDiff中提出的Motion Module提升帧间一致性。

<h2 id="3.人体驱动的方法有哪些?">3.人体驱动的方法有哪些?</h2>

|                                                              |     T2V model                 |     Pose Condition                   |     Injection Type                                     |     Others                |
|--------------------------------------------------------------|-------------------------------|--------------------------------------|--------------------------------------------------------|---------------------------|
|     Magic Animate                                            |     AnimateDiff               |     DensePose                        |     ReferenceNet+ControlNet                            |     w/o. alignment        |
|     Animate Anyone                                           |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     Moore-Animate Anyone (AA unofficial   implementation)    |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/o. alignment        |
|     MusePose                                                 |     AnimateDiff               |     DWPose                           |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     Champ                                                    |     AnimateDiff               |     DensePose/DWPose/Normal/Depth    |     ReferenceNet+Pose Encoder+CLIP                     |     w/. alignment (2d)    |
|     UniAnimate                                               |     AnimateDiff               |     DWPose                           |     Pose Encoder+CLIP                                  |     w/. alignment (2d)    |
|     ViVidPose                                                |     Stable Video Diffusion    |     DWPose/SMPLX-Shape               |     ReferenceNet+Pose   Encoder+CLIP+Face   Encoder    |     w/. alignment (3d)    |


<h2 id="4.可控人体生成的目的是什么，如何做到驱动?">4.可控人体生成的目的是什么，如何做到驱动?</h2>
    
  不管是文本生成、图像生成、视频生成，如果没有具备可控性，AI作为一个工具，本身能够带来的效能的提升就非常的有限。可控人体生成的目的就是希望通过输入一段目标的姿态序列和一张参考人像图片，能够保持参考人像的背景，人物特征的同时，生成其按照目标序列进行运动的人像视频。


<h2 id="5.如何提升人体驱动生成中脸部的ID相似度?">5.如何提升人体驱动生成中脸部的ID相似度?</h2>

  人脸生成，是 AI 生成视频中最难的场景之一。首先是因为人类对人脸本身就很敏感。一个细微的肌肉表情，就能被解读出不同的含义。人们自拍经常要拍几十张相似的照片，才能挑到合适的角度。因此涉及到人脸的一些形变，很容易就会引起我们的注意。在早期的人体驱动工作中，研究者们并没有过多的采用一些额外的模块约束参考人像和生成人像的脸部ID一致性，仅采用ReferenceNet和CLIP Image Encoder来提取了参考人像信息。在此基础上，有几种方式可以提升脸部ID一致性：
  1. 在训练过程中，计算生成人脸和参考人脸的ID Similarity，并加入ID Loss，
  2. 对于参考人像的人脸区域，使用人脸识别网络提取对应的ID信息，在主干网络中注入

<h2 id="6.Animate-Anyone的模型结构和原理">6.Animate-Anyone的模型结构和原理</h2>

  AnimateAnyone是一种能够将角色图像转换为所需姿势序列控制的动画视频的方法，继承了Stable Diffusion模型的网络设计和预训练权重，并在UNet中插入Motion Module以适应多帧输入。为了解决保持外观一致性的挑战，引入了ReferenceNet，专门设计为UNet结构来捕获参考图像的空间细节。

  ![](./imgs/animate_anyone.png)


<h2 id="7.ID保持图像生成和换脸的区别">7.ID保持图像生成和换脸的区别</h2>

  ID保持图像生成和换脸都可以达到生成和参考人脸相似的人体图像。这两者区别在于，ID保持图像生成是在生成过程中保持了参考图像的ID信息，而换脸则是将目标图像的人脸替换为参考图像的人脸。ID保持图像生成的目的是生成一个新的图像，使其在视觉上与参考图像相似，但不是完全相同。而换脸则是将目标图像的人脸替换为参考图像的人脸，使得目标图像的人脸与参考图像的人脸完全一致。其中，换脸还需要保持目标图像的其他信息不变，如头发、衣服等，而ID保持图像生成则不需要保持这些信息。


<h2 id="8.有哪些专注人像生成的预训练模型?">8.有哪些专注人像生成的预训练模型?</h2>

  随着大规模预训练模型的发展，专注人像生成的预训练模型也在不断涌现。目前，一些专注人像生成的预训练模型包括：
  - **CosmicMan**: 一个基于文本的高保真人物图像生成模型，能够产生与文本描述精确对齐的逼真人物图像。CosmicMan在图像质量和文本-图像对齐方面优于现有模型，如Stable Diffusion和Imagen。它在2D和3D人物生成任务中展现了实用性和潜力。

  - **Arc2Face**: 专注于使用人脸识别技术的核心特征来引导图像的生成，从而实现在各种任务中保持人脸身份的一致性。这意味着Arc2Face可以用于创建非常符合特定人物身份特征的人脸图像，为人脸识别、数字娱乐以及安全领域等提供了新的可能性。

    ![](./imgs/arc2face.png)

<h2 id="9.介绍一下IP-Adapter?">9.介绍一下IP-Adapter</h2>

IP-Adapter试图在现有的文本到图像扩散模型中实现图像提示（image prompt）的能力，以便更有效地生成所需的图像，想要解决一下问题：

1. 文本提示的复杂性：使用文本提示生成图像往往需要复杂的提示工程，这使得生成理想图像变得困难。

2. 文本表达的局限性：文本在表达复杂场景或概念时可能不够丰富，这限制了内容创作的灵活性。

3. 现有方法的局限性：直接从预训练模型微调（fine-tuning）以支持图像提示虽然有效，但需要大量的计算资源，且不兼容其他基础模型、文本提示和结构控制。

技术方案：
- 解耦的交叉注意力机制：IP-Adapter为文本特征和图像特征分别设置了独立的交叉注意力层。这种设计允许模型在保持原始文本到图像模型结构不变的情况下，有效地处理和融合图像提示信息。
- 图像编码器：使用预训练的CLIP图像编码器来提取图像提示的特征。CLIP模型通过对比学习在大量图像-文本对上训练，能够很好地理解和表示图像内容。
- 适配器训练：在训练阶段，只有新添加的交叉注意力层的参数会被训练，而预训练的扩散模型保持冻结状态。这使得IP-Adapter可以快速训练并适应不同的模型和应用。
- 通用性和兼容性：IP-Adapter训练完成后，可以轻松地应用于从同一基础模型微调出的自定义模型，并且与现有的结构控制工具（如ControlNet）兼容，从而实现更多样化的图像生成任务。

通过这些设计，IP-Adapter能够在保持模型轻量级的同时，实现与完全微调的图像提示模型相当甚至更好的性能。此外，IP-Adapter的通用性和兼容性使其在多种图像生成任务中具有广泛的应用潜力，也为后续2D数字人生成工作的可控性提供了一个参考。



<h2 id="10.介绍一下InstantID">10.介绍一下InstantID</h2>
InstantID只需输入单个图像即可生成实现 ID 保留生成各类风格的图像。以往这类生成任务都需要多样本，多轮次的训练微调才能得到特定ID模型，比如Textual Inversion、DreamBooth和 LoRA 等方法，其技术方案主要涉及以下要点：
- ID嵌入：使用预训练的面部模型来提取参考面部图像中的身份嵌入，这些嵌入包含了丰富的语义信息，如身份、年龄和性别等。与CLIP图像嵌入相比，这种方法能够提供更强的语义细节和更高的保真度。

- 图像适配器：引入了一个轻量级的适配器模块，该模块具有解耦的交叉注意力机制，允许将图像作为视觉提示与文本提示一起用于图像生成过程。这种设计使得InstantID能够灵活地处理各种风格的图像个性化。

- IdentityNet：设计了一个IdentityNet，它通过整合面部图像、面部关键点图像和文本提示来引导图像生成过程。IdentityNet在生成过程中完全由面部嵌入指导，而不使用任何文本信息，从而确保了面部身份的保留。

- 训练和推理策略：在训练过程中，只优化Image Adapter和IdentityNet的参数，而保持预训练文本到图像扩散模型的参数不变。在推理过程中，InstantID能够通过单步前向传播生成身份保持的图像，无需微调。


<h2 id="11.介绍一下PuLID">11.介绍一下PuLID</h2>

PuLID（Pure and Lightning ID customization），旨在解决文本到图像（Text-to-Image，T2I）生成中的个性化身份（ID）定制问题，在InstantID之后，PuLID使用SDXL作为基底模型取得了惊艳的效果，其关键组成有五个：
- 引入轻量T2I分支：PuLID在标准的扩散去噪训练分支之外引入了一个轻量T2I（Lightning T2I）分支。这个分支利用快速采样方法从纯噪声开始，通过有限且可控的步骤生成高质量的图像。

- 对比对齐损失：为了最小化ID插入对原始模型行为的影响，PuLID构建了一对对比路径，一个仅由文本提示条件，另一个同时由ID和文本提示条件。通过语义上对齐这两个路径的UNet特征，模型学会了如何在不影响原始模型行为的情况下嵌入ID信息。

- 准确的ID损失：由于闪电T2I分支能够生成高质量的x0，PuLID能够在一个更准确的设置中计算ID损失。这与测试阶段的设置相匹配，使得ID损失的优化更直接和有效。

- 全目标优化：PuLID的完整学习目标是结合扩散损失、对比对齐损失和ID损失。只有新引入的MLPs和交叉注意力层中的可学习线性层Kid和Vid会使用这个目标进行优化，其余部分保持冻结。

- 训练过程：PuLID的训练过程分为三个阶段。第一阶段使用传统的扩散损失进行训练；第二阶段引入ID损失以提高ID保真度；第三阶段添加对比对齐损失，使用完整目标进行微调。



<h2 id="12.介绍一下UniPortrait">12.介绍一下UniPortrait</h2>
UniPortrait 能够在保证ID信息的情况下，根据文本提示词来生成带有ID的个性化图片。在多人生成的场景，可以不通过mask以及文本描述词的改变来实现多ID生成：

- ID嵌入模块（ID Embedding Module）：
使用面部识别模型的倒数第二层特征作为基础内在ID特征，以保留与身份相关的空间信息。结合CLIP图像编码器的局部特征和面部识别模型的浅层特征，以增强面部结构表示。通过DropToken和DropPath正则化技术，显式地将面部结构特征与内在ID特征解耦，防止模型过拟合非本质面部细节。

- ID路由模块（ID Routing Module）：
在每个交叉注意力层内集成ID路由模块，为潜在的面部区域分配独特的ID。预测每个空间位置的离散概率分布，并选择最匹配的ID嵌入参与该位置的注意力机制。

- 两阶段训练方案：第一阶段（单身份训练阶段）：只引入ID嵌入模块，专注于单身份图像的训练。第二阶段（多身份微调阶段）：在第一阶段训练完成后，引入ID路由模块，对多身份图像进行微调。

- 路由正则化损失（Routing Regularization Loss）：通过L2损失和Gumbel-softmax技巧，确保所有ID都被路由，并且每个ID只被路由到一个目标面部区域。

- 兼容性与扩展性：UniPortrait设计为即插即用模块，与现有的生成控制工具（如ControlNet和IP-Adapter）兼容，提供了广泛的应用潜力。





<h2 id="13.介绍一下ConsistentID">13.介绍一下ConsistentID</h2>
ConsistentID在个性化面部生成的精度和多样性方面取得了显著的成果，并且在引入更多多模态ID信息的同时，保持了快速的推理速度，下面是这篇文章的主要贡献：

- 多模态面部提示生成器：该组件结合了面部特征、相应的面部描述和整体面部上下文，以增强面部细节的精确度。它使用一个细粒度的多模态特征提取器和一个面部ID特征提取器，从多条件下生成更详细的面部ID特征。

- 面部ID特征提取器：除了细粒度的面部特征输入条件外，该组件还将角色的整体ID信息作为视觉提示注入到ConsistentID中。这依赖于预训练的CLIP图像编码器和专门版本的IP-Adapter模型（IPA-FaceID-Plus）来生成整个图像的面部嵌入。

- ID保持网络：该网络通过面部注意力定位策略来优化，以保持每个面部区域内的ID一致性。这种策略通过在训练期间引入面部分割掩码来获得与增强文本交叉注意力模块学习的注意力分数，从而确保面部特征的注意力与相应的面部区域对齐。

- 训练和推理细节：在训练过程中，只优化面部编码器和整体面部ID特征提取器中的投影模块的参数，同时保持预训练扩散模型的参数冻结。推理过程中采用延迟主题条件，以平衡身份保持和可编辑性。

- 细粒度人类数据集构建：为了提供详细的面部特征和相应的文本提示，作者引入了一个数据集管道来创建FGID数据集，该数据集包含525,258张图像，提供了比现有公共面部数据集更丰富的细粒度ID信息和详细的面部描述。

通过这些步骤，ConsistentID能够在仅使用单个参考图像的情况下，生成具有高度ID一致性和多样化面部细节的个性化面部图像


<h2 id="14.介绍一下PhotoMaker">14.介绍一下PhotoMaker</h2>

根据[程明明老师的讲解](https://zhuanlan.zhihu.com/p/680468694)，定制化的文生图工作中，基于DreamBooth+LoRA的人像定制应用都有三个资源采集和消耗上的痛点：

1. 定制时间慢：由于需要在“测试”（定制）阶段对模型进行微调，理论上消耗的时间往往需要大约15分钟，在一些商业化的APP上，由于资源分配和排队的问题这一定制时间甚至要更久（数小时到数天）。

2. 消耗显存高：如果不进行额外的优化，在SDXL底模上执行定制化过程只能在专业级显卡上进行，这些专业级显卡的昂贵都是出了名的。

3. 对输入素材要求高：以妙鸭举例，它要求用户输入20张以上高质量的人像照片，且每次定制过程都需要重新进行收集。对用户来说非常不友好。

PhotoMaker希望在训练时，我们的输入图像和输出的目标图像都不来源于同一个图像。其次，且送入多个同一ID的图像提取embedding以得到对输出ID的一个全面且统一的表达。这个embedding将它命名为Stacked ID embedding。Stacked ID embedding中存取的每个embedding它们的图像来源可能姿态不同，表情不同以及配饰不同，但ID都是相同的，因此可以隐式的将ID与其他与ID无关的信息解耦，以使其只表征待输出的ID信息。论文的主要贡献在于提出了一种新的个性化图像生成框架，通过创新的堆叠ID嵌入方法，在保持高效率的同时，提供了高质量的生成结果和强大的泛化能力。此外，论文还通过自动化数据构建流程支持了模型的训练，并在多个应用场景中展示了其优越的性能。

<h2 id="15.介绍一下Hallo">15.介绍一下Hallo</h2>
Hallo是复旦大学朱思语老师Fusion Lab的工作，这也是champ之后第二个很受欢迎的数字人生成方向的一个工作，延续使用了RefernceNet和Animatediff模块在音频驱动人脸生成上得到了不错的效果，其主要技术方案涉及以下要点：

1. 分层音频驱动视觉合成模块（Hierarchical Audio-Driven Visual Synthesis）：
- 音频嵌入：使用wav2vec作为音频特征编码器，从音频中提取丰富的语义信息，为后续的动画生成提供运动驱动信息。
- 面部嵌入：采用预训练的面部编码器提取输入面部图像的身份特征，确保生成的动画能够准确地保留输入人物的身份特征，如面部表情、年龄和性别等。
- 交叉注意力机制：在音频和视觉特征之间建立对应关系，使模型能够根据音频信息精确地控制视觉输出中的嘴唇动作、表情变化和头部姿态等，从而实现音频与视觉的精准同步。
- 自适应加权融合：对不同层次的交叉注意力输出进行自适应加权融合，以更好地平衡嘴唇、表情和姿态等不同视觉元素在动画生成中的重要性，提高生成动画的整体质量和自然度。

2. ReferenceNet：利用ReferenceNet引导视觉生成，确保生成的动画与参考图像在视觉纹理信息（如肖像的细节和背景）上保持一致性，进一步提升动画的真实感和视觉效果。

3. 训练和推理策略：
训练过程：包含两个阶段，第一阶段主要优化ReferenceNet和降噪UNet的空间模块，提升单帧图像的生成能力；第二阶段重点训练分层音频-视觉交叉注意力模块，建立音频与视觉信息之间的关系，并引入运动模块以增强模型的时间连贯性。
推理过程：在推理阶段，模型仅需单步前向传播即可根据输入的参考图像和驱动音频生成相应的动画序列，无需进行复杂的微调过程。

<h2 id="16.介绍一下Hallo2">16.介绍一下Hallo2</h2>
Hallo2同样是Fusion Lab的工作，仅仅隔了大约4个月，原团队就升级了Hallo，此工作亮点相对v1主要是支持4K分辨率视频输出和小时级的人像视频生成，并且官方给出的demo效果更加惊艳，文章也被收录到了ICLR 2025会议中，其要点如下：

1. 长期视频生成：通过增量生成方法，结合patch-drop数据增强技术和高斯噪声增强，将肖像动画从几秒钟延长到数十分钟，维持视觉连贯性和身份一致性。
- Patch-drop：通过在条件帧上应用patch-drop技术，随机删除图像块（patches），以防止前一帧的外观信息对当前帧产生影响，从而确保生成视频中角色的外观主要来源于参考图像。
- 高斯噪声增强：在条件帧的潜在表示中加入高斯噪声，增强模型在潜在空间中从污染中恢复的能力，减轻累积误差的影响。

2. 高分辨率视频生成：采用向量量化的潜在代码(VQ-GAN)和时间对齐技术，实现了4K分辨率的肖像视频生成，保持高分辨率细节上的时间连贯性。

3. 语义文本提示控制：引入语义文本提示作为条件输入，允许更精细地控制生成视频中的面部表情和动态，超越了传统的音频提示。

4. 实验验证：在包括HDTF、CelebV和新引入的“Wild”数据集在内的公开数据集上进行了广泛的实验，证明了所提方法在生成长达数十分钟的4K分辨率视频方面的有效性。

<h2 id="17.介绍一下Hallo3">17.介绍一下Hallo3</h2>
Hallo3在Hallo2发布后3个月诞生了，在各家DiT架构的视频基底模型放出后，Hallo3希望探究DiT架构下的视频生成是否可以增强人脸视频生成的整体生成质量，并且开源了其训练代码和数据集，文章已被CVPR 2025会议收录，其主要涉及的技术方案如下几点：

1. 预训练的变换器视频生成模型：
- 基础架构：以CogVideoX模型为基础，利用3D变分自编码器（VAE）对视频数据进行压缩，将潜在变量与文本嵌入相结合，通过专家变换器网络进行处理。
- 条件机制：引入文本提示、语音音频条件和身份外观条件三种条件机制，主要采用交叉注意力和自适应层归一化整合这些条件信息。
2. 身份参考网络：
3D VAE和变换器层：使用因果3D VAE结合42层变换器层的身份参考网络，从参考图像中提取身份特征，嵌入到去噪潜在代码中，基于自注意力机制增强模型对身份信息的表示和长期保持。
特征融合：将参考网络生成的视觉特征与去噪网络的特征进行融合，确保生成的面部动画在长时间序列中保持一致性和连贯性。
3. 语音音频条件：
音频嵌入：使用wav2vec框架提取音频特征，生成帧特定的音频嵌入，通过线性变换层将音频嵌入转换为适合模型的表示。
4. 交叉注意力机制：在去噪网络中，交叉注意力机制将音频嵌入与潜在编码进行交互，增强生成输出的相干性和相关性，确保模型有效地捕捉驱动角色生成的音频信号。
5. 视频外推：引入运动帧作为条件信息，将生成视频的最后几帧作为后续片段生成的输入，用3D VAE处理运动帧，生成新的潜在代码，实现时间一致的长视频推理。

<h2 id="18.介绍一下EchoMimicV2">18.介绍一下EchoMimicV2</h2>
EchoMimicV2 是一种新颖的半身人类动画方法，旨在通过简化条件生成高质量的半身动画。其技术方案主要涉及以下要点：

1. Audio-Pose Dynamic Harmonization (APDH) 减少姿态条件的冗余，同时增强音频条件的控制范围，使音频和姿态条件能够动态协作，生成更自然的半身动画。：
- Pose Sampling (PS)：通过逐步移除关键点，减少姿态条件对唇部运动、面部表情和身体（呼吸节奏）的控制，为音频驱动的过程腾出空间。
- Audio Diffusion (AD)：通过逐步扩散音频条件，增强音频与唇部、面部和身体的相关性。

2. Head Partial Attention (HPA) 弥补半身数据的不足，提升面部表情的生成质量。：
- 数据增强：在音频-面部同步阶段，引入头像数据以增强面部表情，并通过HPA排除填充部分，无需额外的交叉注意力块。
3. Phase-specific Denoising Loss (PhD Loss) 通过在每个阶段应用相应的损失函数，PhD Loss能够更有效地优化模型，确保训练过程更加稳定和高效。：
- 姿态主导阶段 (S1)：主要学习运动姿态和人体轮廓。
- 细节主导阶段 (S2)：主要细化角色特定的细节。
- 质量主导阶段 (S3)：主要增强颜色和其他低级视觉质量。

<h2 id="19.介绍一下LivePortrait">19.介绍一下LivePortrait</h2>
LivePortrait 是一种快手开发的高效肖像动画框架，能够将静态肖像图片转换成动态视频，同时实现高效率和精确控制，在社区有着很高的影响力，在大部分研究者均使用扩散模型来做数字人生成的时候，LivePortrait较为少见的仍使用GAN架构取得了很好的效果，并且支持实时推理，其技术方案主要如下所示：

1. 关键点提取与表示：
- 隐式关键点表示：LivePortrait 使用隐式关键点作为中间运动表示，这些关键点能够有效捕捉面部表情和头部姿态的变化。与传统的显式关键点表示相比，隐式关键点可以更自然地处理面部的细微变化，如眨眼和微笑等。
- 混合图像-视频训练：为了提高模型的泛化能力，LivePortrait 采用了混合图像和视频数据进行训练。这种方法结合了大量高质量的静态肖像图像和视频帧，使得模型能够更好地适应不同风格和类型的输入数据。
2. 运动变换与优化：
- 可扩展的运动变换：LivePortrait 在运动变换中引入了比例因子，解决了原始框架中比例问题，提高了模型对不同身份和姿势的适应性。这种可扩展的运动变换使得模型能够更准确地捕捉和再现面部表情的变化。
- 关键点引导的优化：为了提高对微妙表情的动画效果，LivePortrait 引入了 2D 地标引导优化。通过使用 2D 地标来捕捉微观表情，模型能够更生动地驱动面部表情，如眨眼和眼神移动等。
3. 拼接与重定向模块：
- 拼接模块：LivePortrait 设计了一个拼接模块，能够将动画化的肖像无缝地拼接到原始图像空间中。这一模块解决了在动画化过程中可能出现的像素错位问题，使得动画化的肖像能够自然地融入到原始背景中。
- 眼睛和嘴唇重定向模块：为了实现对眼睛和嘴唇运动的精确控制，LivePortrait 引入了眼睛和嘴唇重定向模块。这些模块允许用户通过简单的标量输入来控制眼睛的开合和嘴唇的动作，从而实现更加个性化和生动的动画效果。


<h2 id="20.介绍一下ConsisID">20.介绍一下ConsisID</h2>

ConsisID的模型来解决身份保持文本到视频（IPT2V）生成中的问题，已中稿CVPR 2025。ConsisID模型主要通过频率分解和层次化训练策略来解决视频生成中的身份保持问题，并在多个维度上验证了模型的有效性，下面详细介绍一下相关方案：

1. 频率分解的身份特征控制信号
- 低频视角（Low-frequency View）：使用全局面部提取器（Global Facial Extractor）来编码参考图像和面部关键点，生成富含低频信息的特征，并将其集成到网络的浅层，以减轻DiT模型的训练挑战。
- 高频视角（High-frequency View）：设计局部面部提取器（Local Facial Extractor）来捕获高频细节，并将其注入到变换器（Transformer）块中，增强模型保持细粒度特征的能力。

2. 层次化训练策略（Hierarchical Training Strategy）

- 粗到细训练（Coarse-to-Fine Training）：首先使用全局面部提取器让模型优先学习低频特征，然后使用局部面部提取器让模型关注高频细节，以确保生成的视频在空间和时间维度上保持一致性。
- 动态掩码损失（Dynamic Mask Loss）：通过将模型的注意力集中在面部区域，减少背景噪声对训练的干扰。
- 动态跨ID损失（Dynamic Cross-Face Loss）：通过引入高斯噪声和跨脸输入，提高模型对未见过身份的泛化能力。

<h2 id="21.什么是视频虚拟换衣">21.什么是视频虚拟换衣</h2>

视频虚拟试穿任务定义为将指定服饰穿着到视频中的人物身上，以实现视频级别的服饰上身展示效果。在此之前，大多数的方法主要聚焦在基于图像的虚拟试穿领域。然而，仅仅依靠静态图像进行虚拟试穿存在一定的局限性，静态试衣图不够生动活泼，并且也无法充分展现服饰所具有的更多物理特性。当直接把基于图像的试穿方法逐帧应用到视频级别的输入时，所生成的换衣视频常常会因为帧与帧之间的不一致性，从而导致时间上出现不连续的情况


<h2 id="22.视频换衣和图像换衣的区别">22.视频换衣和图像换衣的区别</h2>

| 维度       | 图像换衣 | 视频换衣 | 评价                                                                                     |
|------------|----------|----------|------------------------------------------------------------------------------------------|
| 动态展示   | ❌        | ✅        | 视频虚拟换衣可以展示服装在不同角度和动作下的效果，更加立体和真实。                         |
| 物理特性   | ❌        | ✅        | 视频虚拟换衣可以在动态展示中，让用户对衣物的质感、剪裁和摆动等物理特性有更全面的了解。       |
| 吸引力     | ❌        | ✅        | 视频能够吸引用户的眼球，增强用户参与感和体验感，提高他们对产品的兴趣。                     |
| 信息价值   | ⭐️⭐️     | ⭐️⭐️⭐️⭐️ | 在视频中，服装的移动、光影变化等细节都更容易展示，让用户对服装材质和款式有更详细的认识。     |
| 商业价值   | ⭐️⭐️⭐️   | ⭐️⭐️⭐️⭐️ | 视频可以更好展示商品特性，吸引用户注意力，提升点击率促进成交；视频更容易融入丰富的营销内容。 |
| 难度       | ⭐️⭐️⭐️   | ⭐️⭐️⭐️⭐️ | 视频虚拟试衣模型研发难度更高（数据收集、建模训练等）。                                     |
| 成本       | ⭐️⭐️     | ⭐️⭐️⭐️⭐️ | 视频换衣模型的参数量和计算量更大，训练和推理成本更高。                                     |
| 成熟度     | ⭐️⭐️⭐️   | ⭐️⭐️     | 图像换衣技术成熟度更高，视频换衣目前达到业务可用水平。                                     |

<h2 id="23.视频虚拟换衣有哪些应用场景">23.视频虚拟换衣有哪些应用场景</h2>

应用场景：分别服务于平台（自动化营销）、商家（素材生产）、买家（个性化试穿）。
特点：覆盖B端（平台/商家）和C端（买家）需求，形成完整的商业化链路。

| 产品级能力              | 应用场景                                                                                     |
|-------------------------|----------------------------------------------------------------------------------------------|
| 营销视频自动投放        | 面向平台，自动圈选缺乏营销视频的品类，并生成相应的含有卖点信息的营销视频以供投放             |
| 模特试穿素材生成        | 面向商家，帮助商家便捷地产出高质量试穿视频，丰富其店铺营销素材                               |
| 买家试穿效果生成        | 面向买家，帮助买家直观地看到自己心仪的服饰上身后的效果                                       |

<h2 id="24.视频虚拟换衣的技术难点">24.视频虚拟换衣的技术难点</h2>

挑战1：帧间一致性（如动态服装易出现撕裂/闪烁）。 
- 方案：采用时序连贯模型（如3D卷积）保证动作平滑过渡。

挑战2：细节还原（如LOGO模糊、复杂材质失真）。 
- 方案：引入高分辨率VAE重建损失，优化物理仿真算法。

挑战3：计算成本高（如模型参数量大、推理速度慢）。 
- 方案：通过模型压缩（如知识蒸馏）和分布式推理加速。

挑战4：模型基础能力受限（无法生成非常精细的视频细节）
- 方案：持续迭代DiT方案的人物垂领图生视频（img2video）基础模型，作为视频换衣模型的预训练，提升模型对服饰电商领域的泛化性，DiT 结构在参数量和数据集的规模扩展方面更加容易。

<h2 id="25.如何解决视频换衣中服饰与动作的匹配冲突">25.如何解决视频换衣中服饰与动作的匹配冲突</h2>

- 精细化标签体系：定义30+维度的服饰/模板标签（如季节、性别、动作类型），通过图文大模型离线提取标签。

- 动态匹配规则：输入服饰时实时校验标签合法性（如无兜服饰禁止匹配插兜动作模板），避免逻辑矛盾。

- 案例：男性模板+女装场景，系统自动过滤性别标签不匹配的模板，确保换衣结果合理。

<h2 id="26.如何一套成品模特试穿视频生成链路">26.如何一套成品模特试穿视频生成链路</h2>

我们可以根据输入的服饰itemid或者商品图，自动生成符合商品特性的、时长约1分钟的多分镜、多姿态模特换衣视频。具体流程如下：
1. 商品解析与特征提取：输入商品ID或图片后，通过 多模态大模型提取商品关键特征（如款式、颜色、适用场景），并匹配预设的服饰标签库（如“风衣-秋冬-商务”）。结合商品类目和用户历史数据，预测目标分镜类型（如“全身展示-细节特写-动态走秀”
2. 模板智能匹配：从模板库中筛选与商品标签匹配的视频模板（如“插兜动作模板”需匹配有兜服饰）。通过图文匹配模型计算商品图与模板的相似度，优先选择光影、场景适配的模板。
3. 多模态增强（配乐与TTS）：根据商品风格自动匹配背景音乐（如商务风配钢琴曲），并通过 文案生成模型 输出卖点文案，转为语音旁白。结合镜头时长、节奏与音乐节拍，优化视频剪辑点（如高潮部分突出价格信息）。
<h2 id="27.介绍一下SimSwap">27.介绍一下SimSwap</h2>
SimSwap是一个基于GAN用于高保真人脸交换的高效框架，相关论文发表在ACMMM 2020上。该框架通过生成对抗网络实现了高质量的人脸交换，能够在保持原始图像特征的同时，进行自然的面部表情和动作转换。

1. 身份注入模块（IIM, Identity Injection Module）
- 通过特征级迁移源人脸身份信息，利用 ArcFace 模型提取源人脸的嵌入向量（ID特征），并将其通过 AdaIN（自适应实例归一化） 注入目标人脸特征中
- 与传统的像素级操作不同，IIM在编码器-解码器架构的中间层完成身份融合，避免直接修改像素导致的纹理失真

2. 弱特征匹配损失（Weak Feature Matching Loss）
- 在人脸交换任务中，直接修改包含身份和属性信息的特征 ( Fea_T ) 可能会导致属性信息（如表情、姿势、光照等）受到影响，从而导致属性不匹配。
- 为了解决这个问题，SimSwap引入了弱特征匹配损失（Weak Feature Matching Loss），通过对比源人脸和目标人脸的特征图，确保在交换身份的同时保持属性一致性。仅约束解码器高层特征（而非浅层特征），防止过度拟合目标人脸的纹理细节，从而保留表情、姿态等动态属性

通过在FaceForensics++和CelebAMaskHQ数据集上的实验，SimSwap在身份性能和属性保持之间取得了良好的平衡，优于其他现有方法。

<h2 id="28.介绍一下HifiFace">28.介绍一下HifiFace</h2>
HifiFace是由腾讯优图团队于2021年提出的高保真人脸交换技术，核心目标是实现身份信息与目标人脸属性（如表情、光照、姿态）的高度融合，同时保持高质量的图像生成效果。HifiFace在SimSwap的基础上进行了改进，主要包括以下几个方面：

1. 3D形状感知的身份提取器
- 利用3D形态模型（3DMM）和3D人脸重建方法，从源图像和目标图像中提取身份、表情和姿态的系数。通过重新组合这些系数，生成一个新的3D人脸模型，该模型具有源图像的身份和目标图像的表情及姿态。将这个3D人脸模型与通过人脸识别网络提取的身份特征向量结合，形成3D形状感知的身份向量。

2. 语义人脸融合模块
- 该模块优化了编码器和解码器特征的组合，并进行自适应融合，以提高结果的逼真度。在特征级别，语义人脸融合模块通过学习到的自适应人脸掩模，整合编码器的低级特征和解码器特征，以保留空间信息和纹理细节。在图像级别，语义人脸融合模块学习一个稍微扩张的掩模，以适应脸部形状的变化，并解决遮挡问题。

HiFiFace采用端到端学习框架，包括编码器、解码器、3D形状感知身份提取器和语义人脸融合模块模块，协同工作以生成高质量的人脸交换结果。

<h2 id="29.介绍一下Face-Adapter">29.介绍一下Face-Adapter</h2>
Face-Adapter是一个基于扩散模型的高保真面部交换框架，旨在实现高质量的人脸交换。该框架通过引入面部适配器（Face-Adapter）模块来增强面部特征的表达能力，从而提高人脸交换的效果。其主要贡献点如下：

1. 空间条件生成器(Spatial Condition Generator)
该模块用于自动预测3D先验标志点和变化前景区域的掩码，分为两个小部分
- 3D Landmark Projector:，使用3D面部重建方法提取源和目标面部的身份、表情和姿态系数，重新组合这些系数以获取相应的标志点。
- Adapting Area Predictor: 自动预测模型需要生成的区域（适应区域），同时保持其余区域不变。

2. ID注入网络（Identity Encoder）
使用预训练的面部识别模型提取面部嵌入，然后通过变换解码器中的可学习查询将它们转换到文本空间，从而提高生成图像中身份的一致性。这部分架构和Q-Former结构类似，Q-Former是一个用于图像和文本之间的交互的模块，能够将图像特征映射到文本空间，从而实现跨模态的特征融合。这里将ID特征映射到文本空间，主要是为了增强生成图像中的身份一致性。

3. 属性控制器（Attribute Controller）
- 该模块用于控制生成图像中的属性特征，如表情、姿态等，确保生成结果符合预期的属性要求。
- 通过对输入的属性信息进行编码，结合生成模型的输出，动态调整生成图像的属性特征。

论文通过提出Face-Adapter，为利用预训练扩散模型进行高效人脸编辑提供了新的思路和方法，展示了在人脸重演和交换任务上的显著性能提升。

<h2 id="30.视频换脸有哪些挑战">30.视频换脸有哪些挑战</h2>
视频换脸技术面临着多种挑战，主要包括以下几个方面：

1. 真实感：生成的视频需要在视觉上看起来真实，尤其是在面部表情、光照和姿态等方面。任何不自然的细节都可能导致观众的怀疑。

2. 时间一致性：视频是由一系列连续的帧组成的，因此在换脸过程中需要确保每一帧之间的过渡是平滑的，避免出现跳跃或闪烁现象。

3. 细节保留：在换脸过程中，需要保留源图像中的细节和纹理，以确保生成的人脸在细节上与源图像一致。

4. 计算效率：视频换脸通常需要处理大量的数据，因此需要高效的算法和模型，以确保在合理的时间内生成高质量的视频。如若将扩散模型应用在视频换脸中，可能会导致推理速度过慢。需要更高效的架构和少步数的蒸馏算法保证推理速度。

5. 数据隐私：视频换脸技术可能涉及到个人隐私和肖像权的问题，因此在使用时需要遵循相关的法律法规，确保不侵犯他人的隐私权。

6. 训练数据：高质量的视频换脸模型通常需要大量的训练数据，收集和标注这些数据可能会非常耗时和昂贵。并且换脸本身作为没有金标准的任务，研究者们更常用的手段是通过相同人不同表情的图像进行训练，这样会导致模型在生成时对跨ID的变化不够敏感。

7. 适应性：不同的应用场景可能需要不同的换脸效果，因此模型需要具备一定的适应性，以满足不同的需求。

<h2 id="31.介绍一下ReFace">31.介绍一下ReFace</h2>
ReFace图解决的问题是在人脸交换任务中生成真实感强、质量高的交换图像的挑战。尽管在人脸交换领域已经取得了一些进展，但生成真实感强的图像仍然存在困难，尤其是在涉及姿态变化大、颜色差异和遮挡等场景时，常常会产生人工痕迹。为了解决这些问题，ReFace更好地利用扩散模型进行人脸交换，主要贡献包括以下几点：

1. 将人脸交换任务重新定义为自监督的、训练时的修复问题，增强身份特征的传递，同时与目标图像融合。

2. 引入多步去噪扩散隐式模型（DDIM）采样，在训练过程中加强身份和感知相似性。

3. 利用CLIP特征解耦，从目标图像中提取姿态、表情和光照信息，提高保真度。

4. 引入掩模洗牌技术，在修复训练期间使用，允许创建所谓的通用模型进行交换，增加了头部交换的特征。

5. 与依赖多个现成模型的先前工作不同，提出的方法相对统一，因此对其他现成模型中的错误更加健壮。

最终ReFace通过在FFHQ和CelebA数据集上的广泛实验，验证了所提方法的有效性和鲁棒性，展示了高保真、真实感强的人脸交换，并且推理时间很短。代码已开源：https://github.com/Sanoojan/REFace

<h2 id="32.换脸如何和3DMM人脸重建相结合">32.换脸如何和3DMM人脸重建相结合</h2>
3DMM（3D Morphable Model）是一种用于人脸重建和表情建模的技术，它通过对人脸的几何形状和纹理进行建模，能够生成高质量的3D人脸模型。将换脸技术与3DMM相结合，可以在以下几个方面提升换脸效果：

1. 3D人脸重建：通过3DMM技术，可以从2D图像中重建出高质量的3D人脸模型，这为换脸提供了更丰富的几何信息。这样可以更好地处理不同角度、光照和表情变化下的人脸交换。
2. 姿态和表情控制：3DMM可以对人脸的姿态和表情进行精确控制，这使得在换脸过程中能够更好地保持目标人脸的自然性和一致性。通过调整3DMM模型的参数，可以实现对目标人脸的姿态和表情进行细致的调整，从而提高换脸效果。
3. 纹理映射：3DMM可以将源人脸的纹理映射到目标人脸的3D模型上，这样可以更好地保持源人脸的细节和特征，同时避免在2D图像中直接进行纹理交换可能导致的失真和不自然现象。
4. 光照和阴影处理：3DMM可以模拟不同光照条件下的人脸外观，这使得在换脸过程中能够更好地处理光照和阴影问题，从而提高生成图像的真实感。
5. 视角一致性：3DMM可以生成不同视角下的人脸模型，这使得在换脸过程中能够更好地保持视角一致性，避免由于视角变化导致的失真和不自然现象。
6. 训练数据增强：通过3DMM技术，可以生成大量不同姿态、表情和光照条件下的人脸图像，这为训练换脸模型提供了丰富的数据来源，从而提高模型的泛化能力和鲁棒性。



<h2 id="33.介绍一下Phantom">33.介绍一下Phantom</h2>
Phantom 是一个统一的视频生成框架，适用于单主题和多主题参考，基于现有的文本转视频和图像转视频架构构建。它通过重新设计联合文本-图像注入模型，利用文本-图像-视频三元组数据实现跨模态对齐。此外，它在人物生成中强调主题一致性，同时增强了身份保留视频生成。Phantom 主要包括以下几个方面的创新：

1. 数据结构设计
- 构建文本-图像-视频三元组数据结构：为了实现跨模态学习，作者构建了一个包含图像、文本和视频的数据结构。这些数据结构要求视频同时与图像和文本配对。
- 数据分类：将图像提示分为两类：in-paired 和 cross-paired。In-paired 数据通过从视频中选择关键帧作为参考图像，确保图像和视频主体的一致性。Cross-paired 数据通过匹配不同视频中的元素并过滤掉视觉相似度高的片段来构建，以避免模型简单地复制粘贴输入图像。

2. 模型架构设计
- Phantom 架构：Phantom 模型分为一个未训练的输入头和一个可训练的 DiT（Diffusion in Transformer）模块。DiT 模块参考了 MMDiT（Multi-Modal Diffusion in Transformer）结构，这是当前图像和视频基础模型的主流选择之一。
- 输入头设计：输入头包括视频编码器和文本编码器，它们继承了基础模型的权重，将输入视频和文本提示编码为相应的潜在特征。关键的参考图像通过特定的视觉编码器进行编码，然后分别与视频特征和文本特征进行拼接。这些拼接的特征分别输入到 DiT 的视觉分支和文本分支进行计算。
- 视觉编码器设计：视觉编码器由变分自编码器（VAE）和 CLIP 组成。与视频潜在特征拼接的图像特征使用 3D VAE 来保持视觉分支输入的一致性，而与文本分支拼接的图像特征使用 CLIP 提供高级语义信息，以弥补低级 VAE 特征的局限性。

通过上述方法，Phantom 框架有效地解决了主体一致性视频生成的问题，实现了文本和图像模态的深度和同时对齐，生成了高质量且主体一致的视频内容。

<h2 id="34.介绍一下FantasyTalking">34.介绍一下FantasyTalking</h2>
FantasyTalking试图解决从单一静态肖像生成逼真的可动画化头像的挑战。具体来说，它旨在克服现有方法在捕捉微妙面部表情、相关全身运动以及动态背景方面的不足。论文提出了一种新颖的框架，利用预训练的视频扩散变换器模型来生成具有可控运动动态的高保真、连贯的说话肖像。该工作使用了最新的Wan2.1作为视频基底模型，并在此基础上进行了改进。其主要贡献包括：

1. 双阶段音视频对齐策略（Dual-Stage Audio-Visual Alignment）
- 剪辑级训练（Clip-Level Training）：在第一阶段，通过计算整个剪辑长度的音频-视觉令牌序列之间的3D全注意力相关性，建立全局音视频依赖关系，实现整体特征融合。这使得模型能够同时学习与音频弱相关的非语言线索（如眉毛运动、肩膀运动）和与音频同步的嘴唇动态。
- 帧级训练（Frame-Level Training）：在第二阶段，专注于通过帧精确的音视频对齐来细化嘴唇运动。通过分割音频和视频，并根据一对一的映射关系重塑视频令牌和音频令牌，然后计算它们之间的3D全注意力，确保视觉特征只关注其对应的音频特征。此外，利用MediaPipe提取精确的嘴唇掩码，并将其投影到潜在空间，形成嘴唇关注的约束掩码，以确保嘴唇运动与音频内容更紧密地对齐。

2. 身份保持（Identity Preservation）
为了避免使用传统的参考网络，论文提出了一种专注于面部建模的交叉注意力模块。首先从参考图像中裁剪出面部区域，确保模型只关注与身份相关的面部区域。然后利用ArcFace提取面部特征，并使用Q-Former进行对齐，得到ID嵌入。这些身份特征通过专门的交叉注意力层与预训练的DiT注意力块交互，从而在视频生成过程中保持一致的身份。

3. 运动强度调制网络（Motion Intensity Modulation Network）
为了控制面部表情和身体运动的强度，论文引入了一个运动强度调制网络。利用MediaPipe提取面部表情运动系数，利用DWPose计算身体运动系数。这些系数被归一化到[0, 1]范围内，表示面部表情和身体运动的强度。在推理阶段，用户可以通过调整这些系数来控制面部和身体运动的幅度。

FantasyTalking通过双阶段音视频对齐训练过程，有效地捕捉了音频信号与嘴唇运动、面部表情和身体运动之间的关系。通过专注于面部知识学习来增强身份一致性，并利用运动网络控制面部表情和身体运动的幅度，确保了自然和多样化的动画效果。

<h2 id="35.介绍一下InfiniteYou">35.介绍一下InfiniteYou</h2>
InfiniteYou试图解决在高保真度和灵活性下实现身份保留（identity-preserved）图像生成的问题，特别是在利用先进的扩散变换器（Diffusion Transformers，简称DiTs）如FLUX时所面临的挑战。具体来说，论文关注以下几个关键问题:

1. 身份相似性不足：现有方法生成的图像在身份相似性方面表现不够好，即生成的图像与原始人物的身份特征不够接近。
2. 文本-图像对齐不佳：生成的图像与文本描述之间的对齐不够准确，导致图像内容与文本描述不匹配。
3. 生成质量和美学欠佳：生成的图像在整体质量和美学吸引力方面存在不足，无法满足高质量图像生成的需求。
4. 模型扩展性和兼容性问题：现有的基于DiTs的方法在扩展性和与其他方法的兼容性方面存在挑战，限制了其在不同场景下的应用。

该框架通过以下主要方法和技术来解决这些问题:

1. InfuseNet：身份特征注入网络

- 网络架构：InfiniteYou 引入了一个名为 InfuseNet 的组件，它是对 ControlNet 的扩展，能够将身份特征通过残差连接注入到 DiT 基础模型中。InfuseNet 与 DiT 基础模型共享类似的结构，但包含较少的 Transformer 块。具体来说，如果 DiT 基础模型有 M个 Transformer 块，InfuseNet 有 N个 Transformer 块，且 M=N⋅i，其中 i 是一个乘数因子。

- 身份特征注入：身份图像通过一个冻结的面部身份编码器编码成身份嵌入，然后通过一个投影网络投影到与文本特征类似的维度，并通过注意力层输入到 InfuseNet。InfuseNet 的每个 Transformer 块预测 DiT 基础模型中对应的一组 Transformer 块的输出残差。这种设计有效地将身份特征与文本特征分离，减少了潜在的纠缠和冲突，同时保持了基础模型的生成能力。

2. 多阶段训练策略

- 第一阶段：预训练：使用真实单人单样本（SPSS）数据进行预训练。这些数据虽然质量不高，但足以用于初步训练模型，使其能够学习重建真实人物肖像。
- 第二阶段：监督微调（SFT）：利用合成的单人多样本（SPMS）数据进行监督微调。这些合成数据由第一阶段预训练的模型生成，并通过一系列现成的模块（如美学模块、增强模块、人脸交换模块等）进行处理，以提高数据的质量和美学效果。通过这种方式，模型在保持身份相似性的同时，学习生成高质量和美学上更吸引人的图像。

3. 残差连接与身份特征注入

- 残差连接：InfuseNet 通过残差连接将身份特征注入到 DiT 基础模型中，而不是直接修改注意力层。这种方法与传统的 IP-Adapter 不同，后者直接修改注意力层，可能会损害基础模型的生成能力。通过残差连接，InfU 有效地保持了基础模型的生成能力，同时提高了身份相似性。

通过上述方法，InfiniteYou 在身份相似性、文本-图像对齐、生成质量和美学方面均取得了显著的改进。



<h2 id="36.介绍一下FlashFace">36.介绍一下FlashFace</h2>
FlashFace旨在解决人类图像个性化的问题，同时保持高保真度的身份特征，其主要贡献包括以下几个方面：

1. 特征图编码：
与以往将参考面部编码为一个或几个图像标记的方法不同，FlashFace使用Face ReferenceNet将参考图像编码为一系列特征图。这种方法能够保留更多的面部细节，如疤痕、纹身和脸型等。

2. 解耦的集成策略：
为了平衡文本和图像指导之间的冲突，FlashFace采用了一种解耦的集成策略。具体来说，它通过在U-Net解码器中引入额外的参考注意力层来整合参考特征，这些层与用于文本条件的交叉注意力层是分开的，确保了两种控制信号的解耦。

3. 新的数据构建流程：
论文提出了一种新的数据构建流程，通过收集单个人物的多张图像来确保在训练过程中参考图像和目标图像之间的面部变化。这种数据确保了模型不会简单地复制参考面部，而是从文本提示中获取指导，从而提高了遵循文本提示的能力。

FlashFace基于广泛使用的SDV1.5项目构建，使用U-Net进行去噪，引入CLIP的语言分支来编码语言提示，并通过交叉注意力操作进行整合。此外，还包括Face ReferenceNet来提取详细的面部特征，并使用额外的参考注意力层将它们纳入网络。通过这些方法，FlashFace能够在不同的下游任务中展示出色的个性化能力，包括人类图像定制、在不同语言提示下的面部交换，以及将虚拟角色变为真实人物等。

<h2 id="37.介绍一下SkyReels-A1">37.介绍一下SkyReels-A1</h2>
SkyReels-A1是一个基于视频扩散 Transformer（DiT）的框架，用于生成高质量的肖像动画。现有的方法在身份保持、背景稳定性和面部动态真实性方面仍存在问题，尤其是在全身动画场景中。SkyReels-A1通过以下几个方面来解决这些问题：

1. 整体框架
- 输入与输出：给定一个输入视频片段和一个参考肖像图像，SkyReels-A1 从视频中提取面部表情感知关键点（expression-aware landmarks），并将这些关键点作为运动描述符，将捕捉到的表情转移到肖像上。
- 条件视频生成框架：基于 DiT 的条件视频生成框架，直接将面部表情感知关键点的特征整合到输入潜在空间中。使用基于 VAE 架构的姿态引导器（pose guider）将面部表情感知关键点编码为 DiT 框架的条件输入，确保模型能够捕捉到准确的低维视觉信息，同时保持面部语义信息的完整性。
- 面部图像-文本对齐模块：通过将面部特征映射到文本特征空间，增强身份一致性，实现精确控制面部表情和动作的同时保持角色身份。

2. 关键模块

表情感知关键点（Expression-Aware Landmark）：
- 3D Facial Expressions：引入一个神经渲染模块来增强重建面部表情的准确性和真实性。与传统方法相比，该模块能够提取高精度的 3D 关键点，捕捉更复杂的面部动态和细微的表情变化。
- 3D Landmark Guider：使用 3D 因果编码器作为核心组件，确保运动信号与输入视频潜在表示之间的时空一致性。通过将运动信号直接投影到与视频潜在表示共享的潜在空间中，实现精确的运动对齐。
- 面部图像-文本对齐（Facial Image-Text Alignment）：
- 轻量级映射模块：通过多层感知机（MLP）将面部特征映射到文本特征空间，增强身份一致性。这种方法不仅提高了面部特征的准确性，还允许无缝转移预训练基础模型的能力。

3. 训练策略

- 运动驱动训练（Motion-Driven Training）：在这一阶段，模型通过 3D 地标引导模块将运动条件整合到视频生成过程中。通过训练 PatchEmbedding 模块的卷积层，模型能够适应地标特定的变化，同时保持图像到视频（IT2V）架构的核心功能。
- 身份保持训练（Identity-Preserving Training）：在这一阶段，模型专注于增强动画肖像中的身份保持。通过使用 CLIP 图像编码器生成面部表示，并将其投影到文本特征空间，模型能够在不同表情和动作序列中保持身份一致性。
- 多模块联合微调（Multi-module Joint Fine-Tuning）：在这一阶段，模型联合优化 3D 地标引导器、DiT 块和线性投影层，进一步提高模型的泛化能力，使其能够处理各种肖像类型和运动场景。

SkyReels-A1 能够在保持身份一致性的同时，生成高质量、自然且具有表现力的肖像动画。

<h2 id="38.介绍一下SkyReels-A2">38.介绍一下SkyReels-A2</h2>
SkyReels-A2旨在解决如何将任意视觉元素（例如人物、物体、背景等）根据文本提示组装成合成视频的问题，同时保持与每个元素的参考图像的严格一致性。其贡献点包括：

1. 数据管道设计
- 构建数据集：设计了一个全面的数据管道，用于构建用于模型训练的提示-参考-视频三元组。这些三元组包括多个参考图像（用于不同视觉元素）和相应的文本提示。
- 数据处理：对原始视频进行预处理，包括过滤、基于关键帧的时间分割、多专家视频字幕模型生成的全面描述和结构化概念注释、检测和分割模型提取视觉元素等。
- 参考图像选择：为了避免生成视频中的“复制粘贴”效果，SkyReels-A2通过计算不同视频片段之间的相似度，选择多样化的参考图像。

2. 模型架构
- 双流结构：对于每个输入的参考图像，模型采用双流结构进行处理。第一流使用语义图像编码器（如 CLIP 图像编码器）提取全局和语义特征；第二流使用 3D VAE 提取空间和局部详细特征。
- 特征融合：将语义特征和空间特征分别注入到扩散 Transformer 中。语义特征通过额外的交叉注意力层整合到扩散过程中，而空间特征则与噪声视频标记沿通道维度拼接后传递给扩散 Transformer。
- 训练目标：训练过程中，模型学习在潜在空间中重建目标视频，优化标准的扩散 MSE 损失。

3. 工程推理加速优化
- 多步调度：采用 UniPC 多步调度方法加速推理过程。
- 并行化策略：通过 Context Parallel、CFG Parallel 和 VAE Parallel 策略，提高模型在多 GPU 上的推理效率。
- 用户级 GPU 部署：通过模型量化和参数级offload策略，显著降低 GPU 内存消耗，使模型能够在消费级显卡上运行。

4. 评估基准 A2-Bench
- 自动评估指标：提出了一个全面的自动评估框架，涵盖三个主要维度：组合一致性、视觉质量和提示遵循。具体指标包括人物身份一致性、物体一致性、背景一致性、图像质量、审美质量、运动平滑度、动态程度和文本-视频相似度。
- 用户偏好研究：为了补充自动评估，还进行了用户偏好研究，评估生成视频的视觉质量和元素保真度。用户根据详细的评分标准对不同模型的结果进行评分。

通过这些方法，SkyReels-A2 能够生成高质量、多样化的视频，同时保持对参考元素的精确控制，并且为可控视频生成领域提供了一个新的基准。

<h2 id="39.介绍一下HiFiVFS">39.介绍一下HiFiVFS</h2>
HiFiVFS（High Fidelity Video Face Swapping）提出了一个高保真的视频换脸框架，旨在解决现有方法在视频换脸任务中面临的挑战，如身份保持、细节保留和时间一致性等。其主要贡献包括：

1. 细粒度属性学习（Fine-grained Attributes Learning, FAL）
- 属性特征提取：通过编码器Eattr从目标视频中提取属性特征fattr。
- 身份特征融合：将fattr与源图像中提取的身份特征frid融合，生成一个修改后的身份V′t。
- 属性特征的解耦：通过比较Vt和V′t的属性特征的一致性，实现属性特征的解耦。
- 对抗学习：使用对抗学习来提高生成质量，并保留更多的细节。

2. 详细的身份证学习（Detailed Identity Learning, DIL）
- 详细身份特征提取：使用预训练人脸识别模型的最后Res-Block层的特征fdid，而不是全局512维特征fgid，以获取更详细的身份证信息。
- Detailed Identity Tokenizer (DIT)：将fdid转换为token，通过卷积层对齐通道数，然后展平并转置这些特征，每个空间像素作为一个token输入到UNet的交叉注意力和时间注意力模块中。


HiFiVFS框架基于SVD构建，该模型利用3D卷积层、时间注意力层和时间解码器来确保视频的稳定性。HiFiVFS将SVD从图像到视频的任务扩展到视频到视频的任务，使其更适合于换脸应用。

<h2 id="40.介绍一下VividFace">40.介绍一下VividFace</h2>
VividFace试图解决的主要问题是视频人脸交换中的挑战，尤其是在保持时间一致性、身份特征保留、以及处理复杂场景（如大姿态变化和遮挡）时的困难。具体来说，现有方法主要关注静态图像的人脸交换，而在视频中，由于时间维度的复杂性，往往会导致生成结果的不一致性和不自然感。VividFace通过以下几个方面来解决这些问题：

1. 图像-视频混合训练框架：

- 引入一个新颖的图像-视频混合训练策略，充分利用静态图像数据和视频序列数据，以增强训练样本的多样性，并解决仅使用视频训练时样本多样性不足的问题。

2. 特别设计的变分自编码器：
- 结合一个特别设计的 VidFaceVAE，这些模型可以处理静态图像和时序视频数据，以更好地保持生成视频的时间连贯性。

3. Attribute-Identity Disentanglement Triplet (AIDT) 数据集：
- 构建AIDT数据集，其中每个三元组包含三个面部图像，两个图像共享相同的姿态，两个共享相同的身份。这种数据集设计提高了模型在身份和姿态特征解耦方面的能力，增强了身份和属性的保留。

4. 综合遮挡增强：
- 引入遮挡数据增强策略，通过向目标图像添加各种遮挡物和动态时间模式，提高模型对真实世界场景的鲁棒性。

通过这些方法，VividFace框架有效地缓解了视频人脸交换中的关键挑战，包括时间闪烁、身份保留以及对遮挡和姿态变化的鲁棒性。不过关于AIDT数据集的构建仍然存在一些问题，比如在解耦的过程中，AIDT采用了其他换脸方法来生成数据，这可能会导致最终训练上限的提升有限。不过这种思想在数据集构建上是值得借鉴的。

<h2 id="41.介绍一下InstantCharacter">41.介绍一下InstantCharacter</h2>
InstantCharacter试图解决基于文本提示的个性化角色图像生成问题，特别是针对现有方法在生成具有特定身份（如特定人物或角色）的图像时存在的局限性。其主要贡献包括：

1. 可扩展适配器设计（Scalable Adapter Design）
为了有效地将角色特征与现代扩散变换器（DiTs）的潜在空间对齐，论文设计了一个可扩展的全变换器适配器。这个适配器由以下三个部分组成：

- 通用视觉编码器（General Vision Encoders）：使用预训练的大型视觉基础编码器（如SigLIP和DINOv2）提取角色的一般特征。SigLIP在捕捉细粒度角色信息方面表现出色，而DINOv2则增强了特征的鲁棒性，减少了背景或其他干扰因素导致的特征丢失。通过通道级联将这两种特征结合起来，得到更全面的角色表示。
- 中间编码器（Intermediate Encoders）：由于通用视觉编码器的预训练分辨率较低（384），直接输出可能会丢失高分辨率角色图像的细节特征。为了缓解这一问题，论文采用双流特征融合策略，分别探索低级和区域级特征。从通用视觉编码器的浅层直接提取低级特征，捕捉在高层中可能丢失的细节。将参考图像划分为多个不重叠的块，将每个块输入视觉编码器以获得区域级特征。然后通过专用的中间变换器编码器对这两条特征流进行层次化整合，得到综合的融合表示，捕捉多级互补信息。
- 投影头（Projection Head）：将细化的角色特征通过投影头投影到去噪空间，并与潜在噪声进行交互。通过时间步感知的Q-former处理中间编码器的输出作为键值对，同时通过注意力机制动态更新一组可学习的查询。然后通过可学习的交叉注意力层将转换后的查询特征注入去噪空间，从而实现对角色身份的忠实保持和对复杂文本驱动修改的灵活适应。

2. 三阶段训练策略（Three-Stage Training Strategy）
为了有效地训练框架，InstantCharacter构建了一个包含1000万级别样本的大规模角色数据集，并设计了一个三阶段训练策略，以分别优化角色一致性、文本可控性和图像保真度。具体步骤如下：

- 第一阶段：未配对低分辨率预训练（Unpaired Low-Resolution Pretraining）：使用未配对数据进行训练，将角色图像作为参考指导，以重建自身并保持结构一致性。发现使用512分辨率的训练效率显著高于1024分辨率。
- 第二阶段：配对低分辨率训练（Paired Low-Resolution Training）：在低分辨率（512）下继续训练，但切换到配对训练数据。以角色图像作为输入，根据给定的文本描述生成角色在不同动作、姿势和风格下的新场景图像。这一阶段有效地消除了复制粘贴效应，增强了文本可控性，确保生成的图像准确地遵循文本条件。
- 第三阶段：高分辨率联合训练（High-Resolution Joint Training）：使用配对和未配对图像进行高分辨率联合训练。发现有限的高分辨率训练迭代可以显著提高图像的视觉质量和纹理。这一阶段利用高质量图像实现高保真和文本可控的角色图像。

InstantCharacter通过其可扩展的适配器设计和三阶段训练策略，有效地解决了个性化角色图像生成中的挑战，为基于文本提示的角色定制提供了一个强大的解决方案。

<h2 id="42.介绍一下FuseAnyPart">42.介绍一下FuseAnyPart</h2>

FuseAnyPart 的基本思路是通过将不同人物的局部特征进行融合来生成新的形象。这种方法使得合成图像中的五官发生变化，与原图产生实质性差异，并且基于组合原理可以确保生成人脸的多样性。目前相关研究工作已经收录于 NeurIPS 24（Spotlight）中，其大致步骤如下:

1. 面部检测和掩码提取：首先使用一个开放集检测器识别面部图像，以获取各种面部部位（如眼睛、鼻子、嘴巴等）的掩码。
FuseAnyPart首先利用开放集检测模型（例如Grounding-DINO）来生成不同面部器官的掩模，包括眼睛、鼻子、嘴巴等。这些掩模用于标识图像中的不同区域。

2. 特征提取：接着，通过图像编码器利用这些掩码从面部图像中提取相应的局部特征。
通过一个预训练的图像编码器（例如CLIP）提取这些区域的特征。在这里，FuseAnyPart提取的是编码器倒数第二层的未压缩特征图，而非更为抽象的全局特征。这些特征包含更丰富的空间信息和细节，能更好地表示面部局部区域。

3. 基于掩码的融合模块：将提取到的面部部位特征和掩码输入到基于掩码的融合模块，在潜在空间中组合出一个完整的面部。
基于生成的掩模和区域特征，FuseAnyPart在潜在空间中进行特征拼合。具体来说，它将来源图像的某些区域特征替换目标图像中的相应区域。最终融合后的特征被聚合并输入到多层感知器（MLP）中，生成用于指导生成网络的条件特征图。

4. 加法注入模块：整合后的特征被传送到基于加法的注入模块，以便在扩散模型的 UNet 结构中进行进一步的融合。
传统的基于交叉注意力的融合机制通常在将多模态特征（如文本和图像）融合时有效，但在对齐细粒度的面部区域特征时表现欠佳。因此，FuseAnyPart引入了基于加法的注入模块，以直接将融合后的图像特征注入到扩散模型的UNet网络中。


这种技术使得生成的人脸具有高度多样性和自然感。通过在潜在空间中的灵活组合和调整，可以创造出新的面部形象，从而有效解决人脸多样性的问题，并为人物驱动提供高质量、多样化的人脸素材。经过 FuseAnyPart 的融合处理，可以得到清晰、自然的合成人脸图像，并且与一些基线方法相比表现出更佳的效果。此外，FuseAnyPart 在跨种族和跨年龄样本的合成上也展示出了不错的表现，能够生成多样化的人脸图像。

<h2 id="43.介绍一下OmniHuman-1">43.介绍一下OmniHuman-1</h2>

OmniHuman模型是一种混合条件的人类视频生成模型。它利用全条件训练策略来整合各种与运动相关的条件及其相应的数据。与现有减少数据的严格过滤方法不同，OmniHuman从大规模混合条件数据中受益。生成了高度真实和生动的人类运动视频，同时支持多种模式。它在不同的肖像和输入纵横比下表现良好。OmniHuman显著改善了手势生成，这是以前端到端模型的挑战，并支持各种图像风格，显著优于现有的音频条件人类视频生成方法，而且这在其发布时也是全身效果最好方法，拆解其技术细节如下：

1. 音频条件注入
关于音频条件，采用wav2vec模型提取声学特征，随后使用MLP进行压缩，以匹配MMDiT的隐藏大小。每个帧的特征与相邻时间戳的音频特征连接，生成当前帧的音频标记。这些音频标记通过交叉注意注入到MMDiT的每个块中，实现音频标记与噪声潜在表示之间的交互。

2. Pose条件注入
为了结合姿态条件，OmniHuman使用姿态引导器对驱动姿态热图序列进行编码。得到的姿态特征与相邻帧的特征连接，获得姿态标记。然后将这些姿态标记与噪声潜在沿通道维度堆叠，并输入到统一的多条件扩散模型中，以实现视觉对齐和动态建模。

3. 参考图像条件注入
这里OmniHuman并没有使用ReferenceNet的结构来进行参考图像注入，而是在原始的去噪MMDiT输入层面进行注入，通过自注意力机制将参考图像的特征与噪声潜在表示进行交互。

4. 训练策略
多条件设计的优势在于，可以将模型训练划分为多个任务，包括图像和文本到视频、图像和文本、音频到视频以及图像和文本、音频、姿态到视频。在训练过程中，不同的模式针对不同的数据激活，允许更广泛的数据参与训练过程，增强模型的生成能力。在传统的文本到视频预训练阶段之后，OmniHuman遵循两个训练原则来扩展条件化人类视频生成任务。原则1:较强的条件任务可以利用较弱条件任务及其对应数据来实现模型训练过程中的数据扩展。原则2条件越强，使用的训练比例应越低，以确保模型在混合条件训练中充分利用每种运动相关条件的优势，并学习它们的运动生成能力。这种条件的混合训练策略也使OmniHuman能够在训练过程中充分利用每种运动相关条件的优势，并学习它们的运动生成能力。OmniHuman在训练过程中使用了两种不同的条件：文本和音频。文本条件用于指导生成的内容，而音频条件则用于控制生成的动作和表情。OmniHuman通过将文本和音频条件结合起来，能够生成更自然、更生动的人类运动视频:

- 阶段1：仅使用文本+图像条件（利用全部数据）
- 阶段2：加入音频条件（50%数据含音频标注）
- 阶段3：加入姿态条件（20%数据含姿态标注）

OmniHuman通过创新的多条件混合训练策略，突破了端到端人体动画模型的数据瓶颈。整体的训练数据规模达到了18.7小时，是同类模型的10倍以上。可以去项目主页中看到很多演示视频，展示其在唱歌、对话、物体交互等场景的实际效果，生成的结果非常自然流畅，且在不同的输入纵横比下表现良好。OmniHuman在手势生成方面也有显著改善，支持多种图像风格，展现了其强大的生成能力和灵活性。

<h2 id="44.介绍一下StableAnimator">44.介绍一下StableAnimator</h2>

StableAnimator试图解决的问题是人像图像动画生成中的身份一致性问题。具体来说，当前的扩散模型在处理表现出显著运动变化的姿势序列时，常常会在面部区域产生显著的扭曲和不一致性，从而破坏身份信息。为了解决这个问题，论文提出了一个名为StableAnimator的端到端的身份保持视频扩散框架，该框架能够在没有任何后处理的情况下，基于参考图像和一系列姿势合成高质量且保持身份一致性的视频。其关键技术如下：

1. 全局内容感知面部编码器
利用现成的提取器分别获取参考图像的面部嵌入和图像嵌入。面部嵌入通过全局内容感知面部编码器进一步细化，增强面部嵌入对参考图像整体布局（如背景）的感知。

2. 分布感知ID适配器
将面部编码器的输出送入ID适配器进行进一步对齐，避免直接将图像域的身份保持方法整合到视频扩散模型中时发生的空间特征失真。在每个去噪步骤中，ID适配器基于特征分布对细化的面部嵌入和扩散潜在表示进行对齐，有效避免特征失真。

3. HJB方程基础的面部优化
在推理过程中，为了进一步增强面部质量和减少对后处理工具的依赖，StableAnimator通过解决Hamilton-Jacobi-Bellman (HJB)方程来进行面部优化。将HJB方程的解决过程整合到扩散去噪过程中，允许一个可控变量指导和限制去噪过程的方向，从而优化身份一致性。是一种即插即用的优化方法，能够在不需要额外训练的情况下，增强生成视频的面部质量。

StableAnimator为高质量、身份一致的人类图像动画生成提供了一个创新的解决方案，并在理论和实践上都进行了深入的探讨和验证，可以参考一下基于HJB方程的面部优化方法。

<h2 id="45.介绍一下Sonic">45.介绍一下Sonic</h2>

Sonic试图解决在音频驱动的肖像动画生成（Talking Face Animation）领域中，如何更有效地利用音频信号来生成高质量、自然且时间连贯的动画视频的问题。具体来说，它关注以下几个关键问题：

1. 音频与视觉的协调性：现有的方法在处理音频与视觉信息的协调方面存在不足，往往将音频特征与视觉帧进行简单匹配，而忽略了音频信号作为全局信号在调整面部表情和口型运动中的潜力。这导致了生成的动画在自然性和时间一致性上存在缺陷。

2. 全局音频感知的缺失：当前的方法主要依赖于局部音频信息或视觉运动（如运动帧）来稳定动画，而没有充分利用音频信号的全局特性，如语调、节奏和语速，这些特性可以为面部表情和头部运动提供隐含的先验知识。

3. 长时间视频生成的稳定性：在生成长时间视频时，现有的方法难以保持稳定性和连贯性，因为它们缺乏有效的全局音频感知机制来指导跨片段（clip）的运动一致性。

4. 动作多样性和自然性：现有的方法在生成多样化和自然的面部表情及头部运动方面存在局限，因为它们没有充分利用音频信号中的信息来驱动这些运动。

其主要解决方法如下所示：

1. 上下文增强的音频学习
使用 Whisper-Tiny 作为预训练的音频特征提取模型，提取多尺度的音频特征。对于每个视频帧，使用0.2秒的音频特征来提供丰富的上下文。通过三个线性层将音频特征投影到与交叉注意力维度匹配的维度。在空间交叉注意力层中注入音频信号，以指导空间特征的生成。

2. 运动解耦控制器
引入独立的增强运动桶参数（motion-bucket parameters），分别控制头部运动和表情运动的幅度。在训练阶段，通过计算视频剪辑中边界框的方差来确定平移运动，通过计算相对地标记的方差来确定表情运动。在推理阶段，通过音频和参考图像的 CLIP 嵌入预测运动桶，并通过比例因子 β 调整动态程度。

3. 时间感知位置偏移融合
在每个时间步，模型从一个新的位置开始处理音频片段，通过累积偏移 α来平滑上一个时间步的片段。使用循环填充策略处理序列末尾的索引超出长度的问题。这和之前例如MimicMotion或其他Comfyui中的长视频推理框架不同，Sonic通过累计的偏移来平滑不同时间步数之间的overlap，避免了overlap处会产生阵间一致性不佳的问题。

Sonic 通过全局音频感知显著提高了音频驱动的肖像动画生成的质量和自然性，在HDTF数据集中达到较好的客观指标FID = 29.104，FVD = 301.173，Smoothness = 0.9970，其方法在音频驱动的肖像动画生成领域具有重要的应用潜力和研究价值。面试中可以对比时间感知位置偏移融合和MimicMotion的不同之处，总结长视频推理的不同实现方法。

<h2 id="46.如何对音频驱动人脸数据进行预处理">46.如何对音频驱动人脸数据进行预处理</h2>

1. 移除损坏的视频文件：
对数据集中的视频文件进行检查，过滤掉那些无法正常播放或文件损坏的视频，以确保后续处理能够顺利进行。
2. 重采样视频帧率至25 FPS，音频重采样至16000 Hz：
将视频的帧率统一调整为25帧/秒（FPS），使视频在时间和空间上具有统一的采样间隔，方便后续的帧级处理。
同时，将音频的采样率调整为16000赫兹（Hz），这有助于与视频帧进行同步处理，并且16kHz的采样率能够在保证音频质量的同时，减少计算量和存储空间。
3. 使用PySceneDetect进行场景检测：
场景检测的目的是识别视频中场景的变化点，以便将视频分割成逻辑上连贯的片段。PySceneDetect是一个开源的场景检测工具，它提供了多种算法来检测视频中的场景变化，例如基于帧间差异、内容感知等方法。
4. 将每个视频分割成5-10秒的片段：
根据场景检测的结果，将原始视频分割成多个5到10秒长的片段。这种分割方式有助于模型在训练时更好地学习局部的音视频特征，并且可以提高训练效率。
5. 根据InsightFace检测到的面部关键点进行仿射变换，然后将面部大小调整为256 × 256像素：
首先使用InsightFace（一种面部检测和识别工具）来检测视频中人脸的关键点（如眼睛、鼻子、嘴巴等的位置）。然后，根据这些关键点进行仿射变换，将人脸对齐到一个标准的正面视图，以便模型能够更好地学习面部特征。接下来，将处理后的人脸图像大小调整为256 × 256像素，这一步是为了统一输入数据的尺寸，使模型能够接收固定大小的输入，同时也符合模型对输入尺寸的要求。
6. 移除同步置信度分数低于3的视频，并将音频-视频偏移量调整为0：
使用SyncNet（一种用于评估音频和视频同步程度的工具）来计算每个视频片段的同步置信度分数，移除那些分数低于3的视频，因为这些视频可能存在明显的音频与视频不同步的问题，会影响模型的训练效果。对于保留的视频片段，需要调整音频和视频的偏移量，使其达到同步状态（偏移量为0），以确保音频和视频之间的时间对齐，这对于唇形同步任务来说是至关重要的。
7. 计算HyperIQA分数，并移除分数低于40的视频：
HyperIQA是一种用于视频质量评估的工具，它可以对视频的视觉质量进行打分。通过计算每个视频片段的HyperIQA分数，移除那些分数低于40的视频，即视觉质量较差的视频，这样可以保证训练数据的质量，使模型能够学习到更清晰、更高质量的音视频特征。

<h2 id="47.如何将REPA思想应用在唇形对齐中">47.如何将REPA思想应用在唇形对齐中</h2>

首先REPA是谢赛宁团队在ICLR2025发表的一种更高效训练扩散模型的方法，其思想是通过大规模自监督预训练模型在扩散模型浅层特征进行对齐，来提升扩散模型的训练效率和生成质量。那么在唇形对齐中，我们可以借鉴REPA的思想，达到更好的时序一致性，具体步骤如下：

1. 特征提取：使用大规模自监督视频模型（如 VideoMAE-v2）分别对生成的唇形视频序列和真实唇形视频序列进行特征提取。该模型经过在大规模未标注数据集上的训练，其输出的时间表征具有很强的泛化能力、鲁棒性和高信息密度。

2. 特征对齐：通过对比损失函数（如对比损失或三元组损失）来对齐生成的唇形视频序列和真实唇形视频序列的特征表示。通过最小化生成序列和真实序列之间的特征距离，来增强生成序列的时序一致性。

在实际应用中，还可以结合其他损失函数（如唇形同步精度损失、视觉质量损失等）共同训练模型，以实现唇形对齐的多方面优化。

<h2 id="48.如何实现精确的人脸条件控制">48.如何实现精确的人脸条件控制</h2>

在大多数工作中，人脸条件控制通常是通过将参考图像的特征编码为一个或多个标记，然后在扩散模型的去噪过程中使用这些标记来指导生成。然而，这种方法可能会导致生成结果与参考图像之间存在一定的差异，尤其是在细节和身份特征方面。以换脸为例，目前已有的工作尝尝会将人脸的条件粗粒度的分为身份信息和运动信息，但这种方法可能无法精确地控制生成结果中的细节特征，运动信息的控制也可能不够精细。为了解决这些问题，可以通过精准分离控制不同属性的方式来实现更精确的人脸条件控制。具体步骤如下：

1. 背景保留条件：通过掩码分离背景（$I_{bg} = (1 - M_{swap}) \circ I_{tgt}$）；这样在生成特定人脸的同时保留背景信息，避免生成结果中出现不自然的背景变化。

2. 形状感知法线图：结合源人脸身份形状（$\alpha_{src}$）与目标姿态（$\beta_{tgt}$）的3DMM参数，渲染对齐的2D法线图（$I_{normal}$）。这种方法可以通过3D解耦的形式确保生成的人脸在形状上与目标人脸保持一致，同时保留源人脸的身份特征。

3. 表情相关标志点：提取目标表情关键点并转换为语义分割图（$I_{exp}$），移除身份相关特征（如嘴唇厚度）；通过这种方式，可以更精确地控制生成结果中的表情变化，同时保持身份特征的稳定性。

4. 身份擦除光照图：使用UV纹理图来代表光照信息，并对目标UV纹理模糊化，保留光照但破坏身份信息。

这样的多条件注入方法可以有效地提高人脸生成的质量和一致性，同时为后续的精细化调整提供了更大的灵活性。

<h2 id="49.视频人脸交换中的时间一致性如何保障">49.视频人脸交换中的时间一致性如何保障</h2>

视频人脸交换中的时间一致性是指在连续帧中，生成的人脸在外观、表情、光照等方面保持平滑、自然的过渡，避免出现闪烁、跳变或失真。常见的保障方法包括：

1. **时序建模**：采用3D卷积、时序Transformer或时序注意力机制，在模型结构中显式建模帧间依赖，提升生成结果的连贯性（参考AnimateDiff）。

2. **帧间一致性损失**：在训练时引入帧间一致性损失（如光流一致性、特征一致性、感知一致性等），约束相邻帧的生成结果在空间和语义上保持一致。

3. **运动引导与对齐**：利用人体/人脸关键点、光流或姿态估计等运动信息，辅助模型理解和对齐帧间的运动变化，减少错位和漂移。

4. **时序数据增强**：通过视频片段的随机裁剪、时序扰动等方式增强训练数据，提高模型对时序变化的鲁棒性。

5. **后处理平滑**：在推理阶段对生成视频进行时序滤波、插帧或基于光流的后处理，进一步消除残余的闪烁和不连续。


<h2 id="50.如何实现高保真身份注入">50.如何实现高保真身份注入</h2>

目前较为主流的身份注入方法主要有两种：Q-former注入方式和ReferenceNet注入方式。

1. Q-former注入方式
Q-former是一种用于跨模态特征对齐的模块，常见于图像与文本、身份特征与扩散模型之间的交互。其核心思想是引入一组可学习的查询（Query），通过交叉注意力机制与外部特征（如身份嵌入、视觉特征等）进行交互，生成适合注入主干模型的特征表示。


2. ReferenceNet注入方式
ReferenceNet是一种专门为捕获参考图像空间细节而设计的网络结构，常用于保持外观一致性。其典型流程如下：

- 特征提取：ReferenceNet以UNet结构为主，输入参考图像，输出多尺度空间特征图。
- 特征融合：在扩散模型的UNet主干中，将ReferenceNet输出的特征通过拼接、加权融合或专用的参考注意力层与主干特征结合。

两种方式的主要区别在于Q-former注入方式更侧重于跨模态特征对齐，通过可学习的查询动态生成特征表示，而ReferenceNet注入方式则更注重空间细节的保持，通过专门设计的网络结构直接提取和融合参考图像的空间特征。在实际应用中，选择哪种方式取决于具体任务的需求和模型架构的设计。例如，如果需要更灵活的跨模态特征对齐，Q-former可能更合适；如果需要保持高保真的空间细节，ReferenceNet则可能更有效。

<h2 id="51.如何解决长视频生成的闪烁问题">51.如何解决长视频生成的闪烁问题</h2>
长视频生成中的闪烁问题通常是由于模型在处理长时间序列时，缺乏足够的上下文信息和一致性约束，这种case主要在使用AnimateDiff这种2.5D方法或者在context length较小情况下时发生，导致生成的帧之间存在明显的视觉不连续性。为了解决这个问题，可以采用以下几种方法：

1. 分段融合
将长视频分块处理，相邻块重叠$O$帧；按距离中心帧的权重混合潜变量（$\hat{\mathbf{z}}^{(i)} = w_i \hat{\mathbf{z}}_{S_k}^{(i)} + (1-w_i) \hat{\mathbf{z}}_{S_{k+1}}^{(i)}$）。这样就可以减小在Overlap区域的闪烁问题。

2. 时序一致性损失
在训练过程中引入时序一致性损失（如光流一致性、特征一致性等），约束相邻帧之间的生成结果在空间和语义上保持一致。这可以通过计算相邻帧之间的特征距离或视觉差异来实现。

3. TV正则化
在每步去噪后添加梯度约束：$\hat{\mathbf{z}}_{t-1} \leftarrow \hat{\mathbf{z}}_{t-1} - \lambda_{TV} \nabla \| D_z \hat{\mathbf{z}}_{t-1} \|^2$；显示的约束梯度平滑性，减少生成结果的闪烁。但这种方式也有概率使生成的结果变得模糊，因此需要在平滑性和细节保留之间进行权衡。

<h2 id="52.如何在人脸或人体生成中避免训练时条件之间的信息冗余">52.如何在人脸或人体生成中避免训练时条件之间的信息冗余</h2>

在训练人脸或人体生成模型时，避免条件之间的信息冗余是一个重要的挑战。信息冗余可能导致模型过拟合、生成结果缺乏多样性或细节。以下是一些常用的方法来减少信息冗余：
1. 条件解耦：在模型设计中，采用条件解耦的方式，将不同的条件（如身份、表情、姿态等）分离处理。可以使用多任务学习框架来实现条件的独立建模。

2. 条件选择性注入：在模型的输入层或中间层，使用选择性注入机制，只在需要时注入特定条件的特征。这可以通过注意力机制或门控机制来实现，确保只有相关的条件信息被传递到模型中。

3. 动态丢弃：在训练中随机屏蔽某些条件，强制模型从其他条件学习互补信息；这种思想类似于Dropout，但也应用于条件特征。

4. Classifier-free Guidance：在生成过程中使用类似Classifier-free Guidance的技术来丢掉某一条件，通过调整条件强度来控制生成结果的多样性和细节。这种方法可以在不引入额外冗余信息的情况下，增强模型对特定条件的响应。

<h2 id="53.如何通过背景处理解决源-目标人脸形状不匹配问题">53.如何通过背景处理解决源-目标人脸形状不匹配问题</h2>

当源人脸与目标人脸脸型差异大时（如方脸→瓜子脸），传统方法直接复用目标掩码会导致：

边缘伪影：生成人脸轮廓与背景衔接生硬；
身份污染：模型学习到目标脸型特征，影响源身份保留；
遮挡错误：头发、耳朵等区域因形状错位暴露拼接痕迹。

为了解决这些问题，我们可以分别从训练和推理阶段来进行分析：

1. 训练阶段：增强泛化能力

使用人脸解析器分割目标面部区域 $M_{tgt}$ 和遮挡区域 $M_{occ}$；对 $M_{tgt}$ 施加随机偏移生成 $M_{tgt}^{shift}$，强制模型学习泛化背景修复（避免过拟合目标脸型）；

2. 推理阶段：形状对齐融合

用形状对齐的法线图掩码 $M_{tgt}^{normal}$ 替代 $M_{tgt}^{shift}$（融合源身份形状与目标位置）；最终掩码：$M_{swap} = (M_{tgt} \cup M_{tgt}^{normal}) \cap (1 - M_{occ})$；背景注入：$I_{bg} = (1 - M_{swap}) \circ I_{tgt}$ 作为条件输入。

<h2 id="54.虚拟换衣有哪些技术难点">54.虚拟换衣有哪些技术难点</h2>

1. 数据采集和处理：高质量的3D人体和服装数据获取难度大，且处理复杂。如何高效地采集和处理这些数据是一个亟待解决的问题。
2. 实时性：实现流畅的实时试穿效果需要强大的计算能力。当前的计算资源限制了虚拟试衣技术的实时应用。
3. 细节表现：高精度地模拟服装的材质、纹理和物理特性仍具有挑战性。特别是对于一些复杂面料的表现，现有技术仍有改进空间。
4. 用户接受度：如何提升用户的使用意愿和信任度也是一个关键问题。

<h2 id="55.虚拟换衣的技术路线有哪些">55.虚拟换衣的技术路线有哪些</h2>

基于图像的虚拟换衣方法可以分为两种主要方法：基于掩码的方法和无掩码的方法。主要区别在于在推理过程中是否需要二进制掩码来指定个人图像上应该进行试穿的区域以及哪些区域应该保持不变。

- Mask-based方法：虚拟换衣中基于掩码的方法将任务视为图像掩码-inpainting问题，其中一个人的试戴区域使用参考服装进行内绘。早期方法利用专用图像Warping模块在人类姿势解析特征的指导下，使服装图像变形以适应试穿区域。这些模块需要准确的人类姿势特征来有效地扭曲衣服。随着扩散模型在图像生成方面的快速发展，最近的方法使用预先训练的编码器对服装图像和人类姿势进行编码，在掩蔽区域内涂装之前，将服装与隐藏空间的人类姿势对齐。然而，由于不准确的遮罩估计，这些方法会导致原始图像内容丢失，并严重依赖人类姿势解析器的性能。有些方法通过额外的掩码预测步骤减少掩码区域来减轻信息丢失，但其预测掩码的质量仍然不稳定，并且无法完全解决掩码造成的问题。

- Mask-free方法：无掩码的虚拟换衣方法不依赖于二进制掩码，而是通过将服装图像与人类姿势特征对齐来生成试穿图像。这些方法通常使用预训练的编码器来提取服装和人类姿势的特征，并在隐藏空间中进行对齐。由于不需要掩码，这些方法可以更好地保留原始图像内容，并且不受掩码质量的影响。然而，这些方法仍然面临着如何准确对齐服装和人类姿势特征的问题。
相比之下，无掩码方法在推理过程中不需要掩码作为输入。它们是专门为减轻解析器的负面影响而设计的。这些方法首先训练基于掩码的教师模型，然后使用教师模型生成的结果提炼一个无掩码的学生模型。然而，知识蒸馏不仅将基于掩码的模型的缺陷转移到学生网络上，而且还限制了学生网络在野外场景中的推广能力。所以为了使模型能够在野外场景中保留复杂的非试穿内容，如何采集和构造更加真实的三元组数据集是一个关键问题。

<h2 id="56.Mask-based虚拟换衣方法中掩码引入的形状信息如何处理">56.Mask-based虚拟换衣方法中掩码引入的形状信息如何处理</h2>

现有的跨类别试穿研究经常遇到体形渲染不准确的问题。这个问题的出现是因为之前的方法通常严格基于人体轮廓解析来构建不可知的掩码，这种掩码构建策略可能会导致训练过程中服装体形信息的泄露，从而导致模型在推理过程中倾向于填满整个掩码区域。

为了缓解这个问题，FitDit提出了一种扩张-放松掩模策略来解决形状泄漏问题，使模型能够在训练过程中自动学习目标服装的最佳长度。具体来说，对于上身试穿，导出能够同时包含手部关键点和人体对上身服装解析的最小外接矩形。对于下身试穿，利用足部关键点和人体对下身服装解析构建了不可知掩码。对于连衣裙，同时考虑手部和足部关键点以及人体对连衣裙的解析。此外，为了增强模型对服装形状的感知，将掩模的边缘随机扩展几个像素，以覆盖部分不变的区域。即尽可能的扩大掩码区域，保证不泄漏大致的原衣服形状，但又不需填充过多的区域。

<h2 id="57.虚拟换衣如何构造三元组数据">57.虚拟换衣如何构造三元组数据</h2>


1. 使用目前已有的Mask-based方法构造：通过整合多个公共数据集和从互联网上爬取的图像对，并使用辅助工具如Automasker和segment model生成遮罩，然后利用FLUX-Controlnet-Inpainting进行图像修复，以此来扩充数据集。

2. 使用目前效果较好的图像编辑模型：如GPT-4o, Flux-Kontext筛选出真实性强、质量高且与试穿模特图像姿势一致性高的图像三元组（试穿模特图像、衣物图像、修复后的输入模特图像）。

<h2 id="58.虚拟换衣如何筛选优质的训练数据">58.虚拟换衣如何筛选优质的训练数据</h2>

在虚拟换衣任务中，筛选优质的训练数据是确保模型性能的关键。以下是一些常用的方法来筛选高质量的训练数据：
1. 人体姿态的一致性：用 CLIP-I 计算 (I, M) 的 pose 一致性 > 0.82；
2. 衣服纹理一致性：用 DISTS 计算 (I, C) 在衣服区域的纹理一致性 < 0.25；
3. 人脸一致性：用 face-recognition 库检测人脸关键点，确保面部未被误修复；
4. 人工抽检：每 500 张抽 30 张，重点看手指、耳朵、项链是否被误涂掉。

<h2 id="59.虚拟换衣常用的数据集有哪些">59.虚拟换衣常用的数据集有哪些</h2>

1. VITON-HD：纯白背景，品类包含上衣，共13679 件衣物和 27358 张人模图像，分辨率为1024 × 768，收录于 CVPR 2021。
- 论文链接：https://arxiv.org/abs/2103.16874
- 数据集链接：https://github.com/shadow2496/VITON-HD?tab=readme-ov-file#dataset

2. FashionTryOn：28714个三元组，每个三元组包含一个服装项目图像和两个不同姿势的模特图像，共计 86142 张图像，分辨率为256 × 192，收录于ACM Multimedia 2019。
- 论文链接：https://zhennaziyu.github.io/homepage/fp452-zhengA.pdf
- 数据集链接：https://fashiontryon.wixsite.com/fashiontryon

3. StreetTryOn：大多为街景背景，从大型时尚检索数据集 DeepFashion2 中筛掉超过 90% 的不适用于试穿任务的图像（例如非正面视图、大遮挡、黑暗环境等），由 12364 张用于训练的街头人物图像和 2089 张用于验证的街头人物图像组成。
- 论文链接：https://arxiv.org/pdf/2311.16094.pdf
- 项目链接：https://cuiaiyu.github.io/StreetTryOn/

4. DressCode：纯白背景，包含53792 件衣物和 107584 张对应的人模图像，品类多样，包含上衣、下装、裙子，分辨率为1024 × 768，是目前品类较为丰富的换衣数据集

- 论文链接：https://arxiv.org/abs/2204.08532
- 数据集链接：https://github.com/aimagelab/dress-code


<h2 id="60.如何通过文本更好的控制衣服尺寸">60.如何通过文本更好的控制衣服尺寸</h2>
1. 通过大型多模态模型VLM打标，把“尺寸”这一概念显式地写进文本提示；
2. 让扩散模型在生成时把这些文本token当作条件信号，如使用CLIP模型的文本编码器将文本提示转换为向量表征；
3. 通过提示感知遮罩确保只修改衣服区域，而不破坏人物本身。

| 步骤                 | 操作细节                              | 例子                                                                                      |
| ------------------ | --------------------------------- | --------------------------------------------------------------------------------------- |
| ① 用户输入             | 直接写一句自然语言                         | “把这件 T 恤改成 oversize，下摆盖到臀部”                                                             |
| ② VLM 重写           | 通过VLM(QwenVL, LLaVA)的上下文学习把原句转换成结构化 prompt | `“upper_body, white T-shirt, oversized fit, longline hem covering hips, loose sleeves”` |
| ③ PromptDresser 编码 | 文本编码器把 prompt 变成条件向量              | `oversized fit`、`longline hem` 等 token 获得高注意力权重                                         |
| ④ 遮罩粗→精            | PMG 先用粗遮罩把整件衣服包住，再精细收缩到实际边界       | 保证只有衣服被重绘，人脸/背景完全保留                                                                     |
| ⑤ 扩散生成             | 在衣服遮罩内执行 inpainting；条件向量指导形状、比例   | 扩散过程把原紧身体恤逐渐去噪成宽松长版                                                                     |


| 想达到的效果 | 推荐 prompt 片段                                        |
| ------ | --------------------------------------------------- |
| 更紧身    | `slim fit, body-hugging, cropped length`            |
| 更宽松    | `oversized fit, boxy silhouette, dropped shoulders` |
| 加长     | `longline, extended hem, tunic length`              |
| 缩短     | `cropped, waist-length, exposed midriff`            |
| 袖子变长   | `long sleeves, oversized sleeves, covering wrists`  |
| 整体放大一号 | `one size up, relaxed fit, loose throughout`        |



<h2 id="61.如果训练数据里没有“卷袖子”的标注，怎么教会模型让生成人像的衣服卷袖子">61.如果训练数据里没有“卷袖子”的标注，怎么教会模型让生成人像的衣服卷袖子</h2>
用无监督或弱监督思路：

1. 随机在衣服掩码内部采样一批起点，再在袖子区域采样一批终点，用光流或简单弹性形变自动生成伪标签。
2. 训练时加一致性损失：同一张图做两次随机采样，两次输出在空间上保持平滑一致，防止过拟合噪声。
3. 上线前做少量人工精标微调（几百张即可），显著提高真实感。

<h2 id="62.模型在512×384上训练，线上要跑1024×768，如何防止衣服纹理糊掉">62.模型在512×384上训练，线上要跑1024×768，如何防止衣服纹理糊掉</h2>

1. 先用超分网络把衣服图单独放大 2×，再上采样到目标分辨率，保证纹理像素充足。
2. 换衣网络内部使用多尺度判别器：低尺度判别整体结构，高尺度判别纹理细节，迫使网络在放大后仍保留高频信息。
3. 训练时随机尺度增广（0.8–1.2 倍），让网络对不同分辨率鲁棒。

<h2 id="63.发现生成人像的头发经常盖在新换的衣服上，怎么修复">63.发现生成人像的头发经常盖在新换的衣服上，怎么修复</h2>

1. 加一道“头发分割”小模型，推理前先把头发区域扣出来。
2. 换衣网络在贴衣服时把头发区域 mask 掉，最后再贴回头发层，避免穿透。
3. 如果头发边缘硬切明显，再加边缘羽化 + Poisson Blending 做后处理，让过渡自然。

<h2 id="64.如果只能用文字描述，想让一件紧身上衣在视觉上变宽松，该如何实现">64.如果只能用文字描述，想让一件紧身上衣在视觉上变宽松，该如何实现</h2>

最关键的是把“宽松”这一概念量化到扩散模型的条件空间。做法大致分两步：
1. 用 LLM 把口语“宽松”翻译成可解析的 attribute token（如 loose-fit、drop-shoulder、+10 cm ease）。
2. 在扩散阶段，把这些 token 的 embedding 与 U-Net 的 cross-attention map 对齐，让“宽松”信号只在衣服 mask 内生效。如果第 2 步没对齐，模型会把“宽松”当成全局风格，连背景都被拉变形。所以这里的mask引入了形状信息，确保只在衣服区域生效。

<h2 id="65.如何评估文本-尺寸的忠实度">65.如何评估文本-尺寸的忠实度</h2>

把衣服区域里每一个像素对“oversize”这个词的 cross-attention 强度 取出来，再归一化到 0-1，就得到一张灰度图：越亮的地方表示文本对“放大”越敏感。

| 指标      | 公式                               | 直观含义                              |
| ------- | -------------------------------- | --------------------------------- |
| **SRR** | Σ(PSRM>τ) / Total\_cloth\_pixels | 有多少比例的衣服面积被文本真正“触及”到。  |
| **SRF** | Σ(PSRM²) / Σ(PSRM)               | 响应是否集中在衣服中央。越接近 1 越集中；如果分散到背景就会明显下降。 |

<h2 id="66.音频驱动多人物视频生成有哪些难点">66.音频驱动多人物视频生成有哪些难点</h2>

1. 多流音频输入：每个说话者对应一个音频流，模型必须同时处理并保持对齐；传统单流方法往往把所有音频信息混合，导致所有人同时张嘴。
2. 音频—人物绑定：确保第 i 个音频只驱动第 i 个角色的嘴型/表情/动作；若绑定失败会出现多人同时发声或音画错位。
3. 动态人物定位：参考图像中人物会移动或变形，需要模型动态找到视频 latent 中对应人物区域以便局部驱动。
4. 指令/提示跟随：在文本提示描述复杂动作（如“左侧人物挥手”）时，模型需要保持原有的提示响应能力，同时加入音频驱动。
5. 时序与长视频生成：长视频需处理累积误差与帧间一致性。

<h2 id="67.MultiTalk的整体架构是什么">67.MultiTalk的整体架构是什么</h2>

1. 基础视觉骨干：DiT-based image→video diffusion + 3D-VAE
	- DiT（Diffusion-in-Transformer）作为视频生成 backbone，3D-VAE 在时空上做压缩，降低 video latents 的时序长度和计算量。
2. 文本条件与 CLIP global context
	- 文本经文本编码器；CLIP 图像编码器提取参考图像的全局上下文，通过 decoupled cross-attention 注入到 DiT。
3. 音频条件路径（audio pipeline）
	-	用 Wav2Vec 提取音频 embedding；使用 audio adapter 做时间压缩 / 对齐，然后把音频以 cross-attention 的 keys/values 注入到 DiT（queries 来源于 video latents）。
4. 多流注入
	- 在每个 DiT block 的 text-cross-attention 之后加入 audio-cross-attention 层，并通过 L-RoPE（Label Rotary Position Embedding）实现多流音频与人物区域的绑定。
5. 训练策略
	- 保留基础模型的 instruction-following 能力，通过只微调 audio-cross-attention & adapter（partial parameter training）与混合 I2V/AI2V 任务训练来保持能力。

<h2 id="68.介绍一下L-RoPE(Label Rotary Position Embedding)">68.介绍一下L-RoPE(Label Rotary Position Embedding)</h2>

直观核心想法：把标签信息（指示某个 token 属于哪位人物/背景）融入到 rotary position embedding 中，使得带有同一标签的 audio embeddings 与 video latent queries 在 cross-attention 中天然匹配，从而“激活”该人物的 attention 区域，解决音频->人物绑定问题。RoPE（rotary positional embedding）通过对 query/key 向量做相位旋转把位置信息编码进内积相似度。L-RoPE 将位置角度替换/扩展为标签角度，从而把标签信息变为 attention 权重的调制条件。

实现要点：
1.	给 video latent 的每个 token 分配一个 label l_i（例如 person1 在 latent 中的一个范围被赋到 [a,b]）。
2.	给对应的音频帧（或音频 token）分配相同/相近的 label l_j。
3.	在计算 cross-attention 的 Q/K 前，对 Q（来自 video）和 K（来自 audio）分别做基于标签的旋转：Q_rot = RoPE(Q, angle = base_angle * l_i)，K_rot = RoPE(K, angle = base_angle * l_j)。
4.	这样 Q_rot·K_rot 的相似度在 label 相近时被放大，从而在 attention map 上点亮对应 person 区域。

<h2 id="69.如何实现自适应人物定位生成">69.如何实现自适应人物定位生成</h2>
背景：reference image 中人物位置可能与生成过程中 latent token 的位置并不一一对应（人物可能移动、镜头变换等），直接用静态 bbox 无法可靠定位 latent 中对应区域。

解决方案：
1. 利用 DiT 的 reference-image-to-video self-attention map：在 I2V（image→video）模型中，第一帧（reference）与后续视频帧在 self-attention 中已经建立了相似性分布。
2. 计算相似度矩阵 S，其中 S[t, p] 表示 video latent 第 t 个 token 与 reference 图像中第 p 个 subject（或 mask 区域）的相似性（按 attention map 平均化得到）。
3. 对每个 token，基于 S 的最大或平均响应分配属于哪位人物，从而得到每个 token 的 label（用于 L-RoPE）。论文里把 background 也作为一个 label。 

<h2 id="70.多人音频驱动有哪些音频注入方式">70.多人音频驱动有哪些音频注入方式</h2>

1.	拼接音频 embeddings直接把多路音频在 time/channel 拼接后供 attention。
2.	分别 attention 后相加 对每一路音频单独 cross-attention，再把结果相加。
3.	按左右分割 video latent 把 video latent 左右切分，各自 attention 对应 audio。
4.	L-RoPE label化 + rotary embedding。

前两种方式效果较差，因为网络难以在训练中自发学出把第 i 路 audio 丢给第 i 段 latent这样的binding；attention 没有额外的结构化信号来指示哪个音频应该影响哪个空间区域，结果是音频倾向于驱动所有人物。

<h2 id="71.音频驱动多人物视频生成有哪些应用场景">71.音频驱动多人物视频生成有哪些应用场景</h2>

应用场景一：影视与动画制作
	- 动画电影与游戏动画
快速生成多角色对话场景，节省绘制、合成、演员拍摄等成本，同时保证 lip-sync 与互动自然度
	- 影视前期可视化
导演和团队可用 AI 快速预览对话场景的情绪与动作，引导正式拍摄或调整剧本结构  ￼ ￼

应用场景二：教育与培训
	- 在线课堂或语言教学
利用卡通或虚拟人物生成多角色对话教学场景，提升课程吸引力与互动性  ￼ ￼
	- 企业培训 & 情景模拟
用虚拟形象模拟访谈、角色扮演或面试练习，让学习者在低成本环境中练习对话技巧  ￼ ￼

应用场景三：内容创作与社交媒体
	- 短视频创作
博主或内容创作者快速生成多角色 skit、段子或对话视频，提高制作效率和内容互动性  ￼ ￼
	- 虚拟直播与虚拟 YouTuber
虚拟形象进行多人直播或互动，观众可看到 avatar 与配音同步交流，增强沉浸感  ￼ ￼

应用场景四：广告与营销
	- 虚拟代言人与产品演示
多角色协作场景中，虚拟代言人可根据音频讲解产品特点，呈现生动的广告形式  ￼ ￼
	- 个性化促销视频
根据用户偏好生成动态、多角色的营销片段，提高用户参与度和信息传达效率  ￼

应用场景五：智能客服与虚拟助理
	-	多人物虚拟客服
虚拟客服与用户在视频中面对面对话，多个角色（如客服、助手、提示角色）同步对话，提升沟通体验  ￼
	-	交互式指导机器人
虚拟角色根据用户语音生成视频反馈，适用于医疗、法律或心理辅助场景


<h2 id="72.Wan-S2V模型在数字人生成中如何处理手部动作的生成和优化">72.Wan-S2V模型在数字人生成中如何处理手部动作的生成和优化</h2>

Wan-S2V模型通过使用姿态跟踪技术和优化的生成策略来处理手部动作的生成和优化。模型使用VitPose和DWPose跟踪每个角色的2D姿态，并将其作为多模态控制信号，以确保生成的手部动作自然且准确。此外，模型在训练过程中通过多种指标（如手部关键点置信度HKC和手部关键点方差HKV）评估生成手部动作的质量，确保生成的手部动作不仅清晰准确，而且丰富多样。通过这些方法，模型可以生成更加自然和逼真的手部动作，提升数字人的整体表现力。


<h2 id="73.Wan-S2V模型在数字人生成中如何解决长视频生成的稳定性问题">73.Wan-S2V模型在数字人生成中如何解决长视频生成的稳定性问题</h2>

Wan-S2V模型通过FramePack模块解决了长视频生成的稳定性问题。FramePack模块通过不同的时间压缩比率减少运动帧的token数量，从而在不增加过多计算复杂度的情况下，保持长视频的稳定性。具体来说，该模块会根据视频内容的复杂度和运动趋势，对运动帧进行压缩，减少token的数量。这样可以在保持视频运动信息的同时，降低计算开销，使得模型能够更高效地处理长视频生成任务，解决了长视频生成中的稳定性和一致性问题。

<h2 id="74.Wan-S2V模型在音频驱动视频生成领域的主要创新点是什么">74.Wan-S2V模型在音频驱动视频生成领域的主要创新点是什么</h2>

Wan-S2V模型的主要创新点在于其针对复杂影视制作场景的音频驱动角色动画问题提出了一套综合解决方案。首先，它基于Wan T2V模型，整合了音频驱动能力，同时保留了文本控制能力，实现了音频和文本的互补优势。其次，在音频特征提取方面，使用Wav2Vec对音频波形进行编码，并采用加权平均层整合不同层次的音频特征，增强了与复杂音频信号的同步性。此外，引入FramePack模块进行运动帧压缩，通过不同的时间压缩比率减少运动帧的token数量，有效保持了长视频的稳定性。在数据处理上，结合自动化筛选和手动挑选收集数据，并通过姿态跟踪、细粒度筛选以及视频质量评估等步骤，构建了高质量的训练数据集。最后，采用混合并行训练策略(FSDP)和多阶段训练方案，实现了大规模、全参数模型训练，提升了模型的性能和稳定性。


<h2 id="75.Wan-Animate模型的核心思想和它试图解决的主要问题是什么">75.Wan-Animate模型的核心思想和它试图解决的主要问题是什么</h2>

Wan-Animate 提出了一种统一的角色动画和替换框架，其核心思想是“整体复制”（Holistic Replication）。它旨在解决现有方法在处理复杂的角色动画和替换任务时遇到的两个主要挑战：
1. 外观细节保持不足： 许多现有方法在动画化或替换角色时，难以完整保留源角色的精细外观特征，比如复杂的服装纹理、配饰等，容易导致细节丢失或失真。

2. 动作控制精度有限： 在进行动画生成时，尤其是在处理一些细微或剧烈的动作时，模型可能无法精确地跟随驱动视频的姿态，导致动作僵硬或不自然。

为了解决这些问题，Wan-Animate 通过一个统一的框架，将角色动画（Character Animation）和角色替换（Character Replacement）两个任务结合起来，利用整体复制的思路，力求在保留源角色高保真度外观的同时，实现对目标动作的精准驱动。

<h2 id="76.Wan-Animate中提到的整体复制是如何实现的">76.Wan-Animate中提到的整体复制是如何实现的</h2>

“整体复制”是 Wan-Animate 的核心机制，指的是在生成动画的每一帧时，都尽可能完整、全面地从源角色的参考图像中复制并保留其所有的外观特征，包括身份、服装、发型、配饰等。
它的实现主要依赖于以下几个关键技术点：
1. 特征对齐与融合： 模型首先会从源角色图像和驱动视频的姿态中提取特征。然后，通过精心设计的网络结构（如注意力机制或空间变换网络），将源角色的外观特征与目标姿态进行精确对齐。
2. 分层或多尺度的特征处理： 为了捕捉从粗到细的各种细节，模型可能会在不同的特征层级上进行操作。例如，在粗略的层级上对齐身体结构，在精细的层级上复制纹理和配饰细节。
3. 统一的生成模型： 将角色动画和替换任务整合到一个统一的框架中。这个模型被训练用来理解如何根据目标姿态“粘贴”或“绘制”源角色的外观，而不是从头生成，从而最大限度地保留原始信息。

通过这种方式，模型会学习如何将源图像的像素或特征复制并粘贴到新的姿态上，从而实现外观的整体保留。

<h2 id="77.如何设计训练数据和损失函数才能让模型既能处理人体驱动，又能处理人体替换">77.如何设计训练数据和损失函数才能让模型既能处理人体驱动，又能处理人体替换</h2>

数据构建策略：
1. 自重构任务 (Self-Reconstruction)： 这是训练“角色动画”能力的基础。从一个视频中，随机抽取一帧作为源图像（Reference Image），再从同一视频中抽取另一帧的目标姿态（Target Pose）。模型的任务是根据源图像和目标姿态，重建出目标帧。
2. 交叉重构任务 (Cross-Reconstruction)： 这是训练“角色替换”能力的关键。从视频A中抽取一帧作为源图像，从完全不同的视频B中抽取一帧的目标姿态。模型的任务是将A的外观“穿”在B的姿态上。
数据增强： 在训练过程中，以一定的概率混合这两种任务。例如，80% 的时间进行自重构，20% 的时间进行交叉重构。这使得模型必须学会一个通用的“外观-姿态”组合能力，而不是死记硬背特定身份与其动作的绑定关系。

损失函数设计：
1. 核心扩散损失 (Diffusion Loss)： 预测添加到图像中的噪声，这是基础。通常使用 L1 或 L2 损失。
2. 身份保持损失 (Identity Loss)： 为了确保生成角色的脸部、体型等与源角色一致，可以使用一个预训练的人脸识别网络（如 ArcFace）或人体 Re-ID 网络来提取特征。计算生成图像的身份特征与源图像身份特征之间的余弦相似度损失。这个损失对于抑制身份漂移至关重要。
感知损失 (Perceptual Loss)： 使用预训练的 VGG 网络提取生成图像和真实目标图像的深层特征，并计算它们之间的 L1 或 L2 距离。这比像素级的 L2 损失更能保证生成结果在视觉感官上的相似性。
3. 姿态一致性损失 (Pose Consistency Loss)： 虽然姿态已经作为强条件输入，但可以额外增加一个损失，即对生成的图像提取姿态，并与输入的目标姿态进行比较，以惩罚任何姿态上的偏差。

通过这种混合数据训练和多目标损失函数的设计，模型被迫学习到一个通用的映射：将任意的外观特征，根据任意的姿态条件，渲染到图像上。

<h2 id="78.Wan-Animate中的Face-Adapter是如何实现的">78.Wan-Animate中的Face-Adapter是如何实现的</h2>

Face Adapter 的核心目标是精确地驱动角色面部的表情动画，同时要避免将驱动视频中人物的身份特征“泄露”到生成结果中。它通过一个精巧的多阶段流程来实现这一目标，摒弃了传统的面部关键点（landmarks）方案，而是直接使用图像作为驱动信号。以下是 Face Adapter 的具体实现步骤：
1. 输入信号：使用原始面部图像而非关键点
为了捕捉更丰富、更精细的表情细节，Wan-Animate 不使用稀疏的面部关键点作为控制信号，而是直接从驱动视频的每一帧中裁剪出面部区域的图像（raw facial images）作为输入。这样做的好处是能够保留所有细微的变化，如肌肉牵动、眼神光泽等。
2. 特征提取与压缩：
裁剪出的面部图像首先会被送入一个特制的 面部编码器（Face Encoder）。
编码器结构：该编码器的结构与LIA(ICLR 2022, PAMI 2024)研究中所使用的结构相同，善于提取面部特征。
空间压缩：编码器会将二维的面部图像在空间上压缩成一个一维的特征向量（1D latent vector）。这一步会强制模型将面部的核心信息（主要是表情）编码到一个紧凑的表示中，从而初步剥离了空间上的身份信息（如脸型、五官的具体形状）。
3. 身份与表情的解耦（Disentanglement）
这是 Face Adapter 最关键的技术挑战。为了确保只传递表情而非身份，模型采用了多种策略：
- 数据增强：在训练阶段，对作为输入的裁剪面部图像进行大量的数据增强，包括随机缩放、色彩抖动和添加噪声。这使得编码器难以学习到稳定不变的身份特征，被迫专注于捕捉动态变化的表情信息。
- 线性运动分解（Linear Motion Decomposition）：对提取出的特征进行正交化处理，这种技术可以更好地分离和解耦表情特征。
4. 时间维度的处理与对齐
从视频中提取的是一系列连续的面部特征向量，需要进行时间上的处理，以匹配主干网络（Diffusion Transformer）的序列长度。
- 1D 因果卷积：这些一维的面部特征向量序列会通过一个由多层 1D 因果卷积组成的网络进行处理。
- 时间降采样：该网络会对序列进行时间上的降采样，使其长度与主干网络中噪声潜在表示的序列长度对齐。
5. 特征注入主干网络
经过处理的面部表情特征最终需要注入到主干的 Diffusion Transformer (DiT) 模型中，以引导视频的生成。
- 专用的 Face Blocks：在 DiT 架构中，设计了专门的 "Face Block" 模块用于接收面部特征。
- 时序对齐的交叉注意力机制：面部特征是通过交叉注意力机制注入的。在这种机制下，DiT 自身的特征作为查询（Query），而对齐后面部特征序列作为键（Key）和值（Value）。“时序对齐”意味着在计算注意力时，每一帧的视频特征只会关注对应时间步的面部表情特征，从而实现精准的逐帧驱动。
- 稀疏注入：为了降低计算负担，面部特征并不会在 DiT 的每一层都注入，而是采用稀疏注入的策略。例如，在拥有40层的 Wan-14B 模型中，每隔5层注入一次。
6. 渐进式训练策略
在整个模型的训练流程中，面部控制是作为一个专门的阶段被引入的。通常在身体控制（Body Control）训练之后，模型会进入面部控制训练阶段[2][4]。在这一阶段，会特别使用肖像视频数据进行训练，并对面部区域（如眼睛、嘴巴）施加更高的损失权重，从而让模型集中学习和优化表情生成的细节[3]。

总结来说，Face Adapter 的实现是一个提取-解耦-对齐-注入的完整流程。它通过直接编码原始面部图像来保证细节的丰富性，利用空间压缩和数据增强等手段剥离身份信息，再通过时间卷积网络对齐序列，最终利用交叉注意力将纯净的表情信号精确地注入到主生成模型的特定模块中，从而实现了高保真且身份可控的面部动画。

<h2 id="79.Wan-Animate中的Body-Adapter是如何实现的">79.Wan-Animate中的Body-Adapter是如何实现的</h2>

Body Adapter 是整个 Wan-Animate 框架的结构性支柱。它的核心任务是将驱动视频中的人体姿态和动作，转化为精确的、可供主生成模型（Diffusion Transformer）理解和遵循的几何与结构引导信号。

1. **姿态特征提取与空间对齐**：采用 VitPose 网络对驱动视频序列进行逐帧处理，提取高精度的人体关键点坐标。这些关键点经过空间标准化和对齐处理后，构成姿态表示矩阵 $P \in \mathbb{R}^{T \times H \times W \times C}$，其中 $T$ 为时间维度，$H \times W$ 为空间分辨率，$C$ 为关键点通道数。

2. **姿态潜在空间编码**：Body Adapter 将姿态矩阵 $P$ 输入到预训练的 Wan-VAE 编码器中，通过卷积神经网络和下采样操作将其压缩至低维潜在空间，得到姿态潜在表示 $L_P \in \mathbb{R}^{T \times h \times w \times d}$。该编码过程确保了姿态特征在时空维度上与目标噪声潜在 $L_{target}$ 的维度一致性。

3. **特征融合与注入机制**：通过可学习的线性投影层 $\phi: \mathbb{R}^d \rightarrow \mathbb{R}^d$ 对姿态潜在 $L_P$ 进行特征变换，生成姿态嵌入 $E_P = \phi(L_P)$。随后采用逐元素加法操作将姿态嵌入与噪声潜在进行融合：$L_{fused} = L_{noise} + E_P$，融合后的特征作为 Diffusion Transformer 的输入。

4. **身份-姿态解耦策略**：在特征注入过程中，参考潜在 $L_{ref}$ 被严格隔离，不参与任何姿态信息的融合操作。这种设计通过显式的特征分离机制，确保参考图像仅提供身份和外观信息，而姿态控制完全由驱动序列决定，从而实现了身份保持与动作迁移的解耦。

<h2 id="80.Wan-Animate如何保证新角色与环境的一致性">80.Wan-Animate如何保证新角色与环境的一致性</h2>

对于人物替换，在新的环境中直接维持源角色的外观可能会导致光照和色调不匹配,Wan-Animate针对此问题设计了辅助重新照明LoRA来增强替换角色和新环境之间的一致性。该 LoRA 模块将适当的环境照明和色调应用于角色，而不会影响其外观一致性。

- 训练构建特定的数据对：使用 IC-Light 将从参考视频帧中分割并裁剪的人物合成到新的随机背景上。此过程会根据新背景自然地影响人物的光照和色调，从而为 Relighting LoRA 学习如何调整提供差异。

<h2 id="81.Wan-Animate在推理时如何分别主体和环境">81.Wan-Animate在推理时如何分别主体和环境</h2>

在推理时，Wan-Animate 通过一个简单的策略来区分主体和环境：

1. 在动画模式下，目标生成的条件帧以零填充，其二进制掩码设置为 0。该模型在保留源图像背景的同时生成角色视频。

2. 在替换模式下，原始角色从参考视频中分割出来，并通过将分割后的主体区域清零来生成环境图像。这些环境图像作为条件帧的内容，其中二进制掩码设置为 1（表示环境区域），0（表示主体区域）。然后，模型仅在被掩码的区域内生成内容，从而有效地替换角色，同时保留参考视频的背景。分割采用sam2_hiera_large模型。

这种范式确保了对不同条件需求的适应性，同时最大限度地减少了基础 Wan-I2V 模型在训练后出现的分布偏移。

<h2 id="82.Wan-Animate在推理时如何处理人体体型差异较大的问题">82.Wan-Animate在推理时如何处理人体体型差异较大的问题</h2>

在推理时，Wan-Animate 通过Pose Retargeting来处理体型差异较大的角色替换。

1. 提取 2D 骨骼 (Skeleton Extraction)。系统首先从源角色图像和驱动视频的每一帧中提取出 2D 骨骼关键点（例如使用ViTPose）。
2. 计算骨骼长度比率 (Length Ratio Calculation)。对源角色和驱动角色的骨骼进行比较，计算出每一根对应骨骼（肢体）的长度比例。例如，计算源角色“左上臂”的像素长度与驱动角色“左上臂”像素长度的比值。
3. 调整目标姿态 (Target Pose Adjustment)。将上一步计算出的比率应用到驱动视频的整个骨骼动画序列上。具体来说，就是将驱动视频每一帧中的每一根骨骼的长度，都缩放到与源角色的骨骼长度相匹配。这样，整个动作序列的骨骼比例就变成了源角色的比例。
4. 全局位置对齐 (Pose Translation)。调整完比例后，还需要将整个骨骼在画面中的位置进行平移，使其与源角色图像中的位置对齐。对齐的参考点（reference point）是根据镜头类型动态选择的，例如：全身镜头 (full-body)：对齐脚部。半身或肖像镜头 (half-body or portraits)：对齐颈部。

但仅仅通过这样的比例缩放仍会出现透视效应的问题直接从 2D 图像中计算骨骼长度是不准确的。因为透视（foreshortening）效应，一个对着镜头伸出的手臂会比一个与镜头平行的手臂在 2D 图像上看起来短得多。如果源角色和驱动角色处于不同的姿势，计算出的长度比率就会有很大误差。为了获得更真实的骨骼长度比例，Wan-Animate 采用了一个巧妙的方法，采用图像编辑网络来辅助：

1. 利用一个图像编辑模型，如Kontext, Qwen-Image-Edit
2. 将源角色图像和驱动视频中的角色（例如第一帧）都编辑成一个标准的 T-pose
3. 在 T-pose 状态下，所有肢体都与成像平面基本平行，foreshortening 效应最小。此时再从这两张被编辑过的 T-pose 图像中提取骨骼并计算长度比率，得到的结果会准确得多
4. 将这套准确的“缩放因子”应用到原始的、动态的驱动视频骨骼序列上

另外在人体驱动模式和人体替换模式下，Wan-Animate给出了不同的建议：

动画模式 (Animation Mode)：启用姿态重定向。
- 目的：核心是让源角色“动起来”，因此必须保持源角色自身的身体比例，所以需要将驱动者的动作比例调整到和源角色一致。

替换模式 (Replacement Mode)：不推荐使用姿态重定向。
- 原因：在替换场景中，驱动视频中的角色可能与环境有精确的物理交互（如坐在椅子上、手扶着墙壁）。如果改变了骨骼长度（例如，把一个矮个子的手臂动作拉长以匹配高个子的源角色），就会破坏这种交互，导致手悬在空中或穿过物体。
- 局限性：这也解释了为什么在这种模式下，如果源角色和驱动角色的体型差异巨大，会导致变形 (deformation)。因为模型被迫将一个角色的外观“硬塞”到另一个不同比例的骨架上，自然会产生拉伸或挤压。

<h2 id="83.什么是Copy-Paste现象">83.什么是Copy-Paste现象</h2>

很多定制/换脸方法因为缺少同一身份的多样化成对数据，只能做重建式训练，模型倾向直接复刻参考脸，而不是在姿态、表情、光照变化下保持身份一致，结果像但可控性差。

WithAnyone提出了一个Copy-Paste度量指标，以“生成图-参考图”与“生成图-GT 图”的角距离差为核心，+1 表示完全复制，−1 表示完全贴合 GT。配合 SimGT、身份混淆度、CLIP-I/T、美学分，形成 MultiID-Bench 标准化协议，惩罚无脑复制操作，公式如下：

$\mathcal{M}_{\mathrm{CP}}(g \mid t, r)=\frac{\theta_{g t}-\theta_{g r}}{\max \left(\theta_{t r}, \varepsilon\right)} \in[-1,1]$


<h2 id="84.如何收集多人ID数据集">84.如何收集多人ID数据集</h2>

可以参考MultiID-2M数据集的构建方法，主要包括以下几个步骤：	
1. 收集和聚类单人ID数据：从网络收集大量单人图像，并基于人脸识别特征进行聚类，为每个ID建立一个丰富的参考图像库。
2. 收集多人ID数据：通过特定关键词（如人名、场景）进行靶向搜索，收集包含多个ID的合影。
3. ID图像配对：将单人数据和多人数据中的人脸进行匹配，形成“（单人参考图，合影目标图）”这样的成对数据。
4. 后处理：进行质量控制、美学评分和风格化处理，最终形成高质量的训练数据。

<h2 id="85.HunyuanVideo-Avatar如何均衡视频的动态性和一致性">85.HunyuanVideo-Avatar如何均衡视频的动态性和一致性</h2>

现有的方法在使用参考图像来强制生成视频与参考之间的一致性时，往往会限制视频的动态性，导致生成的视频动作不够自然。HunyuanVideo-Avatar通过设计条件注入方式来解决这一问题：

- 角色图像注入模块：该模块将角色图像特征注入到模型中，解决了训练和推理之间的条件不匹配问题。该模块将角色图像特征转换为更适合模型学习的表示形式，并沿通道维度注入这些特征，避免了直接使用潜在空间导致的动态性与一致性之间的权衡。
- 具体实现：角色图像首先被重复T次，然后通过一个投影模块（由全连接层组成）直接添加到视频潜在表示中。这种方法在保持角色一致性的同时，显著提高了视频的动态性。

通过这种设计，HunyuanVideo-Avatar能够在生成视频时既保持与参考图像的一致性，又允许视频展现出丰富的动态变化，从而生成更加自然和生动的视频内容。

<h2 id="86.HunyuanVideo-Avatar如何加强音频情感与面部表情对齐">86.HunyuanVideo-Avatar如何加强音频情感与面部表情对齐</h2>


现有的方法在音频情感与面部表情之间的对齐方面存在不足，导致生成的视频在情感表达上不够准确，HunyuanVideo-Avatar通过音频情感模块来加强音频情感与面部表情对齐。该模块从情感参考图像中提取情感线索，并将其转移到目标生成视频中，实现音频情感与角色表情之间的精确对齐。情感参考图像被压缩成特征，然后通过一个全连接层和空间交叉注意力机制注入到视频潜在表示中。情感参考图像的特征作为 Key 和 Value，而原始视频潜在表示作为 Query，通过交叉注意力机制将情感信息融合到视频中，使模型能够更好地理解音频情感与面部表情之间的关系。

<h2 id="87.HunyuanVideo-Avatar如何加强音频与特定ID绑定能力">87.HunyuanVideo-Avatar如何加强音频与特定ID绑定能力</h2>

现有的方法在处理多角色场景时，无法有效地将音频与特定角色的动作和表情对齐，限制了其在实际应用中的可用性。HunyuanVideo-Avatar通过引入ID特定mask来加强音频与特定ID的绑定能力。

- 面部感知音频适配器：该模块通过在潜在空间中应用面部掩码，将音频驱动的角色动作限制在特定的面部区域，从而实现多角色场景中独立的音频驱动动画。
- 具体实现：使用 InsightFace 方法检测面部区域的边界框，并将这些掩码应用于从输入中提取的潜在特征，生成面部掩码视频潜在表示。然后，通过交叉注意力机制将音频信息与这些掩码特征融合。由于音频主要影响掩码的面部区域，因此可以
独立地驱动不同角色，实现多角色对话生成。

<h2 id="88.HunyuanVideo-Avatar如何处理长视频生成">88.HunyuanVideo-Avatar如何处理长视频生成</h2>

HunyuanVideo-Avatar采取了类似sonic的长视频处理方法，通过时间感知位置偏移融合来实现长视频生成。该方法允许模型在每个时间步中处理音频片段，并通过平滑连接前后时间步的片段，实现连续的视频生成。

- 时间感知位置偏移融合方法：该方法通过在每个时间步中使用音频片段作为输入来预测对应的潜在表示，并使用起始偏移量平滑地连接上一个时间步的片段。这种方法允许模型自然地桥接上下文，实现连续的视频生成。
- 具体实现：在每个时间步中，模型从新的位置开始处理音频片段，并在每个时间步中向前移动$\alpha$步。通过这种方式，模型可以生成与音频提示一致的长视频。
