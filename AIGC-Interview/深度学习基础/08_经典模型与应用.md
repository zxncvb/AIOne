# 目录

## 第一章 CNN 经典模型

   ### 01.01_LeNet 系列模型
   ### 01.02_AlexNet 模型
   ### 01.03_VGG 系列模型
   ### 01.04_ResNet 系列模型
   ### 01.05_DenseNet 模型

## 第二章 Transformer 经典模型

 ### 02.01_Transformer 基础模型

- [1. 正余弦位置编码也有外推、相对距离表达、远程衰减，为什么大模型都用RoPE？](#user-content-1正余弦位置编码也有外推、相对距离表达、远程衰减，为什么大模型都用RoPE？)

- [2. RoPE的base有什么作用、在控制什么？](#user-content-2RoPE的base有什么作用、在控制什么？)

- [3. RoPE为何能从2维扩展到n维？](#user-content-3RoPE为何能从2维扩展到n维？)

- [4. Qwen中RoPE有GPT-J和GPT-NeoX两种实现，和理论不同，二者等价吗？](#user-content-4Qwen中RoPE有GPT-J和GPT-NeoX两种实现，和理论不同，二者等价吗？)

- [5. 长度外推中传统位置编码的OOD问题是什么？](#user-content-5长度外推中传统位置编码的OOD问题是什么？)

- [6. 长度外推中RoPE的OOD问题是什么？](#user-content-6长度外推中RoPE的OOD问题是什么？)

- [7. RoPE是绝对位置编码，训练过程中到底在训练什么？](#user-content-7RoPE是绝对位置编码，训练过程中到底在训练什么？)

- [8. 如何免训练外推RoPE？少量长文本训练如何强化外推？](#user-content-8如何免训练外推RoPE？少量长文本训练如何强化外推？)

- [9. 从几何+傅里叶角度，n维RoPE整体在做什么、代表什么？](#user-content-9从几何+傅里叶角度，n维RoPE整体在做什么、代表什么？)

- [10. RoPE高低频旋转圈数差异，和训练过程如何联系？](#user-content-10RoPE高低频旋转圈数差异，和训练过程如何联系？)


 
 ### 02.02_BERT 系列模型
 ### 02.03_GPT 系列模型
 ### 02.04_T5 与 BART 模型


## 第三章 rnn 经典模型

 ### 03.01_rnn 基础模型
RoPE：从 2D 到 nD 的完美扩展之旅


<h1 id="1正余弦位置编码也有外推、相对距离表达、远程衰减，为什么大模型都用RoPE？">1. 正余弦位置编码也有外推、相对距离表达、远程衰减，为什么大模型都用RoPE？</h1>

原生sinusoidal正余弦位置编码公式为：

$$\begin{cases}
PE_{pos,2i} = \sin\left(\frac{pos}{10000^{2i/d}}\right) \\
PE_{pos,2i+1} = \cos\left(\frac{pos}{10000^{2i/d}}\right)
\end{cases}$$

它看似具备远程衰减、隐式相对位置、弱外推能力，但存在本质性缺陷，远无法满足大模型长文本、稳定泛化的需求，而RoPE从理论和工程上解决了所有核心痛点：

1. 相对位置的表达性质差异

  - sinusoidal PE仅通过 $$PE_{pos+m}$$ 与 $$PE_{pos}$$ 的内积隐式携带相对位置信息，无数学上的显式约束，内积结果仅和相对距离线性相关，无法建模复杂的相对位置依赖；

  - RoPE通过对 $$Q/K$$ 向量做旋转变换，可严格推导出注意力分数直接编码显式相对位置m-n：
    
$$\boldsymbol{q}_m^\top \boldsymbol{k}_n \rightarrow \boldsymbol{q}_m R_{\theta,m}^\top R_{\theta,n} \boldsymbol{k}_n^\top = \boldsymbol{q}_m^\top R_{\theta,m-n} \boldsymbol{k}_n$$

，相对位置直接参与注意力计算，建模能力远强于隐式内积。

2. 向量空间与几何性质差异

  - sinusoidal PE是直接与词嵌入逐元素相加，会破坏语义向量的欧式空间结构，位置信息与语义信息强耦合，干扰语义相似度计算；

  - RoPE是纯正交旋转变换，严格保向量模长、保内积的语义部分，仅对位置信息做旋转注入，语义与位置解耦，注意力分数的语义核心不受干扰。

3. 外推能力的真实性差异

  - sinusoidal PE的外推是伪外推：超过训练长度后，位置编码的内积分布剧烈偏离训练分布，远程内积快速坍缩至0，位置区分度完全失效；

  - RoPE的外推基于几何旋转一致性，训练长度外的相对位置旋转规则与训练时完全统一，无分布突变，具备天然的真外推基础。

4. 长距离依赖与注意力衰减

  - sinusoidal PE的远程衰减是固定频率的被动衰减，无自适应能力，长距离注意力退化无修正；

  - RoPE的高低频分工天然匹配语言近强远弱的先验，配合自注意力可学习权重，长距离依赖建模更稳定。

5. 工程与兼容性

 RoPE无额外可学习参数、计算开销极低、完全兼容标准自注意力的并行计算，适配大模型的训练与推理架构，这是大模型全面弃用原生sinusoidal PE、选用RoPE的核心原因。

<h1 id="2RoPE的base有什么作用、在控制什么？">2. RoPE的base有什么作用、在控制什么？</h1>

RoPE的核心频率定义为 $$\theta_i = base^{2i/d}$$，对应旋转角 $$\phi_{pos,i} = \frac{pos}{\theta_i} = \frac{pos}{base^{2i/d}}$$ ， $$base$$ 默认取值为10000，它是RoPE唯一的全局超参数，核心控制对象如下：

1. 控制所有维度的角频率全局缩放

  $$base$$ 直接决定每个维度的角频率 $$\omega_i = \frac{1}{\theta_i}$$ ：

  - $$base$$ 增大 →  $$\omega_i$$ 降低 → 相同位置 $$pos$$的旋转角 $$\phi_{pos,i}$$ 减小 → 旋转周期拉长；

  - $$base$$ 减小 →  $$\omega_i$$ 升高 → 旋转角 $$\phi_{pos,i}$$ 增大 → 旋转周期缩短。

2. 控制有效可区分的最大位置长度

 旋转角超过 $$2\pi$$ 会发生相位缠绕，不同位置会得到完全相同的旋转结果，位置产生歧义。 $$base$$ 越大，旋转周期越长，可无歧义区分的位置上限越长，这是长文本工作必须调整 $$base$$ 的核心原因。

3. 控制位置编码的分辨率

  - 大 $$base$$ ：频率低，位置变化带来的角度变化小，全局粗粒度编码，长距离可区分但近邻细粒度区分能力下降；

  - 小 $$base$$ ：频率高，近邻小幅度位置变化就有显著角度差，局部细粒度编码，但快速相位缠绕，有效长度极短。

4. 控制高低频维度的频率间隔

 $$base$$ 决定不同通道$$i$$的频率衰减速率，塑造从高频（局部）到低频（全局）的完整频带覆盖，影响局部/全局位置信息的分配比例。

简单总结： $$base$$ 是RoPE的频率尺度控制器，直接控制旋转周期、有效外推长度、位置分辨率、频带分布，是长文本外推优化的核心超参数。

<h1 id="3RoPE为何能从2维扩展到n维？">3. RoPE为何能从2维扩展到n维？</h1>

RoPE从2维扩展到任意偶数维 $$d$$ ，不是强行近似，而是基于高维线性空间的正交分块性质，数学与几何性质完全保真：

1. 高维空间的正交2维子空间分解

 任意偶数维 $$d$$ 的特征空间，可无重叠、无耦合地分解为 $$\frac{d}{2}$$ 个相互正交的2维平面（子空间），形如：

  $$\mathbb{R}^d = \mathbb{R}^2_{(0,1)} \oplus \mathbb{R}^2_{(2,3)} \oplus \dots \oplus \mathbb{R}^2_{(d-2,d-1)}$$

   每个2维子空间独立，变换互不干扰。

2. 2维旋转的直和扩展

 n维RoPE的变换，是对每个独立2维子空间分别复用2维完美旋转矩阵，整体变换为所有2维旋转的直和，对应分块对角正交矩阵：

  $$R_d = \begin{pmatrix} R_{\theta_0} & & & \\ & R_{\theta_1} & & \\ & & \ddots & \\ & & & R_{\theta_{d/2-1}} \end{pmatrix}$$

   该矩阵保留2维旋转的所有核心性质：保模长、正交性、纯旋转无畸变。

3. 理论推导的一致性

 无论分解为多少个2维块，注意力分数最终都能推导出统一的相对位置旋转结果，与维度无关，理论形式完全统一。

4. 工程适配性

 奇数维可对最后一维补零或忽略，不影响核心逻辑，因此RoPE可无压力适配任意模型隐藏维度。

结论：n维RoPE不是对2维的推广，而是高维空间拆解为独立2维单元的组合变换，完整继承2维的所有几何优势。

<h1 id="4Qwen中RoPE有GPT-J和GPT-NeoX两种实现，和理论不同，二者等价吗？">4. Qwen中RoPE有GPT-J和GPT-NeoX两种实现，和理论不同，二者等价吗？</h1>

两种实现仅为工程计算形式不同，数学上严格等价，输出向量的每个元素数值完全一致，不存在理论偏差：

1. 理论RoPE的2维块变换

 对相邻元素 $$(x_{2i},x_{2i+1})$$ ，旋转角 $$\phi$$ ，变换为：

  $$\begin{cases}
x'_{2i} = x_{2i}\cos\phi - x_{2i+1}\sin\phi \\
x'_{2i+1} = x_{2i}\sin\phi + x_{2i+1}\cos\phi
\end{cases}$$

2. GPT-J实现

 严格对齐理论公式，显式构建$$\cos$$、$$\sin$$张量，逐2维块执行上述加减乘运算，维度分组为 $$(0,1),(2,3)\dots$$ ，是最直观的矩阵视角实现。

3. GPT-NeoX实现

 采用复数视角：将每个2维块视作复数 $$z = x_{2i} + x_{2i+1}\cdot i$$ ，旋转变换等价于复数乘以单位复指数 $$e^{i\phi} = \cos\phi + i\sin\phi$$ ：

  $$z' = z \cdot e^{i\phi} = (x_{2i}\cos\phi - x_{2i+1}\sin\phi) + (x_{2i}\sin\phi + x_{2i+1}\cos\phi)\cdot i$$

   计算结果与理论公式逐元素完全一致，仅用复数运算替代显式矩阵乘法。

4. 等价性结论

 复数乘法与2维旋转矩阵是同构变换，只是数学表述与计算路径不同，无精度差异、无理论变形、无结果偏差。Qwen等框架选用其中一种，仅为CUDA向量化、显存占用、计算速度的工程优化，并非理论实现错误，二者完全等价可互换。

<h1 id="5长度外推中传统位置编码的OOD问题是什么？">5. 长度外推中传统位置编码的OOD问题是什么？</h1>

OOD（Out-of-Distribution）指输入长度 $$L_{infer} > L_{train}$$ ，位置信息属于模型从未见过的分布，传统PE（sinusoidal、BERT可学习PE）的OOD问题是致命且无修复空间的：

1. 可学习PE（BERT类）的OOD问题

 可学习PE是查表结构，仅预定义 $$0 \sim L_{train}-1$$ 的位置向量，超过长度的位置无对应参数，推理时只能复用最后一个位置编码或随机初始化：

  - 位置信息完全丢失，超长位置共享相同编码，无任何区分度；

  - 语义与位置强耦合，分布突变直接导致注意力权重混乱，上下文理解完全失效。

2. sinusoidal PE的OOD问题

  - 内积分布崩塌：超长位置的PE内积与训练分布完全偏移，远程内积快速坍缩至0，不同超长位置无法区分；

  - 无几何约束：无相对位置的显式数学约束，外推位置的编码无一致性，模型无泛化基础；

  - 衰减无规律：远程位置编码相互冲突，注意力权重随机化，长距离依赖完全失效。

3. 共性核心缺陷

 传统PE无外推的数学一致性保障，OOD位置的编码不属于训练分布流形，模型无法泛化，长文本推理直接退化。

<h1 id="6长度外推中RoPE的OOD问题是什么？">6. 长度外推中RoPE的OOD问题是什么？</h1>

RoPE具备天然外推性，但仍存在非致命、可修复的OOD问题，这也是NTK-RoPE、YaRN等方法的优化目标：

1. 相位缠绕（Phase Wrap）

 当 $$pos$$ 极大时， $$\phi_{pos,i} = \frac{pos}{base^{2i/d}}$$会超过$$2\pi$$ ，角度周期循环，不同位置得到完全相同的旋转结果，产生位置歧义，模型无法区分超长位置。

2. 频率分布失配

 训练时模型适配 $$L_{train}$$ 内的旋转角度分布，超长文本下高频维度旋转过快、角度饱和，整体旋转分布与训练分布存在轻微偏移，注意力分数的相对模式产生畸变。

3. 长距离注意力模糊

 低频维度虽周期长，但超大 $$pos$$ 下不同位置的角度差极小， $$QK$$ 内积差异可忽略，远程注意力权重趋于均匀，长距离语义依赖建模能力衰减。

4. 无OOD位置训练信号

 原生RoPE无针对超长位置的正则约束，模型未见过OOD旋转分布，泛化能力随长度增加逐步下降。

<h1 id="7RoPE是绝对位置编码，训练过程中到底在训练什么？">7. RoPE是绝对位置编码，训练过程中到底在训练什么？</h1>

首先明确核心事实：RoPE本身无任何可学习参数，是固定数学变换，训练过程不会优化RoPE的任何公式、参数，模型学习的是适配RoPE的特征与注意力机制：

1. 学习适配旋转空间的 $$Q/K$$ 投影矩阵

 训练优化 $$W_Q、W_K、W_V$$ ，将词嵌入映射到适配RoPE旋转的特征空间，让语义信息与旋转位置信息解耦，保证旋转后语义相似度不被破坏。

2. 学习相对位置的注意力权重模式

 RoPE将绝对位置转为显式相对位置 $$m-n$$ ，模型学习不同相对距离下的注意力权重分布：近邻强关联、远程弱关联、特定相对位置的语义绑定规则。

3. 学习高低频维度的分工权重

 模型自动学习为高频维度分配局部细粒度位置建模权重、低频维度分配全局粗粒度位置建模权重，适配语言不同尺度的依赖结构。

4. 学习抑制旋转噪声与相位缠绕

 训练中模型学习过滤高频过快旋转、相位缠绕带来的位置噪声，保留有效相对位置信息，提升鲁棒性。

总结：RoPE是固定的位置编码器，训练不修改PE本身，而是训练整个模型的特征投影与注意力机制，适配RoPE的旋转式位置注入规则。

<h1 id="8如何免训练外推RoPE？少量长文本训练如何强化外推？">8. 如何免训练外推RoPE？少量长文本训练如何强化外推？</h1>

免训练外推（推理时修改，无任何微调）

免训练方法均基于数学缩放修正频率与相位，解决相位缠绕，不修改模型参数：

1. NTK-RoPE

 推理时放大 $$base$$， $$base_{new} = base \times \left(\frac{L_{extrap}}{L_{train}}\right)^{2i/d}$$  ，拉伸角频率周期，推迟相位缠绕，无训练、即插即用。

2. Dynamic NTK

 根据输入长度动态计算 $$base$$ ，自适应拉伸频率，适配任意输入长度，无需预设外推长度。

3. YaRN

 同时修正频率缩放与向量幅度，减少旋转畸变，外推稳定性与效果优于原生NTK，纯推理端修改。

4. 固定角度裁剪

 强制限制最大旋转角小于 $$2\pi$$ ，避免循环，简单轻量。

少量长文本训练（轻量微调，非重预训练）

用远少于预训练的数据，小成本强化OOD泛化：

1. 顶层注意力微调

 冻结模型主干，仅微调顶层 $$W_Q/W_K$$ 与注意力层，适配超长位置的旋转分布。

2. 频率一致性正则微调

 损失函数加入频率分布正则项，约束OOD位置旋转分布与训练分布对齐。

3. 长文本窗口微调

 仅用长文本数据训练滑动窗口内的注意力，强化局部-全局位置的关联建模。

4. 插值修正微调

 结合线性位置插值与RoPE，用少量数据修正插值带来的分布偏移。

<h1 id="9从几何+傅里叶角度，n维RoPE整体在做什么、代表什么？">9. 从几何+傅里叶角度，n维RoPE整体在做什么、代表什么？</h1>

几何角度

1. 空间分解： $$d$$  维向量拆解为 $$\frac{d}{2}$$ 个正交独立的2维平面，平面间无干扰、无交叉变换；

2. 单元变换：每个平面执行纯2维旋转，保模长、保正交、无拉伸剪切；

3. 整体性质：n维变换是分块对角正交变换，保留欧式空间的内积、距离、语义相似度，仅对位置信息做旋转调制。

傅里叶角度

1. 频域基：每个2维块对应一组固定频率的正弦-余弦傅里叶基， $$\theta_i$$ 是基频率， $$pos$$ 是时域位置；

2. 相位调制：旋转操作等价于对每个傅里叶分量做独立相位偏移，位置信息以频域相位的形式编码；

3. 频带覆盖： $$\frac{d}{2}$$ 个频率从高到低排列，构成完整频带——高频基编码局部细粒度位置，低频基编码全局粗粒度位置；

4. 注意力本质： $$QK$$ 内积等价于不同位置傅里叶特征的交叉相关，直接提取相对位置的频域信息。

整体代表：位置信息的多频带傅里叶编码 + 正交2维平面的独立相位旋转，兼具几何完美性与频域完备性。

<h1 id="10RoPE高低频旋转圈数差异，和训练过程如何联系？">10. RoPE高低频旋转圈数差异，和训练过程如何联系？</h1>

旋转圈数定义： $$N_{pos,i} = \frac{\phi_{pos,i}}{2\pi} = \frac{pos}{2\pi \cdot base^{2i/d}}$$ ，规律为：维度索引越小→频率越低→圈数越多；索引越大→频率越高→圈数越少，该特性与训练高度耦合：

1. 局部依赖建模（高频维度，少圈数）

 高频维度旋转慢，小 $$pos$$ 变化角度变化显著，细粒度区分近邻位置，训练时负责捕捉短距离依赖：相邻词关联、主谓结构、局部语法、词法搭配。

2. 全局依赖建模（低频维度，多圈数）

 低频维度旋转快，大 $$pos$$ 变化角度变化平缓，粗粒度区分远程位置，训练时负责捕捉长距离依赖：远程指代、上下文呼应、篇章逻辑、跨句语义关联。

3. 天然衰减与语言先验对齐

 高频维度快速饱和/循环，天然实现远程衰减，与语言近强远弱的先验一致，训练时无需额外学习衰减模式，收敛更快更稳定。

4. 训练稳定性与频域完备性

 高低频分工覆盖全尺度位置依赖，避免单一频率的畸变，模型易收敛；同时该分工在外推时保留，是RoPE外推优于传统PE的训练层面核心优势。

5. 梯度与优化友好

 多频带组合让位置信息的梯度传播更稳定，不同尺度的位置依赖都有对应编码通道，减少梯度消失，适配大模型深度训练。

