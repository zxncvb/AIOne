# 目录

- [1.图像编辑模型涵盖哪些细粒度的编辑任务？](#1.图像编辑模型涵盖哪些细粒度的编辑任务？)
- [2.图像编辑模型的主要技术路线有哪些？](#2.图像编辑模型的主要技术路线有哪些？)
- [3.在图像编辑的定量评估中，常用的基准测试和性能指标是什么？](#3.在图像编辑的定量评估中，常用的基准测试和性能指标是什么？)
- [4.Step1X-Edit 通用图像编辑框架是如何构建其大规模、多任务的编辑数据集](#4.Step1X-Edit-通用图像编辑框架是如何构建其大规模、多任务的编辑数据集)
- [5.Step1X-Edit的预训练细节是什么](#5.Step1X-Edit的预训练细节是什么)
- [6.Step1X-Edit的架构是如何设计的？](#6.Step1X-Edit的架构是如何设计的？)
- [7.Z-Image如何构建高效且多样化的图像编辑训练对](#7.Z-Image如何构建高效且多样化的图像编辑训练对)
- [8.Z-Image架构中如何针对图像编辑任务进行专门配置](#8.Z-Image架构中如何针对图像编辑任务进行专门配置)
- [9.Z-Image如何为图像编辑任务生成caption](#9.Z-Image如何为图像编辑任务生成caption)
- [10.Z-Image-Edit的训练阶段有哪些](#10.Z-Image-Edit的训练阶段有哪些)
- [11.Z-Image-Edit如何实现Prompt Enhancer功能](#11.Z-Image-Edit如何实现Prompt-Enhancer功能)
- [12.Z-Image的预训练阶段是如何设计的](#12.Z-Image的预训练阶段是如何设计的)

<h2 id="1.图像编辑模型涵盖哪些细粒度的编辑任务？">1.图像编辑模型涵盖哪些细粒度的编辑任务？?</h2>
图像编辑模型涵盖了多种细粒度的编辑任务，主要包括以下几类：

| 任务类别            | 具体内容                                                                                               |
|--------------------------------------|----------------------------------------------------------------------------------------------------------------------------|
| 内容编辑       | 对象的添加、删除、替换。                                                                                                   |
| 外观修改     | 颜色改变、材质修改(如青花瓷材质)、肖像美化、属性增强。                                                                   |
| 场景转换 | 背景替换、色调调整(色彩分级、去雾、去雨、季节转换)、光照调整。                                                           |
| 文本编辑             | 图像中文本修改、文字重绘、Logo/品牌替换(TOES→NIKE),支持中英文。                                                         |
| 风格迁移            | 输入图像风格抽取并迁移到新图像(线描、水彩、浮世绘、赛博朋克、胶片感等)。                                                 |
| 运动/姿态变换        | 姿态操控、人物动作变化、新颖视角合成(NVS)。                                                                              |
| 结构变换     | 几何结构调整、比例变化、身体/面部结构修改(瘦脸、延长腿部、局部形状改变等)。                                              |
| 换装                | 修改服饰款式、更换衣服、替换面料、保持人物身份一致。                                                                       |
| 人脸编辑              | 表情增强、五官调整、面部置换、身份保持编辑、数字人表情合成。                                                               |
| 光影增强                | 调整光源方向、强度、制造戏剧光、补光、逆光修复。                                                                            |
| 材质与质感增强 | 提升皮肤质感、物体表面质感、金属/玻璃/木材等材质变化。                                                                      |
| 分辨率增强        | 清晰度提升、细节补全、去噪、改善压缩损坏。                                                                                  |
| 视频一致性编辑 | 多帧一致性编辑、物体跟踪编辑、模板保持、跨帧风格或属性保持。                                                              |
| 多轮次编辑      | 多次编辑指令叠加、保持一致性、迭代生成历史记忆式编辑。                                                                     |
| 构图调整     | 裁剪、放大、改构图、视角转换、镜头焦距模拟(广角/长焦)。                                                                  |
| 电商图像增强     | 去除模特、衣物换色、产品精修、SKU合成、背景统一、电商风光影。                                                                                                                              |

<h2 id="2.图像编辑模型的主要技术路线有哪些？">2.图像编辑模型的主要技术路线有哪些？</h2>

目前主流的图像编辑模型主要采用以下几种技术路线：
| 模型名称        | 文本编码器 / 条件编码方式                         | 生成范式                               | 核心骨干网络                          |
|-----------------|---------------------------------------------------|----------------------------------------|---------------------------------------|
| Emu3.5          | 使用大语言模型分词器进行文本编码。                 | 自回归生成；通过并行预测技术加速推理。 | 大规模 Transformer 解码器，包含 64 层，并使用分组注意力与旋转位置编码。 |
| Qwen-Image      | 多模态模型作为条件编码器；同时使用图像编码器获取潜在表征。 | 基于流匹配的扩散生成。                 | 多模态扩散 Transformer；使用多尺度旋转位置编码以对齐图文位置。 |
| Step1X-Edit     | 多模态模型进行语义推理，输出图像编辑相关潜在条件。 | 基于归一流公式的扩散模型。             | 采用 DiT 结构，并通过连接器接收多模态模型提取的特征。 |
| FLUX.1 Kontext  | 使用专用文本编码器；在双流结构中分离处理文本与图像。 | 基于流匹配的扩散生成模型。             | 归一流 Transformer，包含双流块与融合块的组合结构。 |
| Seedream 4.0    | 使用视觉语言模型进行多模态理解。                   | 基于扩散模型；采用对抗式加速训练体系。 | 高效可扩展的 DiT 架构。               |

<h2 id="3.在图像编辑的定量评估中，常用的基准测试和性能指标是什么？">3.在图像编辑的定量评估中，常用的基准测试和性能指标是什么？</h2>

1. 关键基准测试:

- GEdit-Bench: 评估模型在 11 个不同类别下的真实世界用户指令编辑能力，支持英文（EN）和中文（CN）指令。

- ImgEdit: 涵盖 9 个常见编辑任务，共 734 个真实世界测试用例。
- KontextBench: 专注于上下文图像生成和编辑任务，包括文本编辑、局部编辑、全局编辑、风格参考和角色参考等六项任务。

- 此外，还有针对文本渲染的 CVTG-2K (英文) 和 ChineseWord (中文) 基准。

2. 评估指标:
通常采用 VIEScore 框架，由先进的 MLLM（如 GPT-4.1 或 Qwen2.5-VL）进行自动评估，分数范围为 0 到 10。主要指标包括：
- 语义一致性 (Semantic Consistency, SQ/SC): 衡量编辑结果与给定指令的符合程度。
- 感知质量 (Perceptual Quality, PQ): 评估图像的自然度和是否存在伪影。
- 总体得分 (Overall Score, O): 根据上述评估计算得出。

对于中文文本渲染，Qwen-Image 引入了基于字符级别的 准确率 (Accuracy)

<h2 id="4.Step1X-Edit-通用图像编辑框架是如何构建其大规模、多任务的编辑数据集">4.Step1X-Edit 通用图像编辑框架是如何构建其大规模、多任务的编辑数据集</h2>
Step1X-Edit 致力于构建一个涵盖 11 种多样化编辑任务（包含主体添加与删除、主体替换、背景替换、颜色变化、材质修改、动作姿态变化、肖像美化、色调转换、文本修改、风格迁移）的高质量数据集。该数据集的构造是一个多阶段的自动化和人工过滤管线，利用了多种先进的视觉工具：

1. 基础任务（主体增添/移除）：
- 对象定位与分割： 使用 Florence-2 进行对象标注和 SAM-2 进行分割。
- 指令生成： 通过 Step-1o 自动生成“从源图像到目标图像”的编辑指令（Instruction）。
2. 复杂内容操作（主体替换/背景改变）：
- 对象识别： 利用 Qwen2.5-VL 和 Recognize-Anything Model 来识别目标对象或关键词。
- 内容填充： 采用 Flux-Fill 进行内容感知修补/填充（inpainting）。
3. 高级编辑（材质修改）：
- 深度信息引入： 使用 ZeoDepth 进行深度估计。
- 条件控制生成： 结合 SD3.5 ControlNet，利用深度图和掩码等辅助信息，实现精确的材质或风格修改（例如，将龙虾材质改为青花瓷）。
4. 风格转移： 通过 SD3.5 ControlNet 和 边缘图像实现风格化注释和转换。

Step1X-Edit 的数据集通过这种混合工具链生成，然后经过严格的人工验证，确保了数据的高质量。

<h2 id="5.Step1X-Edit的预训练细节是什么">5.Step1X-Edit的预训练细节是什么</h2>

Step1X-Edit 的训练流程基于构建大规模、高质量的数据集和初始化策略：
1. 数据生成管线构建：
- 数据集目标： 团队设计了一个复杂的数据生成管线，旨在生产大规模、高质量的三元组（源图像、编辑指令、目标图像）。
- 规模与质量： 该管线共生成了超过 2000 万个三元组。经过 MLLM（例如 step-1o）和人工标注者的严格过滤（保留比例约为 20:1），最终保留了超过 **100 万个高质量三元组 Step1X-Edit-HQ**用于训练。
- 任务覆盖： 数据集涵盖了 11 种不同的图像编辑子任务，包括主体增添/移除/替换、背景改变、颜色/材质修改、动作变化、色调转换、文本修改和风格转移等。
- 工具辅助： 数据构造利用了多种先进的视觉工具，例如 Florence-2 和 SAM-2 进行对象分割，Flux-Fill 进行内容感知修复，以及 ControlNet 和 Zeodepth 协助进行精确的材质或颜色修改。
2. 模型初始化与训练目标：
- 初始化： Step1X-Edit 的训练从一个文本到图像（T2I）模型开始，以确保保留模型已有的美学质量和视觉一致性。
- 训练目标： 模型通过优化扩散损失 (diffusion loss) 来训练，并遵循归一流 (rectified flow) 公式进行生成。训练过程联合优化了连接器和下游的DiT网络

<h2 id="6.Step1X-Edit的架构是如何设计的？">6.Step1X-Edit的架构是如何设计的？</h2>
Step1X-Edit设计为一个统一的图像编辑模型，旨在紧密集成语义理解能力和图像生成能力，引入了多模态大语言模型。Step1X-Edit具体由三个关键组件构成：

1. 多模态大语言模型：
- 作用： MLLM（Qwen2.5-VL）用于处理参考图像和用户的编辑指令。它通过单次前向传播，联合捕获指令和视觉内容之间的语义关系。

- 指令特征提取： 模型在 MLLM 处理过程中，会选择性地丢弃与系统前缀相关的令牌嵌入。这样做是为了隔离并强调直接与编辑信息对齐的语义元素，确保后续处理能够精确聚焦于编辑要求。
2. 连接器模块：
- 作用： 这是一个轻量级模块，用于将 MLLM 提取的高级语义嵌入重构为紧凑的多模态特征表示。这些特征将用于指导下游的图像生成过程。
3. 扩散 Transformer (DiT)：
- 骨干网络： Step1X-Edit 采用 DiT 风格的扩散架构作为图像解码器。
- 功能： DiT 接收由 MLLM 通过连接器提供的编辑条件，并生成目标图像的潜在嵌入 (latent embedding)，最终获得输出图像。

这种统一架构消除了在编辑过程中对额外掩码 (mask) 的要求，从而实现了无掩码的通用编辑。总而言之，Step1X-Edit 的设计目标是实现 MLLM-Diffusion 的无缝集成，解决传统方法中编辑指令遵循与图像保真度之间的矛盾。

<h2 id="7.Z-Image如何构建高效且多样化的图像编辑训练对">7.Z-Image如何构建高效且多样化的图像编辑训练对</h2>
考虑到编辑操作的复杂性和多样性，Z-Image通过可扩展和可控的策略构建了大规模训练编辑数据，这里Z-Image的核心思想是在有限算力预算的情况下，尽可能让数据无荣誉、概念覆盖全、质量高且每个图像对都在正确的时间点喂给模型，一共构建了四个链路：

1. 混合编辑与专家模型：利用任务特定的专家模型合成高质量数据，并构建混合编辑数据，将多个编辑动作整合到一个编辑对中，从而在单个复合对中增强模型的多任务编辑能力。
2. 高效图形化表示：通过对同一输入图像的不同编辑版本进行任意配对组合和排列，以零成本扩大训练数据量。这不仅能产生混合编辑数据，还能生成逆向对（inverse pairs），以提高数据质量。
3. 视频帧配对图像：利用媒体池中从视频帧收集的具有固有相关性（如共同主题、场景或风格）的图像，通过CN-CLIP计算图像嵌入的余弦相似度进行筛选，提供高任务多样性、固有耦合的编辑类型和卓越的可扩展性。
4. 文本编辑渲染系统：针对文本编辑中自然图像文本内容稀缺的问题，开发了可控文本渲染系统，精确控制文本内容和视觉属性（字体、颜色、大小、位置），系统性地生成配对图像，确保了编辑指令的真实性。

<h2 id="8.Z-Image架构中如何针对图像编辑任务进行专门配置">8.Z-Image架构中如何针对图像编辑任务进行专门配置</h2>

为了支持图像编辑任务，Z-Image在S3-DiT的文生图架构进行了以下增强和配置：

1. 语义增强： 架构中加入了 SigLIP 2 模型 来捕获来自参考图像的抽象视觉语义。
2. 统一输入： 在序列层面连接文本、视觉语义 token 和 VAE 图像 token，以最大限度地提高参数效率。
3. 时序区分： 在编辑任务中，源图像（参考图像）和目标图像的 token 会被分配对齐的空间 RoPE 坐标，但通过时间维度上的单位间隔偏移进行分离，以区分两者是干净图像还是带噪图像

<h2 id="9.Z-Image如何为图像编辑任务生成caption">9.Z-Image如何为图像编辑任务生成caption</h2>

编辑数据一般需要source_image -> target_image的指令化文本，Z-Captioner 采用三步思维链（CoT）流程系统地分解编辑任务：

1. 详细标注： 分别为源图像和目标图像生成包含 OCR 信息的综合标注。
2. 差异分析： 模型利用原始图像及其生成的标注进行比较分析，识别所有视觉和文本差异，列出两张图片的“增删改”物体清单。
3. 指令合成： 模型基于识别出的差异，生成简洁的编辑指令，比如：“把红色裙子更换为蓝色裙子，删除左肩背包，背景换成悉尼歌剧院”。

总的来说，这样的好处是可以让模型学会细粒度的局部编辑，而非仅仅是全图转换。并且差异文本可以和编辑图像对一一对应，提升了训练数据的有效性，可以校验有没有位置和内容是否修改正确。并且可以支持混合编辑，单条指令覆盖多项任务。

<h2 id="10.Z-Image-Edit的训练阶段有哪些">10.Z-Image-Edit的训练阶段有哪些</h2>

Z-Image-Edit在Z-Image文生图基础模型的基础上，通过继续训练来强化编辑能力，该阶段分为两步：
1. 预训练阶段： 使用构建好的编辑对数据和文生图(T2I) SFT 数据一起训练。首先在 512x512 分辨率上进行几千步的训练，以快速适应编辑任务，然后提高到1024x1024分辨率以追求更高的生成质量。由于图像编辑数据对价格昂贵且难以获取，它们的总量明显小，而且与T2I数据相比，多样性要小得多，为了避免性能下降，建议T2I数据和I2I数据的比例相对较高（例如 4:1）。
2. SFT 阶段： 手动构建了各项任务平衡的高质量子集，以进一步提高模型的整体性能，特别是指令遵循能力。在最终训练阶段，虽然易于获取且指令遵循精度高的合成数据（例如用于文本编辑的渲染文本数据），但由于其远离真实世界用户输入的分布，因此被大量降采样。


<h2 id="11.Z-Image-Edit中Prompt Enhancer的作用">11.Z-Image-Edit中Prompt-Enhancer的作用</h2>

Z-Image-Edit的Prompt Enhancer (PE)至关重要，并且全程无需不产生额外的LLM/VLM训练成本，仅需要使用一个fixed pretrained LLM/VLM。PE也参与到了训练过程，在SFT阶段时，也使用了PE后的数据进行训练，该策略确保Z-Image-Edit在SFT期间与增强后的提示保持一致。其作用包括：

1. 处理歧义和不清晰的意图： PE能够解决用户指令中存在的含糊不清的意图。
2. 世界知识和推理注入： PE能够注入世界知识并支持推理，例如在论文示例中，PE解决了因缺乏配料与菜肴之间关系推理而导致的错误。这与它在文生图任务中的功能相似。

<h2 id="12.Z-Image的预训练阶段是如何设计的">12.Z-Image的预训练阶段是如何设计的</h2>

Z-Image的预训练阶段是其训练策略的核心部分，主要分为两个关键阶段：pretraining和omni-pretraining:

1. Pretraining阶段，这一阶段是模型获取基础能力的引导阶段，具有以下特点：
- 目标： 快速高效地获取基础的视觉-语义对齐和合成知识。
- 分辨率和任务： 训练完全以 256x256分辨率进行，任务仅限于文生图生成。
- 知识获取： 模型的大部分基础视觉知识（例如，中文文本渲染能力）是在这个低分辨率训练阶段获得的。
- 计算消耗： 尽管分辨率低，该阶段消耗了总预训练计算量超过一半的资源（约 147.5K H800 GPU 小时）
- 训练效率优化： 为了应对多分辨率训练带来的信噪比（SNR）变化，训练过程中采用了 动态时间偏移策略，确保噪声水平能根据不同的图像分辨率进行适当缩放，从而实现更有效的训练

2. Omni-pretraining阶段，这是一个统一的多任务阶段，旨在整合多种能力，并分摊沉重的预训练预算，从而避免了单独、资源密集型阶段的需要。这一阶段体现在以下三个关键方面:
- 任意分辨率训练：Z-Image 采用任意分辨率训练策略。原始图像分辨率被映射到一个预定义的训练分辨率范围内（分桶策略）。这种方式允许模型学习跨尺度的视觉信息，减轻了固定分辨率下采样造成的信息损失。完成后，模型能够生成高达 1k-1.5k 范围的任意分辨率图像，并且同时支持图像和文本输入。
- 文生图与图生图联合训练：Z-Image在omni-pretraining阶段将图生图任务也整合到了预训练框架中，该阶段利用大规模、自然出现且弱对齐的图像对数据进行训练，从而提升了模型的图像编辑能力。这种联合训练策略使得模型能够在单一架构下同时掌握生成和编辑两种任务，提高了其多功能性和适应性。学习自然图像对之间的关系也为后续的图像编辑等下游任务提供了强大的初始化。重要的是，这种联合预训练机制没有对文生图任务的性能引入可察觉的退化。
- 多级别和双语标注训练：采用 Z-Captioner 生成双语、多级别合成标注，包括长、中、短描述、标签和模拟用户提示； 原始的文本元数据也会以小概率被纳入，以进一步增强模型对世界知识的获取；使用不同粒度和视角的标注（例如，包含长描述、短描述和标签）能够提供广泛的模式覆盖；对于图生图任务，模型会随机采样目标图像的标注或成对差异标注，分别对应于参考指导的图像生成或多任务图像编辑。

在pretraining和omni-pretraining阶段结束后，模型具备了根据图像和文本输入进行条件化输出的能力，为后续的 Z-Image和Z-Image-Edit模型的训练提供了合适的初始点。