# 目录

- [1.Ip-adapter的模型结构与原理](#1.Ip-adapter的模型结构与原理)
- [2.ControlNet的模型结构与原理](#2.ControlNet的模型结构与原理)
- [3.人物一致性模型PhotoMaker原理](#3.人物一致性模型PhotoMaker原理)
- [4.人物一致性模型InstantID原理](#4.人物一致性模型InstantID原理)
- [5.单ID图像为什么InstantID人物一致性比Photomaker效果好](#5.单ID图像为什么InstantID人物一致性比Photomaker效果好)
- [6.ControlNet++的模型结构和原理](#6.ControlNet++的模型结构和原理)
- [7.Controlnext的模型结构和原理](#7.Controlnext的模型结构和原理)
- [8.ODGEN的模型结构和原理](#8.ODGEN的模型结构和原理)
- [9.Controlnet如何处理条件图的？](#9.Controlnet如何处理条件图的？)
- [10.加入Controlnet训练后，训练时间和显存的变化？](#10.加入Controlnet训练后，训练时间和显存的变化？)
- [11.T2I-Adapter的模型结构和原理](#11.T2I-Adapter的模型结构和原理)
- [12.T2I-Adapter和ControlNet的异同点是什么？](#12.T2I-Adapter和ControlNet的异同点是什么？)
- [13.InstanceDiffusion的模型结构和原理](#13.InstanceDiffusion的模型结构和原理)
- [14.BeyondScene的模型结构和原理](#14.BeyondScene的模型结构和原理)
- [15.HiCo的模型结构和原理](#15.HiCo的模型结构和原理)
- [16.LayoutDM的模型结构和原理](#16.LayoutDM的模型结构和原理)
- [17.LayoutDIffusion的模型结构和原理](#17.LayoutDIffusion的模型结构和原理)
- [18.LayoutDiffuse的模型结构和原理](#18.LayoutDiffuse的模型结构和原理)
- [20.AnyScene的模型结构和原理](#20.AnyScene的模型结构和原理)
- [21.MIGC的模型框架和原理](#21.MIGC的模型框架和原理)
- [22.介绍一下Training-free Composite Scene Generation for Layout-to-Image Synthesis](#22.介绍一下Training-free-Composite-Scene-Generation-for-Layout-to-Image-Synthesis)
- [23.Isolated Diffusion的框架和原理](#23.Isolated-Diffusion的框架和原理)
- [24.MIGC++的框架和原理](#24.MIGC++的框架和原理)
- [25.DynamicControl的框架和原理](#25.DynamicControl的框架和原理)
- [26.MaxFusion的框架和原理](#26.MaxFusion的框架和原理)
- [27.CreatiLayout的框架和原理](#27.CreatiLayout的框架和原理)
- [28.Ctrl-X的框架和原理](#28.Ctrl-X的框架和原理)
- [29.OMNIBOOTH的框架和原理](#29.OMNIBOOTH的框架和原理)
- [30.EasyPhoto的训练和推理流程是什么样的？](#30.EasyPhoto的训练和推理流程是什么样的？)
- [31.FaceChain的训练和推理流程是什么样的？](#31.FaceChain的训练和推理流程是什么样的？)
- [32.ReCo的框架和原理](#32.ReCo的框架和原理)
- [33.Be Yourself（Bounded Attention for Multi-Subject Text-to-Image Generation）的框架和原理](#33.Be-Yourself（Bounded-Attention-for-Multi-Subject-Text-to-Image-Generation）的框架和原理)
- [34.IFadapter的框架和原理](#34.IFadapter的框架和原理)
- [35.LAW-Diffusion的框架和原理](#35.LAW-Diffusion的框架和原理)
- [36.Check, Locate, Rectify（A Training-Free Layout Calibration System for Text-to-Image Generation）的框架和原理](#36.Check,Locate,Rectify（A-Training-Free-Layout-Calibration-System-for-Text-to-Image-Generation）的框架和原理)
- [37.Conceptrol的框架和原理](#37.Conceptrol的框架和原理)
- [38.EasyPhoto中应用到了哪些人脸特征处理算法？](#38.EasyPhoto中应用到了哪些人脸特征处理算法？)
- [39.Prompt-to-Prompt是什么方法？](#39.Prompt-to-Prompt是什么方法？)
- [40.InstructPix2Pix的训练和推理流程是什么样的？](#40.InstructPix2Pix的训练和推理流程是什么样的？)
- [41.ControlNet系列模型有多少种控制条件？](#41.ControlNet系列模型有多少种控制条件？)
- [42.ControlNet的最小单元是什么样的？](#42.ControlNet的最小单元是什么样的？)
- [43.ControlNet中的zero卷积层初始权重为什么是0?zero卷积层为什么有效？](#43.ControlNet中的zero卷积层初始权重为什么是0?zero卷积层为什么有效？)
- [44.ConrtolNet是如何训练的？](#44.ConrtolNet是如何训练的？)
- [45.ControlNet的损失函数是什么？](#45.ControlNet的损失函数是什么？)
- [46.ControlNet有哪些高阶用法？](#46.ControlNet有哪些高阶用法？)
- [47.ControlNet中Balanced、My prompt is more important、ControlNet is more important三种模式的区别是什么？](#47.ControlNet中Balanced、My-prompt-is-more-important、ControlNet-is-more-important三种模式的区别是什么？)
- [48.ControlNet 1.1与ControlNet相比，有哪些改进？](#48.ControlNet-1.1与ControlNet相比，有哪些改进？)
- [49.介绍一下ControlNet Canny条件控制的原理](#49.介绍一下ControlNet-Canny条件控制的原理)
- [50.介绍一下ControlNet Depth控制条件的原理](#50.介绍一下ControlNet-Depth控制条件的原理)
- [51.介绍一下ControlNet NormalMap控制条件的原理](#51.介绍一下ControlNet-NormalMap控制条件的原理)
- [52.介绍一下ControlNet OpenPose控制条件的原理](#52.介绍一下ControlNet-OpenPose控制条件的原理)
- [53.介绍一下ControlNet MLSD控制条件的原理](#53.介绍一下ControlNet-MLSD控制条件的原理)
- [54.介绍一下ControlNet Lineart控制条件的原理](#54.介绍一下ControlNet-Lineart控制条件的原理)
- [55.介绍一下ControlNet SoftEdge控制条件的原理](#55.介绍一下ControlNet-SoftEdge控制条件的原理)
- [56.介绍一下ControlNet Scribble/Sketch控制条件的原理](#56.介绍一下ControlNet-Scribble/Sketch控制条件的原理)
- [57.介绍一下ControlNet Segmentation控制条件的原理](#57.介绍一下ControlNet-Segmentation控制条件的原理)
- [58.介绍一下ControlNet Shuffle控制条件的原理](#58.介绍一下ControlNet-Shuffle控制条件的原理)
- [59.介绍一下ControlNet Tile/Blur控制条件的原理](#59.介绍一下ControlNet-Tile/Blur控制条件的原理)
- [60.介绍一下Controlnet Inpaint控制条件的原理](#60.介绍一下Controlnet-Inpaint控制条件的原理)
- [61.介绍一下ControlNet InstryctP2P控制条件的原理](#61.介绍一下ControlNet-InstryctP2P控制条件的原理)
- [62.介绍一下一下ControlNet Reference-only控制条件的原理](#62.介绍一下一下ControlNet-Reference-only控制条件的原理)
- [63.介绍一下ControlNet Recolor控制条件的原理](#63.介绍一下ControlNet-Recolor控制条件的原理)
- [64.训练ControlNet模型的流程中有哪些关键参数？](#64.训练ControlNet模型的流程中有哪些关键参数？)
- [65.ControlNet模型的训练流程一般包含哪几部分核心内容？](#65.ControlNet模型的训练流程一般包含哪几部分核心内容？)
- [66.ControlNet有哪些万金油级应用案例？](#66.ControlNet有哪些万金油级应用案例？)
- [67.SD/SDXL/FLUX.1 ControlNet之间有哪些区别？](#67.SD/SDXL/FLUX.1-ControlNet之间有哪些区别？)
- [68.介绍一下Controlnet-Union的原理](#68.介绍一下Controlnet-Union的原理)
- [69.ControlNet是如何起作用的？](#69.ControlNet是如何起作用的？)
- [70.ControlNet模型包含十几种控制功能可以归位几大类？](#70.ControlNet模型包含十几种控制功能可以归位几大类？)


<h2 id="1.Ip-adapter的模型结构与原理">1.Ip-adapter的模型结构与原理 </h2>
IP-Adapter 采用了一种解耦的交叉注意力机制，将文本特征和图像特征分开处理，从而使得生成的图像能够更好地继承和保留输入图像的特征。

![](./imgs/Ip-adapter.png)
图像编码：IP-Adapter 使用预训练的 CLIP（Contrastive Language-Image Pre-training）图像编码器来提取图像提示的特征。

解耦交叉注意力机制：IP-Adapter 通过这种机制，将文本特征的 Cross-Attention 和图像特征的 Cross-Attention 分区开来。在Unet 的模块中新增了一路 Cross-Attention 模块，用于引入图像特征。

适配模块：IP-Adapter 包含一个图像编码器和包含解耦交叉注意力机制的适配器。这个适配器允许模型在生成图像时，同时考虑文本提示和图像提示，生成与文本描述相匹配的图像。


<h2 id="2.ControlNet的模型结构与原理">2.ControlNet的模型结构与原理 </h2>

![](./imgs/Controlnet.png)

权重克隆：ControlNet 将大型扩散模型的权重克隆为两个副本，一个“可训练副本”和一个“锁定副本”。锁定副本保留了从大量图像中学习到的网络能力，而可训练副本则在特定任务的数据集上进行训练，以学习条件控制。

零卷积：ControlNet 引入了一种特殊类型的卷积层，称为“零卷积”。这是一个 1x1 的卷积层，其权值和偏差都初始化为零。零卷积层的权值会从零逐渐增长到优化参数，这样设计允许模型在训练过程中逐渐调整和学习条件控制，而不会对深度特征添加新的噪声。

特征融合：ControlNet 通过零卷积层将额外的条件信息融合到神经网络的深层特征中。这些条件可以是姿势、线条结构、颜色分布等，它们作为输入调节图像，引导图像生成过程。

灵活性和扩展性：ControlNet 允许用户根据需求选择不同的模型和预处理器进行组合使用，以实现更精准的图像控制和风格化。例如，可以结合线稿提取、颜色控制、背景替换等多种功能，创造出丰富的视觉效果。


<h2 id="3.人物一致性模型PhotoMaker原理">3.人物一致性模型PhotoMaker原理 </h2>

编码器（Encoders）：PhotoMaker使用CLIP图像编码器来提取图像嵌入（image embeddings），并通过微调部分Transformer层来优化模型对于屏蔽图像中ID特征的提取能力。

堆叠ID嵌入（Stacked ID Embedding）：通过将多个输入ID图像的编码堆叠起来，构建了一个统一的ID表示。这个表示能够保留每个输入ID图像的原始特征，并在推理阶段接受任意数量的ID图像作为输入，从而保持高生成效率。

融合（Merging）：利用扩散模型中的交叉注意力机制来自适应地合并堆叠的ID嵌入中包含的ID信息，从而在生成过程中实现对ID特征的有效控制。

ID导向的数据构建流程（ID-Oriented Human Data Construction）：为了支持PhotoMaker的训练，研究者设计了一个自动化流程来构建一个以ID为中心的人类数据集。这个数据集包含大量具有不同表情、属性、场景的图像，有助于训练过程中的ID信息学习。

![](./imgs/Photomaker.png)


<h2 id="4.人物一致性模型InstantID原理">4.人物一致性模型InstantID原理 </h2>
ID Embedding：InstantID利用预训练的人脸模型（如insightface库中的模型）来提取面部特征的语义信息，这些特征被称为ID Embedding。与使用CLIP模型相比，这种方法能够更精准和丰富地捕获人物面部表情的特征。

Image Adapter：这是一个轻量级的自适应模块，它结合了文本提示信息和图像提示信息。该模块的设计思路与IP-Adapter相似，通过解耦交叉注意力机制来适应不同的生成任务。

IdentityNet：为了在粗粒度上改进图像生成并更精确地控制人物ID图像的生成，InstantID引入了IdentityNet模块。这个模块利用弱空间信息（如面部关键点）和强语义信息（从ID Embedding模块提取的面部表情特征）来引导图像的生成。

ControlNet：InstantID还使用了ControlNet来增强面部特征提取，进一步提高图像生成的质量和准确性。
![](./imgs/InstantID.png)


<h2 id="5.单ID图像为什么InstantID人物一致性比Photomaker效果好">5.单ID图像为什么InstantID人物一致性比Photomaker效果好</h2>

1.InstantID利用预训练的人脸模型（如insightface库中的模型）来提取面部特征的语义信息。与Photomaker所使用的CLIP模型相比，这种方法能够更精准和丰富地捕获人物面部表情的特征。

2.InstantID还使用ControlNet来增强面部特征提取，进一步提高图像生成的质量和准确性。

3.Photomaker是先将文本特征和图像特征通过MLPs融合，再做CrossAttention加入U-net.InstantID是图像特征和文本特征分开做CrossAttention,再融合。（可以认为是区别，不要一定是效果好的原因）


<h2 id="6.ControlNet++的模型结构和原理">6.ControlNet++的模型结构和原理</h2>

论文链接：https://arxiv.org/pdf/2404.07987.pdf

参照cyclegan，采用预先训练的判别优化循环一致性损失。

输入的条件信息与从生成的图像中提取出来的条件信息做损失，如mask。

![img](./imgs/controlnet++_reward_loss.jpg)

![img](./imgs/controlnet++框架.jpg) 

同时提出了一种通过添加噪声扰乱训练图像的一致性，并使用单步去噪图像进行奖励微调的新策略。相比从随机噪声开始多步采样，此方法显著减少了时间和内存成本，同时保持了高效的奖励微调,最终提高了生成图像与输入条件的一致性。

![img](./imgs/Controlnet++_reward.jpg)


<h2 id="7.Controlnext的模型结构和原理">7.Controlnext的模型结构和原理</h2>

论文链接：https://arxiv.org/pdf/2408.06070

该论文提出了一种名为ControlNeXt的新方法，用于高效且可控的图像和视频生成。作者指出，现有的可控生成方法通常需要大量的额外计算资源，尤其是在视频生成方面。此外，这些方法在训练时也面临挑战，如控制能力弱或收敛缓慢。为了解决这些问题，作者设计了一种更为简单且高效的架构，并提出了新的技术，如跨归一化（Cross Normalization），以替代传统的零卷积（Zero Convolution），从而实现更快更稳定的训练收敛。

### 主要内容：

1. **ControlNeXt架构**：该架构减少了与基础模型相比的额外成本，提供了一种轻量级的结构，能够无缝整合其他权重（如LoRA权重），无需额外训练即可实现风格转换。
2. **跨归一化技术**：作者提出了一种新的归一化方法，用于大规模预训练模型的微调，使得训练更加高效和稳定。
3. **参数效率**：ControlNeXt减少了多达90%的可学习参数，并在多个基于扩散模型的图像和视频生成任务中展示了其鲁棒性和兼容性。

整体训练框架如下图所示：

![image-20240902204404683](./imgs/Controlnext.png)

ControlNeXt 采用精简的架构，消除了繁重的辅助组件，从而最大限度地减少延迟开销并减少可训练参数。这种轻量级设计使其能够充当具有强大鲁棒性和兼容性的即插即用模块，进一步允许与其他 LoRA 权重集成以改变生成样式，而无需额外训练。提出的交叉归一化，用新引入的参数对预训练的大型模型进行微调，从而促进更快、更稳定的训练收敛。


<h2 id="8.ODGEN的模型结构和原理">8.ODGEN的模型结构和原理 </h2>

论文链接：https://arxiv.org/pdf/2405.15199

ODGEN（**O**bject **D**etection **GEN**eration）是一个基于扩散模型（Diffusion Model）的图像生成方法，专门用于生成领域特定的物体检测数据。其核心目标是通过控制扩散模型，生成具有多类物体、复杂场景和重叠物体的高质量图像，以此扩展训练数据，提高物体检测任务的性能。

模型框架如下：

![image-20241008202733181](./imgs/ODGEN.png)

该方法的关键技术包括：

1. **扩散模型的微调**：在领域特定的数据集上微调扩散模型，以便适应目标场景的分布。
2. **对象级别的控制策略**：通过文本提示和视觉条件，确保生成的图像准确地包含指定类别的物体，并符合空间约束（如边界框位置）。

### 1. 领域特定的扩散模型微调

#### 基本思路

扩散模型（如Stable Diffusion）通常是在大规模网络数据上进行预训练的，但这些数据的分布与实际的特定领域数据（如医疗图像、游戏场景）存在很大差异。因此，ODGEN首先在领域特定的数据集上微调扩散模型。

微调的过程中，模型不仅会使用整个图像，还会使用裁剪后的前景物体来优化生成效果。这有助于模型更好地学习前景物体的细节，同时保留生成完整背景场景的能力。

#### 具体步骤

- 首先，从数据集中裁剪前景物体，并将其调整为512×512大小的图像。
- 然后，使用基于边界框和类别的文本提示，生成整个场景和前景物体。对于场景使用模板“a <场景名称>”，对于物体使用模板“a <类别名称>”。
- 最后，通过一个重建损失函数对扩散模型进行优化，确保生成图像的质量。

### 2. 对象级别的控制策略

#### 背景问题

现有的生成方法在处理多类别物体和复杂场景时，往往会出现“概念混叠”现象，即多个类别的物体会在生成的图像中相互干扰。同时，物体重叠时，部分对象可能会丢失。因此，ODGEN引入了对象级别的控制策略，确保生成的图像严格符合指定的物体类别和位置。

#### 控制策略

ODGEN使用了两种主要的控制条件：

1. **文本列表编码**：将每个物体的类别和边界框信息转化为一个文本列表。每个物体的描述采用固定模板“a <类别名称>”，然后通过预训练的CLIP文本编码器编码为文本嵌入。嵌入结果通过卷积网络进行处理，确保每个类别的物体不会相互干扰。
2. **图像列表编码**：对于每个类别的物体，生成对应的前景图像，然后将这些图像根据边界框的位置粘贴到一个空白画布上，形成图像列表。这个图像列表可以有效避免物体之间的遮挡问题，确保每个物体都能正确生成。

#### 最终生成

生成过程通过将文本嵌入和图像嵌入同时输入到ControlNet中，实现对生成图像的精准控制。ControlNet能够处理边界框信息，确保生成的图像在空间和类别上都满足要求。

### 3. 数据集生成流程

ODGEN不仅是一个生成模型，还设计了一套完整的数据集生成流程。该流程包括以下步骤：

1. **对象分布估计**：根据训练数据集中的边界框属性，估计每个类别的物体数量、位置、面积和宽高比等统计分布。
2. **伪标签生成**：从估计的分布中随机采样生成边界框和类别信息，作为生成图像的条件。
3. **图像合成**：使用ODGEN生成带有伪标签的新图像。
4. **标签过滤**：使用训练好的分类器对生成的图像进行过滤，剔除那些未能生成前景物体的图像，确保生成数据的高质量。

流程图如下：

![ODGEN_data_pipeline](./imgs/ODGEN_data_pipeline.png)


<h2 id="9.Controlnet如何处理条件图的？">9.Controlnet如何处理条件图的？</h2>

我们知道在 sd 中，模型会使用 VAE-encoder 将图像映射到隐空间，512×512 的像素空间图像转换为更小的 64×64 的潜在图像。而 controlnet 为了将条件图与 VAE 解码过的特征向量进行相加，controlnet 使用了一个小型的卷积网络，其中包括一些普通的卷积层，搭配着 ReLU 激活函数来完成降维的功能。


<h2 id="10.加入Controlnet训练后，训练时间和显存的变化？">10.加入Controlnet训练后，训练时间和显存的变化？</h2>

在论文中，作者提到，与直接优化 sd 相比，优化 controlnet 只需要 23% 的显存，但是每一个 epoch 需要额外的 34% 的时间。可以方便理解的是，因为 controlnet 其实相当于只优化了unet-encoder，所以需要的显存较少，但是 controlnet 需要走两个网络，一个是原 sd 的 unet，另一个是复制的 unet-encoder，所以需要的时间会多一些。


<h2 id="11.T2I-Adapter的模型结构和原理">11.T2I-Adapter的模型结构和原理</h2>

![T2I-Adapter的模型结构和原理](./imgs/T2I-adapter.png)

如图所示为 T2I-Adapter 模型结构，它包含了四个特征提取模块和三个下采样模块。模型首
先利用了 Pixel Unshuffle 来将 512x512 的图片下采样到 64x64，然后经过 Adapter 模块，他会输出四个不同尺寸的特征图 
$$F_c = \{F^1_c, F^2_c, F^3_c, F^4_c\}$$
注意Adapter 输出的特征图的大小是和 Unet-encoder 输出的特征图
$$F_enc = \{F^1_enc, F^2_enc, F^3_enc, F^4_enc\}$$
大小一致，然后再通过按位相加的方式来完成特征融合：
$$F^i_enc=F^i_enc+F^i_c, i∈\{1,2,3,4\}$$


<h2 id="12.T2I-Adapter和ControlNet的异同点是什么？">12.T2I-Adapter和ControlNet的异同点是什么？</h2>

- 如何将图片转化为隐变量：controlnet 和 T2I-Adapter 采用了不同的方法来将图片转化为隐向量。在 controlnet 中，作者是设计了一个小型的卷积网络，这其中包括了不同的卷积和激活函数，其中由 4x4 和 2x2 的卷积，来完成下采样的功能，而在 T2I-Adapter 中，作者是使用了Pixel-Unshuffle 的操作来完成该功能。

- Condition Encoder：controlnet 和 T2I-Adapter 采用了不同的方式来对条件控制信息进行编码。在 controlnet 中，作者是直接将 sd-encoder 将复制了一份，当作可训练模块，因为作者认为，sd 是在 10 亿级别上进行训练的，它的 encoder 足够强大和鲁棒，因此作者并没有专门设计针对条件信息的编码器。而在 T2I-Adapter 的作者是设计了一个小型的卷积网络，来完成下采样的功能，让下采样倍数和 sd-encoder 的下采样倍数一样，然后通过按位相加的方式来完成特征融合。

- 参数量和训练时间：前面我们提到了，controlnet 的 condition encoder 是 sd-encoder，这其中的参数数目比 T2I-Adapter 的参数要多的多，因此，controlnet 的训练时间也比 T2I-Adapter要长。


<h2 id="13.InstanceDiffusion的模型结构和原理">13.InstanceDiffusion的模型结构和原理</h2>

论文链接：[2402.03290](https://arxiv.org/pdf/2402.03290)

### 模型结构和原理

**InstanceDiffusion** 是一种用于文本到图像生成的扩散模型，旨在实现对图像中个体实例的精确控制。该模型允许用户通过文本描述和多种空间条件（如点、框、涂鸦、掩码）来指定每个实例的位置和属性，实现对图像生成的灵活控制。

### 关键技术

**多实例处理**：对每个实例进行单独的条件编码，使得不同实例之间的信息隔离，减少特征互相干扰。

**跳跃连接的通道缩放**：通过傅里叶变换对低频特征进行精确缩放，提升模型对实例的响应能力，特别是位置和姿态的控制。

**实例融合去噪**：在生成过程中，模型通过对每个实例的独立去噪与全局特征的融合，提升生成图像的质量和一致性。

- ![image-20241021160616972](./imgs/instancediffusion.png)

### 关键组件

#### UniFusion Block

- **功能**：将实例级别的条件（包括文本描述和位置条件）融入到扩散模型的视觉特征中。

- **工作方式**：
  
  - 不同位置条件（如框、掩码、点、涂鸦）被转化为二维点集，通过傅里叶映射进行特征化。
  
  - 文本描述则通过 CLIP 文本编码器编码。
  
  - UniFusion Block 将这些特征与模型主干的视觉特征融合，从而在生成过程中提供实例级别的控制。
  
    ![image-20241021160711438](./imgs/instancediffusion_unifusion_block.png)

#### ScaleU Block

- **功能**：重新校准 UNet 模型中的主干特征和跳跃连接特征，以更好地遵循输入的布局条件。
- **工作方式**：
  - 对于主干特征和跳跃连接特征，使用可学习的通道缩放向量进行重新校准。
  - 通过傅里叶变换，仅对跳跃连接特征的低频成分进行缩放，从而提升模型对实例位置的精准响应。

#### Multi-instance Sampler

- **功能**：在生成图像时降低不同实例条件之间的信息泄漏，提升生成图像的质量和一致性。

- **工作方式**：
  
  - 对每个实例进行独立的去噪操作，生成实例特征。
  
  - 将这些特征与全局特征进行融合，再对融合后的特征进行进一步去噪。
  
    ![image-20241021160902698](./imgs/Multi-instance_Sampler.png)
  

<h2 id="14.BeyondScene的模型结构和原理">14.BeyondScene的模型结构和原理</h2>

### 模型结构和原理

**BeyondScene**是一个基于预训练扩散模型的创新框架，旨在生成高分辨率、以人为中心的场景。该方法解决了现有扩散模型在生成高分辨率复杂场景中的局限性，尤其是涉及多个人物时的场景生成。以下是该模型的主要结构和工作原理。

![image-20241021162011652](./imgs/BeyondScene.png)

#### 关键技术
**分阶段生成**：模型首先生成基础图像，然后逐步细化场景细节，尤其是人物的姿态和外观。初始生成阶段关注场景的大致布局，后续阶段增加细节，提升图像分辨率和细节表现。

**实例感知放大**：在放大过程中，BeyondScene通过高频细节注入技术（如基于Canny边缘检测）提高图像清晰度，确保生成图像在放大时不会失去关键细节。

**自适应联合扩散**：动态调整扩散步幅，确保关键区域（如人物）的精细度和视觉一致性，避免多个视图中出现重复的对象或模糊区域。

#### **关键组件**
- **姿势引导的T2I扩散模型**：在生成基础图像时，利用现有的姿势引导T2I扩散模型生成多个人物实例。通过对实例的分割、裁剪和调整大小，BeyondScene确保了每个人物实例的细节精确。
- **自适应步幅和视图条件**：在联合扩散过程中，BeyondScene根据每个视图中的实例情况动态调整扩散的步幅和文本条件，确保场景中的每个部分都能精确控制，尤其是人物的姿势和外观。

![image-20241021162101382](./imgs/Adaptive_joint_diffusion.png)

- **高频细节注入**：在图像放大过程中，通过边缘检测和像素扰动技术注入高频细节，避免模糊效果，提升最终图像的清晰度。

![image-20241021162056066](./imgs/High_frequency_injected_foward_diffusion.png)


<h2 id="15.HiCo的模型结构和原理">15.HiCo的模型结构和原理</h2>

论文链接：[2410.14324](https://arxiv.org/pdf/2410.14324)

### 模型结构和原理

HiCo模型采用了**层次化可控扩散模型**（Hierarchical Controllable Diffusion Model），其中核心是多分支的网络结构。这个结构的主要模块和组件如下：

- **多分支结构**：HiCo模型设计了多个分支，每个分支负责特定的布局区域的生成，包括背景和多个前景对象。通过权重共享，HiCo能够在不同分支间实现一致的特征表示，同时又能分别建模不同区域的细节。
- **控制条件的引入**：HiCo受到ControlNet和IP-Adapter的启发，在分支中引入了外部条件，通过**边路结构**来引导生成过程。这些条件包括文本描述（caption）和边界框（bounding box）等。
- **融合模块（Fuse Net）**：多分支网络生成的各区域特征最终在Fuse Net中融合。Fuse Net支持多种融合方式，包括加和、平均、掩码（mask）等。默认使用的是掩码方式，通过在生成过程中对不同区域进行掩码处理，实现不同前景和背景区域的解耦。

![image-20241104172029684](./imgs/HiCo.jpg)

HiCo的工作原理基于以下步骤：

### （1）布局建模

HiCo首先将输入的布局信息（如文本和位置）解耦成不同的区域，由每个分支独立处理。每个分支在生成过程中会提取该区域的局部特征，并利用权重共享机制确保全局一致性。

### （2）扩散过程

HiCo基于扩散模型（Diffusion Model）的原理，从随机噪声逐步生成图像。在每个扩散步骤中，模型会使用特定的控制条件（如文本描述和位置信息）来指导图像生成。HiCo在扩散的不同阶段引入了多个层次的控制条件，从而确保图像生成符合设定的布局。

### （3）特征融合

在完成每个区域的特征提取后，HiCo将这些特征在Fuse Net中进行融合。Fuse Net通过掩码操作或加权方式，将不同区域的特征整合在一起，生成和谐的全局图像。这种方式不仅能够保持每个区域的独立性，还能保证整个图像在风格、光照和结构上的一致性。

### （4）支持多概念扩展

HiCo支持多种概念扩展，允许在生成过程中加入多个LoRA插件，用于实现个性化、多语言生成等功能。此外，HiCo还支持快速生成插件（如LCM-LoRA），能够加速分辨率512x512和1024x1024的图像生成。


<h2 id="16.LayoutDM的模型结构和原理">16.LayoutDM的模型结构和原理</h2>

论文链接：[[2305.02567\] LayoutDM: Transformer-based Diffusion Model for Layout Generation](https://arxiv.org/abs/2305.02567)

基于条件布局去噪器（cLayoutDenoiser）逐步去噪，生成符合给定属性的布局。

**条件布局去噪器（cLayoutDenoiser）**：

- 完全基于 Transformer 架构，包含以下模块：
  - **几何嵌入（Geometric Embedding）**：将布局元素的几何参数（中心坐标与尺寸）嵌入到特定维度。
  - **属性嵌入（Attributes Embedding）**：将元素的类别标签或文本特征嵌入到特定维度。
  - **时间步嵌入（Timestep Embedding）**：引入正弦时间步嵌入，使模型感知时间步 ttt。
  - **元素嵌入（Element Embedding）**：融合几何、属性和时间信息，为 Transformer 提供输入。

![image-20241118202625628](./imgs/layoutdm-2023.png)


<h2 id="17.LayoutDIffusion的模型结构和原理">17.LayoutDIffusion的模型结构和原理</h2>

论文链接：[[2303.17189\] LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation](https://arxiv.org/abs/2303.17189)

#### 模型结构

LayoutDiffusion是一种用于布局到图像生成的扩散模型，旨在通过高质量生成和精确控制来处理复杂的多对象场景。其主要结构包括以下几个部分：

1. **布局嵌入（Layout Embedding）**：
   - 将布局表示为对象集合，每个对象通过其边界框（bounding box）和类别ID来定义。
   - 通过投影矩阵将边界框和类别嵌入映射到高维空间，并对布局序列进行固定长度填充。
2. **布局融合模块（Layout Fusion Module, LFM）**：
   - 基于自注意力机制的Transformer编码器，促进布局中多个对象之间的交互，生成融合后的布局嵌入。
   - 提升对复杂场景的理解能力。
3. **图像-布局融合模块（Image-Layout Fusion Module）**：
   - 通过构建包含位置和尺寸信息的结构化图像补丁，将图像补丁视为特殊对象，以实现图像和布局在统一空间中的融合。
   - 包含两种融合方式：
     - **全局条件（Global Conditioning）**：直接加和全局布局嵌入。
     - **基于对象的交叉注意力（Object-aware Cross Attention, OaCA）**：同时关注对象的类别、位置和尺寸信息，实现对局部布局的精细控制。

![image-20241118201021782](./imgs/layoutdiffusion.png)


<h2 id="18.LayoutDiffuse的模型结构和原理">18.LayoutDiffuse的模型结构和原理</h2>

论文链接：[[2302.08908\] LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation](https://arxiv.org/abs/2302.08908)

#### 模型结构

LayoutDiffuse 基于 Latent Diffusion Model (LDM)，通过以下两个关键组件对预训练的扩散模型进行适配：

1. **布局注意力层（Layout Attention）**：

   - 在扩散模型的自注意力机制中加入实例感知的区域注意力，使模型更专注于布局中的实例特征。

   - 每个实例通过可学习的类别嵌入（Instance Prompt）进行标记，以增强实例感知。

   - 背景区域使用 Null Embedding，并结合前景-背景掩码进行特征融合。

     ![image-20241118201547348](./imgs/layoutdiffuse-layout_attention.png)

2. **任务自适应提示（Task-Adaptive Prompts）**：

   - 向 QKV 注意力层添加任务提示，帮助模型从预训练任务（如文本到图像生成）适配到布局到图像生成任务。
   - 通过在注意力层的键值（Key 和 Value）中加入可学习嵌入，提供布局生成任务的额外上下文信息。

   整体结构：

![image-20241118201401444](./imgs/layoutdiffuse.png)


<h2 id="20.AnyScene的模型结构和原理">20.AnyScene的模型结构和原理</h2>

论文链接:[AnyScene: Customized Image Synthesis with Composited Foreground | IEEE Conference Publication | IEEE Xplore](https://ieeexplore.ieee.org/document/10657089)

![image-20241216181530973](./imgs/anyscene_model.jpg)

### 1、获取合成前景

在框架的第一阶段，系统需要处理用户提供的元素图像和场景描述文本。首先对原始图像进行背景移除处理，获取纯净的前景元素，然后通过复制粘贴的方式将多个前景元素组合在一起，同时配合用户提供的文本描述，为后续的场景生成做好准备工作。

### 2、生成整体场景

这是框架的核心阶段，它采用了三个关键组件共同工作：前景注入模块将前景信息有效地注入到预训练的扩散模型中；布局控制策略通过前景蒙版来确保生成过程中前景元素的位置和形状得到准确保持；预训练扩散模型则基于这些输入信息生成完整的场景。这个阶段将合成前景、文本提示和随机噪声作为输入，通过复杂的生成过程创建出符合要求的场景图像。

### 3、恢复前景细节

在最后的阶段，框架致力于确保生成图像的视觉质量和细节准确性。系统采用专门的图像融合技术，将原始前景元素的精细细节巧妙地融合到生成的场景中。这个过程不仅确保了前景元素的细节保真度，还能够创造出自然的过渡效果，最终输出一张视觉上和谐统一的合成图像，既保留了前景的准确性，又实现了与背景的完美融合。

示例如下：

![image-20241216181955566](./imgs/anyscene_example.jpg)

这个框架的优势在于能够保持前景细节的准确性，同时生成与之和谐的背景场景，实现自然的视觉效果。


<h2 id="21.MIGC的模型框架和原理">21.MIGC的模型框架和原理</h2>

论文链接：[2402.05408](https://arxiv.org/pdf/2402.05408)

MIGC的总体框架遵循"分而治之"的方法论，将复杂的多实例生成任务分解为若干个简单的单实例特征渲染子任务。这一框架包含三个核心部分：分解（Divide）、处理（Conquer）和组合（Combine），每个部分都针对特定的技术挑战提供了创新的解决方案。

![image-20241216182814233](./imgs/MIGC_框架.jpg)

分解（Divide）部分专注于任务的合理拆分。模型在Cross-Attention层将多实例生成任务分解为多个单实例特征渲染子任务，每个子任务负责在指定区域生成具有特定属性的实例。这种分解方式不仅提高了处理效率，还能确保最终生成结果的整体和谐性。

处理（Conquer）部分着重解决单实例生成的质量问题。该部分引入了Enhancement Attention Layer来增强每个实例的特征渲染效果，通过两阶段的处理方式（预训练Cross-Attention的初步结果和Enhancement Attention的增强结果）来确保生成质量。同时，通过引入位置感知tokens来解决相同描述但不同位置实例的混淆问题。

组合（Combine）部分负责将各个部分的结果有效整合。这一过程首先通过Layout Attention获取整体着色模板，然后使用Shading Aggregation Controller动态融合所有结果。该控制器包含实例内部注意力和实例间注意力两个层次，通过softmax机制确保每个像素位置的权重分配合理，最终生成高质量的整体结果。

### 主要模块

**Enhancement Attention Layer**（增强注意力层）是第一个关键模块。这个模块首先接收实例的位置信息（如"Blue Cat"的边界框坐标[0.10, 0.28, 0.48, 0.88]），通过Fourier编码和MLP进行位置特征提取。同时，将文本描述"Blue Cat"通过CLIP编码器处理。这两部分信息结合后进入Cross-Attention层，与图像特征进行交互。最后，该模块还使用实例mask来确保特征处理仅在指定区域进行。这样的设计有效地增强了每个实例的特征表示。

**Layout Attention Layer**（布局注意力层）是第二个重要模块。该模块首先构建注意力掩码，将输入的布局信息转换为细粒度的attention masks。然后通过查询(Q)、键(K)、值(V)三个矩阵的交互生成注意力图，这个过程考虑了不同实例之间的空间关系。通过matmul（矩阵乘法）运算后，生成一个shading template（特征模板），为后续的特征融合提供指导。

**Shading Aggregation Controller**（特征聚合控制器）是最后一个核心模块。它采用层次化的设计：首先进行Instance Intra Attention（实例内部注意力），然后是Instance Inter Attention（实例间注意力）。这两个阶段都通过softmax操作来调整特征权重。最后，通过加权求和（Sum dim=0）将所有特征整合，得到最终的特征结果。这个模块保证了多个实例特征的合理融合。

这三个模块通过精心设计的联动机制共同工作，实现了对多实例生成任务的精确控制。它们的配合不仅确保了每个实例的特征质量，也保证了实例间的合理关系，最终生成高质量的整体结果。

示例如下：

![image-20241216183035714](./imgs/MICG_example.jpg)


<h2 id="22.介绍一下Training-free-Composite-Scene-Generation-for-Layout-to-Image-Synthesis">22.介绍一下Training-free Composite Scene Generation for Layout-to-Image Synthesis</h2>

论文链接：[2407.13609](https://arxiv.org/pdf/2407.13609)

这篇文章提出的CSG(Composite Scene Generation)的训练无关框架，主要用于解决布局引导的图像生成任务。该框架的核心创新点在于如何在不需要额外训练的情况下，通过精细控制扩散模型中的注意力机制来实现准确的布局控制和高质量的图像生成。

![image-20241216200127090](./imgs/csg_model.jpg)

框架的基本架构包含三个关键组件：**选择性采样机制、交叉注意力约束和自注意力增强。**

**选择性采样机制**通过在目标区域选择最相关的注意力值并随机保留部分样本，在保持潜在噪声自然分布的同时确保充分的注意力覆盖。

**交叉注意力约束**则分为标记内约束和标记间约束，前者确保对象生成在指定区域内，后者解决语义交叉问题。

**自注意力增强**在扩散早期阶段优化像素间关系，提升局部连贯性。

工作原理是通过迭代优化过程来实现的。在每个优化步骤中，模型首先在UNet结构中捕获自注意力和交叉注意力信息，然后应用三种约束条件，最后基于组合损失更新潜在表示。在优化步骤之间，模型还采用注意力重分配机制来校正错位的注意力并减少语义交叉效应。整个过程持续TD个扩散步骤，每个步骤包含TR次优化迭代。

该框架的主要优势在于其对注意力机制的全面处理以及处理多对象复杂场景的能力。通过在保持扩散过程自然特性的同时，精心平衡空间约束和语义关系，模型能够在**不需要额外训练**的情况下实现高质量的布局控制图像生成。

示例如下：

![image-20241216200159483](./imgs/csg_example.jpg)


<h2 id="23.Isolated-Diffusion的框架和原理">23.Isolated Diffusion的框架和原理</h2>

论文链接：[2403.16954](https://arxiv.org/pdf/2403.16954)

Isolated Diffusion旨在解决文本到图像生成中"概念混淆"问题的无训练框架。这个框架主要处理两种情况：多个附属物（如一个企鹅戴着蓝帽子、红围巾和绿衬衫）和多个主体（如一只狗和一只猫）的生成问题。这种方法的核心思想是将不同概念的去噪过程隔离开来，以避免它们之间的相互干扰。主要分为以下几个流程：

第一部分 - **多附属物处理流程**（顶部黄色区域）： 这部分展示了如何处理单个主体的多个附属特征。以"戴着蓝帽子、红围巾和绿衬衫的小企鹅"为例，系统首先使用 GPT4 将输入拆分成多个子提示词，包括基础提示词"a baby penguin"和各个附属特征。这些提示词通过 CLIPtext 进行编码，然后在去噪过程中按照特定的数学公式（图中的数学表达式）进行组合，最终生成准确反映所有特征的图像。

第二部分 - **标准扩散推理流程**（中间粉色区域）： 这部分展示了传统的 Stable Diffusion 处理方式。它直接使用完整的提示词进行处理，但可能导致概念混淆的问题。图中展示了两个例子：一个是颜色特征混淆的企鹅，另一个是物种特征混淆的猫狗图像。这一部分的目的是展示未经改进的模型可能产生的问题。

第三部分 - **多主体处理流程（**底部黄色区域）： 这部分展示了如何处理多个主体的生成过程。以"一只狗在一只猫旁边"为例，系统首先使用 YOLO 进行目标检测，然后使用 SAM 模型生成精确的蒙版。在时间步骤 Tlay 处，系统通过替换其他主体区域的方式独立生成每个主体，最后通过特殊的掩码组合方式将它们整合在一起。图中的数学表达式展示了这个过程中的具体计算方法。

![image-20241216201602167](./imgs/Isolated Diffusion_model.png)

Isolated Diffusion 能够有效解决文本到图像生成中的概念混淆问题，同时保持了较高的图像质量和文本-图像一致性。

示例如下：

![image-20241216202635738](./imgs/Isolated Diffusion_example.png)


<h2 id="24.MIGC++的框架和原理">24.MIGC++的框架和原理</h2>

论文链接：[2407.02329](https://arxiv.org/pdf/2407.02329)

MIGC和MIGC++的区别：

![image-20241216202545895](./imgs/MIGC++difference.jpg)

核心架构设计差异： MIGC采用较为基础的架构，主要在U-net的中间块和深层上采样块中使用Instance Shader来控制位置和粗略属性。相比之下，MIGC++引入了更复杂的架构，不仅保留了Instance Shader，还引入了免训练的Refined Shader来替代原有的Cross-Attention层，从而实现更精细的细节控制。

处理流程的区别： 从图中的处理流程可以看出，MIGC主要通过绿色表示的Cross-Attention块和红色表示的Instance Shader进行处理。而MIGC++增加了蓝色表示的Refined Shader模块，形成了一个更完整的处理链条，使得生成过程更加精细和可控。

**主要创新：**

![image-20241216203143006](./imgs/MIGC++.jpg)

属性泄露防止机制： 论文设计了Instance Shader(实例着色器)作为核心组件，包含三个关键模块：**Enhance Attention**(增强注意力)负责单实例的精确着色，防止属性混淆；**Layout Attention**(布局注意力)创建模板来桥接各个实例，保持空间关系；**Shading Aggregation Controller**(着色聚合控制器)动态整合各个实例结果和模板，生成连贯的最终图像。

**Enhance Attention**:

![image-20241216203515667](./imgs/Enhance Attention_migc++.jpg)

**Layout Attention:**

![image-20241216203534496](./imgs/Layout Attention_migc++.jpg)

**Shading Aggregation Controller:**

![image-20241216203544325](./imgs/Shading Aggregation Controller MIGC++.jpg)

示例如下：

![image-20241216202617256](./imgs/MIGC++_example.jpg)


<h2 id="25.DynamicControl的框架和原理">25.DynamicControl的框架和原理</h2>

论文链接：[arxiv.org/pdf/2412.03255](https://arxiv.org/pdf/2412.03255)

模型pipeline：

![image-20241230185724431](./imgs/DynamicControl.png)

Double-Cycle Controller:

这个组件通过两个一致性评分来评估条件的重要性：条件一致性和图像一致性。它首先使用预训练的条件生成模型为每个条件生成图像，然后通过判别模型提取对应的反向条件，评估输入条件与提取条件的相似度，以及生成图像与源图像的像素级相似度。这种双重检查机制确保了条件选择的准确性。

![image-20241230195829740](D:\Interview-for-Algorithm-Engineer\AI绘画基础\imgs\DCC.png)

Condition Evaluator:

这是一个基于多模态大语言模型(如LLaVA)的评估系统，用于高效地对多个输入条件进行排序和评估。它通过扩展模型词汇表来处理不同类型的条件信息，并在双循环控制器的监督下学习如何最优地组合这些条件。其创新之处在于避免了生成中间图像的开销，同时不依赖源图像就能在推理阶段做出准确的条件选择。

![image-20241230193642091](./imgs/Condition Evaluator.png)

Multi-Control Adapter:

这是一个创新的并行处理架构，专门设计用来处理动态数量的控制条件。它采用混合专家系统(MoE)来并行提取不同视觉条件的特征，通过交叉注意力机制整合这些特征，最终调制ControlNet来实现更精确的图像生成控制。它的独特之处在于可以自适应地选择和组合不同数量和类型的条件，避免了固定条件数量的限制。

![image-20241230195843025](./imgs/Multi_Control_Adapter.png)

示例：

![image-20241230200422953](./imgs/DynamicControl-example.png)


<h2 id="26.MaxFusion的框架和原理">26.MaxFusion的框架和原理</h2>

论文链接：[05506.pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05506.pdf)

框架如下：

![image-20241230204451355](./imgs/MaxFusion.png)

核心发现:

![image-20241230204919620](./imgs/MaxFusion_hexin.jpg)

- 发现扩散模型中间特征图的方差图(variance maps)能够捕捉条件信息的强度
- 可以利用这个特性来融合不同模态的特征

**MaxFusion（最大融合）：** 

核心融合算法，基于特征相关性（correlation）做决策：当相关性高于阈值时进行加权平均，低于阈值时选择方差最大的特征，并引入相对标准差来确保不同模态间的公平性。

优势:

- 无需重训练,可即插即用
- 可以处理互补条件(同一物体的不同特征)和矛盾条件(不同物体的特征)
- 易于扩展到多个条件模态

示例：

![image-20241230205141743](./imgs/MaxFusion_example.png)


<h2 id="27.CreatiLayout的框架和原理">27.CreatiLayout的框架和原理</h2>

论文链接：[[2412.03859\] CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation](https://arxiv.org/abs/2412.03859)

![image-20250113201544960](./imgs/siamLayout.png)

1. 整体架构

- 基于多模态扩散变换器(MM-DiT)设计
- 将布局作为与图像和文本同等重要的独立模态
- 采用孪生(Siamese)分支结构处理不同模态间的交互

2.主要创新点：SiamLayout架构 

这个架构通过两个关键设计解决了模态竞争问题：

a) 独立模态处理：

- 使用单独的transformer参数处理布局信息
- 使布局与图像和文本具有同等地位

b) 孪生分支结构：

- 将三模态交互解耦为两个平行分支:

  - 图像-文本分支
  - 图像-布局分支

- 后期再融合两个分支的输出

- 这种设计避免了模态间的直接竞争

- 作者还对比了其他两种架构变体：

  - Layout Adapter：通过cross-attention引入布局信息
  - M³-Attention：直接将三个模态放在一起做attention

  但实验表明SiamLayout的效果最好，主要原因是它避免了模态间的直接竞争，让每个模态都能充分发挥作用。

  示例：

![image-20250113201809122](./imgs/creatiLayout_示例.png)


<h2 id="28.Ctrl-X的框架和原理">28.Ctrl-X的框架和原理</h2>

论文链接：[2406.07540](https://arxiv.org/pdf/2406.07540

Ctrl-X 是一个训练无关、指导无关的框架，通过操控预训练的 T2I 扩散模型的特征层，提供对生成图像结构（Structure）和外观（Appearance）的控制。

![image-20250113203140236](./imgs/ctrl-x.png)

该框架 **Ctrl-X** 的核心流程可概括为以下几个关键步骤：

1. **结构与外观特征提取**：通过前向扩散过程分别对输入的结构图像 (x^s_t) 和外观图像 (x^a_t) 添加噪声后，将其输入到预训练的扩散模型中，提取卷积特征和自注意力特征。
2. **特征注入与迁移**：
   - **特征注入**：将结构图像的卷积特征和注意力特征注入到目标图像 (x^o_t) 的生成过程中，确保生成图像的结构与输入对齐。
   - **外观迁移**：利用输入外观图像和目标图像的自注意力对应关系，计算加权的均值 (M) 和标准差 (S)，用于对目标图像的特征进行归一化，实现空间感知的外观迁移。
3. **生成过程**：在每一步生成中，将上述注入的结构和外观特征结合，逐步生成符合目标结构与外观的图像。

**优势**：

- 无需额外训练，直接在预训练扩散模型上运行。
- 支持任意类型的结构和外观输入，具有高度的灵活性和高效性。

示例：

![image-20250113203422258](./imgs/ctrl-x-示例.png)


<h2 id="29.OMNIBOOTH的框架和原理">29.OMNIBOOTH的框架和原理</h2>

论文链接：[2410.04932](https://arxiv.org/pdf/2410.04932)

![image-20250113202957539](./imgs/OmniBooth.png)

核心架构分为三部分：

1.双路输入处理：

- 文本路径：Instance Prompt → Text Encoder → 文本嵌入
- 图像路径：Image references → DINO v2 → Spatial Warping → 图像嵌入

2.潜在控制信号(Latent Control Signal)：

- 维度为C×H'×W'的特征空间
- 通过Paint操作融合文本嵌入
- 通过Spatial Warping融合图像特征
- 作为统一的控制信号输入到生成网络

3.生成网络：

- Feature Alignment进行特征对齐
- Diffusion UNet生成最终图像
- 同时接收Global Prompt作为全局引导

示例：

![image-20250113203056301](./imgs/omnibooth_示例.png)


<h2 id="30.EasyPhoto的训练和推理流程是什么样的？">30.EasyPhoto的训练和推理流程是什么样的？</h2>

## EasyPhoto的训练流程

1. 人像得分排序：人像排序流程需要用到人脸特征向量、图像质量评分与人脸偏移角度。其中人脸特征向量用于选出最像本人的图片，用于LoRA的训练；图像质量评分用于判断图片的质量，选出质量最低的一些进行超分，提升图片质量；人脸偏移角度用于选出最正的人像，这个最正的人像会在推理阶段中作为参考人像进行使用，进行人脸融合。
2. Top-k个人像选取：选出第一步中得分最高的top-k个人像用于LoRA模型的训练。
3. 显著性分割：将背景进行去除，然后通过人脸检测模型选择出人脸周围的区域。
4. 图像修复：使用图像修复算法进行图像修复，并且超分，并使用美肤模型，最终获得高质量的训练图像。
5. LoRA模型训练：使用处理好的数据进行LoRA模型的训练。
6. LoRA模型融合：在训练过程中，会保存很多中间结果，选择几个效果最好的模型，进行模型融合，获得最终的LoRA模型。

![EasyPhoto训练流程示意图](./imgs/EasyPhoto训练示意图.jpeg) 

## EasyPhoto的推理流程

### 初步重建

1. 人脸融合：使用人脸融合算法，给定一张模板图和一张最佳质量的用户图，人脸融合算法能够将用户图中的人脸融合到模板人脸图像中，生成一张与目标人脸相似，且具有模版图整体外貌特征的新图像。
2. 人脸裁剪与仿射变换：将训练过程中生成的最佳人脸图片进行裁剪和仿射变换，利用五个人脸关键点，将其贴到模板图像上，获得一个Replaced Image，这个图像会在下一步中提供openpose信息。
3. Stable Diffusion + LoRA重绘和ControlNet控制：使用Canny控制（防止人像崩坏）、颜色控制（使生成的颜色符合模板）以及Replaced Image的Openpose+Face pose控制（使得眼睛与轮廓更像本人），开始使用Stable Diffusion + LoRA进行重绘，用脸部的Mask让重绘区域限制在脸部。

### 边缘完善

1. 人脸再次融合：和初步重建阶段一样，我们再做一次人脸融合以提升人脸的相似程度。
2. Stable Diffusion + LoRA重绘和ControlNet控制：使用tile控制（防止颜色过于失真）和canny控制（防止人像崩坏），开始第二次重绘，主要对边缘（非人像区域）进行完善。

### 后处理

后处理主要是提升生成图像的美感与清晰度。

1. 人像美肤：使用人像美肤模型，进一步提升写真图片的质感。
2. 超分辨率重建：对写真图片进行超分辨率重建，获取高清大图。

![EasyPhoto推理流程示意图](./imgs/EasyPhoto推理示意图.jpeg) 


<h2 id="31.FaceChain的训练和推理流程是什么样的？">31.FaceChain的训练和推理流程是什么样的？</h2>

FaceChain是一个功能上近似“秒鸭相机”的技术，我们只需要输入几张人脸图像，FaceChain技术会帮我们合成各种服装、各种场景下的AI数字分身照片。下面Rocky就给大家梳理一下FaceChain的训练和推理流程：

## 训练阶段

1. 输入包含清晰人脸区域的图像。
2. 使用基于朝向判断的图像旋转模型+基于人脸检测和关键点模型的人脸精细化旋转方法来处理人脸图像，获取包含正向人脸的图像。
3. 使用人体解析模型+人像美肤模型，获得高质量的人脸训练图像。
4. 使用人脸属性模型和文本标注模型，再使用标签后处理方法，生成训练图像的精细化标签。
5. 使用上述图像和标签数据微调Stable Diffusion模型得到人脸LoRA模型。
7. 输出人脸LoRA模型。

## 推理阶段

1. 输入训练阶段的训练图像。
2. 设置用于生成个人写真的Prompt提示词。
3. 将人脸LoRA模型和风格LoRA模型的权重融合到Stable Diffusion模型中。
4. 使用Stable Diffusion模型的文生图功能，基于设置的输入提示词初步生成AI个人写真图像。
5. 使用人脸融合模型进一步改善上述写真图像的人脸细节，其中用于融合的模板人脸通过人脸质量评估模型在训练图像中挑选。
6. 使用人脸识别模型计算生成的写真图像与模板人脸的相似度，以此对写真图像进行排序，并输出排名靠前的个人写真图像作为最终输出结果。

32.ReCo的框架和原理![FaceChain训练和推理流程图](./imgs/FaceChain训练和推理流程图.jpeg)



<h2 id="32.ReCo的框架和原理">32.ReCo的框架和原理</h2>

论文链接：[[2211.15518\] ReCo: Region-Controlled Text-to-Image Generation](https://arxiv.org/abs/2211.15518)

#### 框架架构

1. ReCo基于Stable Diffusion（SD）模型改进，主要包含以下模块：
   1. **双模态输入序列**：在传统文本令牌（Text Tokens）的基础上，引入**位置令牌（Position Tokens）**。这些位置标记通过坐标编码（如边界框的归一化坐标）生成，允许用户在输入查询中混合自由文本描述和区域位置信息。
      - 示例输入格式：
        `"a kitchen with <576> <553> <791> <979> stainless steel appliances and a counter"`
   2. **扩展的文本编码器**：沿用CLIP ViT-L/14文本编码器，但额外支持位置标记的嵌入，联合文本和位置标记生成条件嵌入向量。
   3. **扩散模型架构**：保持SD的U-Net架构，但通过微调使其能结合位置信息进行去噪。位置标记与文本标记通过交叉注意力机制共同指导图像生成。

![image-20250223194247270](./imgs/reco.png)

#### **关键原理**

1. **区域指令的可控性**：
    ReCo通过位置标记直接指定目标区域（如物体位置或范围），降低生成过程中的空间歧义。例如，用户可精确描述"沙发在画面左侧（区域<0, 0, 0.5, 1>）"，从而避免模型对布局的随机猜测。
2. **训练数据构建**：
   - 使用自动标注工具（如GIT captioning模型）为图像中的裁剪区域生成区域描述。
   - 对位置标记进行随机化裁剪与坐标编码，增强模型对多样化区域指令的泛化能力。
3. **保留预训练能力**：
    ReCo仅微调SD模型的文本编码器和交叉注意力层，最大限度保留原有生成质量，并适应区域控制任务。

下面是示例：

![image-20250223194333168](./imgs/reco_example.png)



<h2 id="33.Be-Yourself（Bounded-Attention-for-Multi-Subject-Text-to-Image-Generation）的框架和原理">33.Be Yourself（Bounded Attention for Multi-Subject Text-to-Image Generation）的框架和原理</h2>

论文链接：[[2403.16990\] Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation](https://arxiv.org/abs/2403.16990)

#### **核心问题**

现有文本到图像扩散模型（如Stable Diffusion）在生成包含多个语义相似主体的复杂场景时，常出现**语义泄漏**（Semantic Leakage）问题，导致主体特征混淆（例如，“3只姜黄色小猫和2只灰色小猫”可能混合颜色或形态）。

![image-20250223200119401](./imgs/beyourself_example_for_issue.png)

论文提出**Bounded Attention**（有界注意力）机制，通过约束交叉注意力和自注意力层的相互作用，分割不同主体的生成流程，主要包含以下模块：

1. **Bounded Guidance**
   - **功能**：在去噪过程的早期阶段，通过用户提供的布局（如bounding boxes）生成粗略的语义分割掩模，引导各主体区域的空间定位。
   - **原理**：限制不同区域的注意力交互，抑制跨区域语义干扰（如避免“姜黄色小猫”的特征泄漏到“灰色小猫”区域）。
2. **Bounded Denoising**
   - **功能**：在去噪后期，细化各主体的细节特征，确保其与文本描述严格对齐（如颜色、纹理等）。
   - **原理**：通过修改自注意力层中的查询（Query）和键（Key）的相似性，强制不同主体的特征独立性。
3. **Mask Refinement**
   - **功能**：动态优化分割掩模，提升主体边界清晰度。
   - **原理**：结合去噪过程中的隐变量特征，迭代更新掩模，避免区域重叠导致的特征混合。

![image-20250223195927919](./imgs/beyouself.png)

下面是示例：

![image-20250223200443289](./imgs/BA_example.png)

<h2 id="34.IFadapter的框架和原理">34.IFadapter的框架和原理</h2>

### 研究背景

- 当前的文本到图像（Text-to-Image, T2I）扩散模型在生成单个实例的高质量图像方面表现出色，但在处理多个实例的精确位置和特征生成时存在局限性。
- 布局到图像（Layout-to-Image, L2I）任务通过引入边界框作为空间控制信号解决了位置问题，但在生成精确的实例特征方面仍有不足。

论文提出了一个更具挑战性的任务：**实例特征生成（Instance Feature Generation, IFG）**，旨在同时确保生成内容的位置准确性和特征忠实度。



### IFAdapter框架

IFAdapter（实例特征适配器）是为解决IFG任务而设计的模型，主要包含两个核心组件：

#### 1. 外观令牌（Appearance Tokens）

- 解决问题：现有模型主要使用单一上下文化令牌（EoT令牌）来指导实例特征生成，无法捕捉高频细节特征
- 原理：引入可学习的外观查询，从实例描述中提取特定特征信息，形成外观令牌，与EoT令牌一起工作
- 优势：能够更精确地控制实例特征的生成，特别是复杂的纹理、混合颜色等细节

#### 2. 实例语义图（Instance Semantic Map）

- 解决问题：现有序列到2D定位条件无法提供足够强的空间先验
- 原理：构建2D语义图将实例特征与指定空间位置关联起来，提供增强的空间引导
- 特点：在多个实例重叠的区域，采用门控语义融合机制解决特征混淆问题
- 实现：仅在扩散模型的部分交叉注意力层中集成语义图，实现松散耦合

<img src="./imgs/ifadapter.png" alt="image-20250309161515489" style="zoom:67%;" />

效果如下：

![image-20250309161925832](./imgs/ifadapter_example.png)

IFAdapter的即插即用设计使其能够无缝赋能各种社区模型，应用于图形设计和艺术设计等需要局部高级细节的场景。

这项研究为解决文本到图像生成中的精细控制问题提供了一种有效的方法，在保持位置准确性的同时提高了特征表现力，推动了可控图像生成技术的发展。

<h2 id="35.LAW-Diffusion的框架和原理">35.LAW-Diffusion的框架和原理</h2>

LAW-Diffusion是一种语义可控的布局感知扩散模型，其核心思想是解析对象之间的空间依赖关系，生成具有协调一致对象关系的复杂场景图像。该框架主要包含以下组件：

### 1. 空间依赖关系解析器（Spatial Dependency Parser）

不同于之前仅探索类别感知关系的方法，LAW-Diffusion引入了空间依赖关系解析器，用于编码对象之间的位置感知语义连贯性：

- **对象区域图（Object Region Maps）**：为每个对象实例化区域语义表示，将类别嵌入填充到其边界框指定的区域
- 位置感知跨对象注意力（Location-aware Cross-object Attention）：
  - 将对象区域图分割成区域片段
  - 对相同位置的区域片段执行多头注意力操作
  - 使用可学习的聚合令牌捕获位置感知的组合语义
  - 重新组合聚合片段得到布局嵌入（Layout Embedding）

这种方式同时捕获了类别感知和位置感知的依赖关系，确保在生成图像的局部片段时，能够准确指定对象在特定位置的可能重叠情况。

![image-20250309164931421](./imgs/Spatial Dependency Parser.png)

### 2. 自适应引导调度（Adaptive Guidance Schedule）

为了平衡区域语义对齐与对象纹理保真度之间的权衡，LAW-Diffusion提出了自适应引导调度策略：

- 在采样阶段使用余弦形式的引导幅度衰减函数
- 从初始较大的引导比例逐渐衰减到较小值
- 早期阶段强调语义控制，后期阶段注重纹理细节

这种策略类似于人类绘图时先构思整体语义，再细化细节的直觉过程。

![image-20250309165104833](./imgs/Adaptive Guidance Schedule.png)

### 3. 布局感知潜在嫁接（Layout-aware Latent Grafting）

LAW-Diffusion还支持实例级别的重构能力，包括添加/移除/重新设计生成场景中的实例：

- 从已生成图像的扩散潜在表示中，提取边界框外的区域
- 在相同噪声级别下，将该区域嫁接到由新布局引导的目标潜在表示上
- 通过交替重组局部区域语义和去噪这些嫁接的潜在表示，实现实例重构

![image-20250309165119951](./imgs/Layout-aware Latent Grafting.png)

示例如下：

![image-20250309165333162](./imgs/LAW-Diffusion.png)

LAW-Diffusion通过引入空间依赖关系解析、自适应引导调度和布局感知潜在嫁接等创新技术，显著提升了布局到图像生成的效果，特别是在保持复杂场景中对象之间合理和协调的关系方面。该方法为控制复杂场景生成提供了新的思路，具有重要的理论和应用价值。

<h2 id="36.Check,Locate,Rectify（A-Training-Free-Layout-Calibration-System-for-Text-to-Image-Generation）的框架和原理">36.Check, Locate, Rectify（A Training-Free Layout Calibration System for Text-to-Image Generation）的框架和原理</h2>

本研究论文介绍了SimM，这是一种新颖的系统，旨在解决文本到图像生成中的一个常见挑战：准确实现文本提示中的空间布局指令。

### SimM方法

SimM采用“检查-定位-纠正”流程，无需额外训练即可干预生成过程：

1. **检查**：
   - 使用预定义词汇确定提示是否包含布局要求
   - 使用依赖解析和启发式规则生成对象的目标布局
   - 评估当前生成是否可能偏离这些要求
2. **定位**：
   - 在早期去噪步骤中识别对象当前被放置的位置
   - 使用注意力图找到每个对象的激活区域
3. **纠正**：
   - 将激活从错误位置转移到目标位置
   - 增强目标区域的激活并抑制其他区域的激活
   - 防止不同对象之间的激活重叠

![image-20250309163901844](./imgs/SimM.png)



示例如下：

![image-20250309163945734](./imgs/SimM_example.png)

### 关键创新

1. **无需训练的实现**：可与现有的预训练模型配合使用，无需微调
2. **处理两种空间关系**:
   - 相对关系（例如，“一只狗在猫的左边”）
   - 最高级关系（例如，“左边的一朵花”）
3. **高效的布局生成**：自动从文本中推导出目标布局，无需手动输入
4. **最小的计算开销**：直接修改注意力图，无需复杂的优化

该技术可以改善创意应用中的用户控制，允许在文本到图像生成中更精确地指定布局，而无需具备布局技术知识或额外训练。

<h2 id="37.Conceptrol的框架和原理">37.Conceptrol的框架和原理</h2>

个性化图像生成面临着一个根本性的困境：如何在遵循创意文本提示的同时，平衡从参考图像中保留对象身份。目前的方法主要分为两类：

1. **微调方法**，如 DreamBooth 和 Textual Inversion，每种新概念都需要大量的计算资源和训练时间。
2. **零样本适配器**，如 IP-Adapter 和 OmniControl，效率更高，但在遵循复杂的提示时，通常难以维持概念身份。

核心挑战是，现有的零样本适配器无法有效地利用扩散模型的文本理解能力。它们分别处理参考图像和文本提示，错过了它们之间的关键联系。
Conceptrol 引入了一种非常简单但有效的方法来解决这个困境。关键的见解是，扩散模型已经包含强大的“文本概念掩码”，可以引导注意力集中到图像的相关部分。

![image-20250420205157570](./imgs/conceptrol.png)

该方法通过一个三步过程工作：

1. **文本概念识别**：系统首先识别提示中与参考图像相对应的文本概念（例如，雕像图像的“雕像”）。
2. **概念掩码提取**：它从模型的内部表示中提取与该文本概念相对应的注意力掩码。
3. **引导注意力**：该掩码用于约束参考图像的视觉信息的应用位置，确保它与文本提示的意图对齐。

这种方法不需要额外的训练，并且可以作为现有零样本适配器的即插即用增强功能。它可以应用于基于 U-Net 的模型（如 Stable Diffusion）和基于 DiT 的模型（如 FLUX）。


<h2 id="38.EasyPhoto中应用到了哪些人脸特征处理算法？">38.EasyPhoto中应用到了哪些人脸特征处理算法？</h2>

EasyPhoto作为一款基于Stable Diffusion的AI写真生成工具，深度融合了多类先进的人脸特征处理算法，通过全自动化的流程实现了从训练到推理的高质量人像生成。以下是Rocky对其核心技术进行系统性解析：

### 一、**人脸检测与关键点定位算法：建立面部几何结构基础**
1. **RetinaFace人脸检测**  
   - **功能**：在多张用户上传图片中精准定位人脸边界框（Bounding Box）及五官关键点（如双眼、鼻尖、嘴角等）。  
   - **作用**：  
     - 排除非人像或低质量图片（如人脸尺寸 < 128像素）；  
     - 为人脸对齐、分割、融合等后续步骤提供空间基准。  

2. **人脸关键点对齐（Face Alignment）**  
   - **方法**：基于RetinaFace检测到的5点关键点（双眼、鼻尖、嘴角），通过**仿射变换（Affine Transformation）** 将倾斜人脸旋转为正脸姿态。  
   - **意义**：消除姿态差异，提升后续特征提取的稳定性。对齐后人脸成为“标准脸”，便于CurricularFace等模型提取一致性高的特征向量。  

3. **关键点扩展应用（如68点/96点模型）**  
   - 在推理阶段结合**OpenPose** 技术，生成身体骨骼与面部细节关键点，用于控制生成图像的姿态与表情自然性。  

下表总结了人脸关键点定位在EasyPhoto中的应用场景：

| **技术** | **关键点数量** | **主要应用场景** | **作用** |
|----------|----------------|------------------|----------|
| **RetinaFace** | 5点 | 训练阶段：人脸检测与初步对齐 | 排除低质量图像，提供基础空间基准 |
| **扩展关键点模型** | 68点/96点 | 推理阶段：精细化控制 | 生成身体骨骼与面部细节，控制姿态与表情 |
| **OpenPose** | 全身多关键点 | 推理阶段：姿态控制 | 结合ControlNet实现姿态一致性 |

### 二、**人脸特征提取与身份验证算法：保证人像一致性**
1. **CurricularFace深度特征提取**  
   - **原理**：使用预训练的深度卷积网络（如ResNet），从对齐后人脸中提取**512维特征向量（Embedding）** 。  
   - **应用**：  
     - 计算所有训练图片的**平均人脸特征向量**；  
     - 计算每张图与平均特征的**余弦相似度（0~1）**，用于筛选最接近用户本人特征的Top-K图片。  

2. **人脸质量评分与角度筛选**  
   - **质量评估**：结合图像清晰度、光照均匀性等指标，排除模糊或低质图片。  
   - **偏移角度计算**：  
     ```python
     # 计算双眼连线水平倾斜角
     x = keypoint_right_eye_x - keypoint_left_eye_x
     y = keypoint_right_eye_y - keypoint_left_eye_y
     angle = arctan(y/x)  # 角度归一化为0~1分 (90°→0分, 0°→1分)
     ```  
     最正人脸（最高分）作为推理阶段的**参考人脸（Reference Face）**，用于模板融合。  

### 三、**人像分割与背景处理算法：聚焦人脸区域**
1. **显著性分割（Saliency Segmentation）**  
   - **技术**：使用类似U²-Net的模型分离人像与背景。  
   - **目的**：  
     - 训练阶段：排除背景干扰，使LoRA专注学习人脸特征；  
     - 推理阶段：结合Mask ControlNet，仅重建人脸区域，保持背景完整性。  

2. **背景修复与超分（如GPEN）**  
   - 对分割后的人脸区域进行**去噪、修复缺损部位（如遮挡耳朵）**，再使用**超分辨率模型（如ABPN）** 提升画质至高清。  

### 四、**人像修复与质量增强算法：提升输入数据质量**
- **GPEN人脸修复**：针对模糊、低分辨率或遮挡的人脸，通过生成对抗网络（GAN）重建细节（如皮肤纹理、发丝）。  
- **ABPN美肤模型**：消除痘印、皱纹等瑕疵，生成均匀肤色，为LoRA提供高质量训练样本。  

### 五、**人脸融合与表情迁移算法：实现自然换脸**
1. **参考人脸融合（Reference Fusion）**  
   - 将用户最正人脸（Reference Photo）与模板人脸通过**图像变形（Warping）与泊松融合（Poisson Blending）** 结合，生成过渡自然的“基础脸”。  
   - 输出作为ControlNet的**Canny边缘图输入**，引导生成图像保持原人脸结构。  

2. **仿射变换贴合（Affine Warping）**  
   - 利用5点关键点，将LoRA生成的人脸仿射变换后贴合至模板身体，生成带姿态控制的**Replaced Image**。  

### 六、**人像美化与风格化处理算法：优化生成效果**
1. **双边滤波磨皮（Bilateral Filtering）**  
   - 在推理后处理阶段使用，平滑皮肤同时保留五官边缘锐度，避免“塑料感”。  
2. **HSV色彩空间调整**  
   - 转换至HSV空间，调节饱和度（S）和明度（V）实现**自然美白**，避免RGB直接调整导致的色偏。  
3. **液化变形（Liquify Deformation）**  
   - 通过移动像素位置实现**瘦脸、大眼**等效果，公式控制变形强度随距离中心点衰减。  

### 七、**算法协同框架：从训练到推理的全链路整合**
1. **训练阶段**  
   ```mermaid
   graph LR
   A[用户上传图片] --> B(RetinaFace检测+对齐)
   B --> C(CurricularFace提取特征)
   C --> D{筛选Top-K图片}
   D --> E[分割背景 + GPEN修复]
   E --> F[训练LoRA]
   ```  
   - LoRA训练中每100步验证一次，按人脸相似度自动融合最优权重。  

2. **推理阶段**  
   - **多ControlNet协同**：  
     - Canny边缘控制（防崩坏） + OpenPose姿态控制 + 颜色迁移 + Mask局部重绘。  
   - **两阶段生成**：  
     - 初步重建（人脸区域） → 边缘完善（头发、衣领等衔接处）。  

> 纵观其技术体系，EasyPhoto的核心竞争力在于将学术界前沿算法（如CurricularFace、GPEN）工程化为端到端流程，推动AI写真从“专家可用”迈向“大众可玩”。其开源生态（GitHub Star超9k）亦加速了工业级AI视觉应用的普惠化进程。


<h2 id="39.Prompt-to-Prompt是什么方法？">39.Prompt-to-Prompt是什么方法？</h2>

### 1. 方法概述

**Prompt-to-Prompt (P2P)**是一种基于文本的图像编辑方法，通过操控跨注意力机制，实现仅通过文本提示即可进行精细化图像编辑，而无需额外的用户输入（如遮罩或手动编辑）。核心思想在于利用扩散模型中的**跨注意力层**，操控像素与文本标记之间的交互关系，从而在生成过程中保留原始图像的结构和布局。![image-20241021170146517](./imgs/P2P.png)

### 2. 方法细节

1. **跨注意力机制的作用**:
    - 在图像生成过程中，扩散模型通过跨注意力层将文本嵌入和视觉特征融合，每个文本标记会生成对应的空间注意力图，决定了文本中每个词汇对图像不同区域的影响。
    - 通过控制这些跨注意力图，研究人员能够保留图像的原始结构，同时在不同的生成步骤中调整文本对生成结果的影响。

2. **编辑策略**:
    - **单词替换**: 通过将跨注意力图从原始提示转移到新的文本提示，方法能够在替换部分内容（如“狗”替换为“猫”）的同时保持场景的整体布局。
    - **添加新短语**: 当用户在原始提示上增加描述（如增加风格或颜色），方法会将未改变的部分的注意力图保持一致，使新元素自然融入图像。
    - **调整单词权重**: 方法允许调整某个词的影响程度，实现类似“滑块控制”的效果，使得用户可以增强或减弱某些特定词汇对图像生成的作用。

3. **编辑流程**:
    - 编辑的核心步骤是通过注入原始图像的跨注意力图，将其与新提示中的注意力图结合，并在扩散过程的不同阶段应用这些调整。
    - 通过**时间戳参数**，方法还能调节注意力图的影响范围，从而控制生成图像的保真度和平滑度。

### 3. 应用示例

1. **局部编辑**:
    - 通过调整文本提示中的单词，可以局部替换图像中的特定对象，如将“柠檬蛋糕”变成“南瓜蛋糕”。
    - 这种方法无需用户提供遮罩，能够自然地改变图像中的纹理和物体形状。

2. **全局编辑**:
    - 添加新描述词语使得可以实现全局风格转换或环境变化，例如为图像添加“雪”或改变光照效果。
    - 方法能够保留图像的整体构图，确保新的风格或背景不会破坏原有的视觉结构。

3. **风格化**:
    - 通过在提示中添加风格描述，方法可以将草图转换为照片真实感图像，或生成各种艺术风格的图像。

### 4. 方法优势

- **仅需文本控制**: 不依赖用户手动输入的遮罩或结构化标记，仅通过修改提示文本即可实现多样化和精细化的图像编辑。
- **高保真度**: 方法能够在保持原始图像结构和布局的同时，准确生成与修改提示相符的图像。
- **实时性**: 相比于传统的训练或微调模型，这种基于扩散模型内部跨注意力的操控方法不需要额外的数据或优化步骤。

本文的方法展示了通过文本操控生成模型内部机制来实现图像编辑的新可能性，为未来更加智能、直观的图像生成和编辑工具奠定了基础。


<h2 id="40.InstructPix2Pix的训练和推理流程是什么样的？">40.InstructPix2Pix的训练和推理流程是什么样的？</h2>

论文链接：[2211.09800](https://arxiv.org/pdf/2211.09800)

![image-20241021152129263](./imgs/ip2p.png)

### 1. 训练流程

1. **生成训练数据**：
   - 使用 **GPT-3** 生成文本三元组，包括输入图像描述、编辑指令、编辑后的图像描述。
   - 利用 **Stable Diffusion** 和 **Prompt-to-Prompt** 方法，根据文本生成配对的图像（编辑前和编辑后），并通过 **CLIP** 过滤确保图像质量和一致性。
2. **训练 InstructPix2Pix 模型**：
   - 使用 **Stable Diffusion** 的预训练权重进行初始化。
   - 输入原始图像、编辑指令和目标编辑后的图像。
   - 训练目标是最小化潜在扩散目标函数，应用无分类器引导技术以平衡图像和文本指令的影响。

### 2. 推理流程

1. **输入**：
   - 一张待编辑的真实图像和一条人类编写的编辑指令。

2. **处理**：
   - 将输入图像编码到潜在空间。
   - 应用条件扩散模型，根据输入图像和文本指令生成编辑后的潜在表示。
   - 使用无分类器引导，通过调整两个引导尺度（s_I 和 s_T）平衡图像和指令的影响。

3. **输出**：
   - 将生成的潜在表示解码为编辑后的图像，通常生成 **512x512** 分辨率的结果。
   - 每张图像的编辑过程在 **A100 GPU** 上大约需要 **9 秒**，使用 **100** 个去噪步骤。


<h2 id="41.ControlNet系列模型有多少种控制条件？">41.ControlNet系列模型有多少种控制条件？</h2>


<h2 id="42.ControlNet的最小单元是什么样的？">42.ControlNet的最小单元是什么样的？</h2>

下图是ControlNet模型的最小单元：

![ControlNet模型的最小单元结构示意图](./imgs/ControlNet模型的最小单元结构示意图.png)

从上图可以看到，**在使用ControlNet模型之后，Stable Diffusion/FLUX.1模型的权重被复制出两个相同的部分，分别是“锁定”副本（locked）权重和“可训练”副本（trainable copy）权重**。

**我们如何理解这两个副本权重呢？** Rocky从训练角度和推理角度给大家进行通俗易懂的讲解。

**首先不管是训练阶段还是推理阶段，ControlNet都在“可训练”副本上输入控制条件** $c$，然后将“可训练”副本输出结果和原来Stable Diffusion/FLUX.1模型的“锁定”副本输出结果**相加（add）**获得最终的输出结果。

在训练阶段，**其中“锁定”副本中冻结参数，权重保持不变，保留了Stable Diffusion/FLUX.1模型原本的能力**；与此同时，**使用新数据对“可训练”副本进行微调训练，学习数据中的控制条件信息**。因为有Stable Diffusion/FLUX.1模型作为预训练权重，**复制“可训练”副本而不是直接训练原始权重还能避免数据集较小时的过拟合**，所以我们使用常规规模数据集（几K-几M级别）就能对控制条件进行学习训练，同时不会破坏Stable Diffusion/FLUX.1模型原本的能力（从数十亿张图像中学习到的大型模型的能力）。

另外，大家可能发现了**ControlNet模型的最小单元结构中有两个zero convolution模块，它们是1×1卷积，并且在微调训练时权重和偏置都初始化为零（zero初始化）**。这样一来，在我们开始训练ControlNet之前，所有zero convolution模块的输出都为零，使得ControlNet完完全全就在原有Stable Diffusion/FLUX.1底模型的能力上进行微调训练，这样可以尽量避免训练加入的初始噪声对ControlNet“可训练”副本权重的破坏，保证了不会产生大的能力偏差。

<h2 id="43.ControlNet中的zero卷积层初始权重为什么是0?zero卷积层为什么有效？">43.ControlNet中的zero卷积层初始权重为什么是0?zero卷积层为什么有效？</h2>

大家很可能就会有一个疑问，如果zero convolution模块的初始权重为零，那么梯度也为零，ControlNet模型将不会学到任何东西。**那么为什么“zero convolution模块”有效呢？（AIGC算法面试必考点）**

Rocky进行下面的推导，相信大家对一切都会非常清晰明了：

我们可以假设ControlNet的初始权重为： $y=wx+b$ ，然后我们就可以得到对应的梯度求导：

$$\frac{\partial y}{\partial w}=x,\frac{\partial y}{\partial x}=w,\frac{\partial y}{\partial b}=1$$

如果此时 $w=0$ 并且 $x \neq 0$ ，然后我们就可以得到：

$$\frac{\partial y}{\partial w} \neq 0,\frac{\partial y}{\partial x}=0,\frac{\partial y}{\partial b}\neq 0$$

这就意味着只要 $x \neq 0$ ，一次梯度下降迭代将使w变成非零值。然后就得到： $\frac{\partial y}{\partial x}\neq 0$ 。**这样就能让zero convolution模块逐渐成为具有非零权重的卷积层，并不断优化参数权重**。

<h2 id="44.ConrtolNet是如何训练的？">44.ConrtolNet是如何训练的？</h2>

我们对ControlNet整体训练过程进行拆解理解。在我们不使用ControlNet模型时，**可以将Stable Diffusion/FLUX.1底模型的图像生成过程表达为：**

![不使用ControlNet模型时扩散模型推理示意图](./imgs/不使用ControlNet模型时扩散模型推理示意图.png)

接着，我们在此基础上假设将训练的所有参数锁定在 $\Theta$ 中，然后将其复制为可训练的副本 $\Theta_{c}$ 。复制的 $\Theta_{c}$ 使用额外控制条件信息c进行训练。因此在使用ControlNet之后，**Stable Diffusion/FLUX.1底模型 + ControlNet模型整体的图像生成表达式转化成为：**

![StableDiffusion和FLUX.1底模型+ControlNet模型的整体图像生成过程](./imgs/StableDiffusion和FLUX.1底模型+ControlNet模型的整体图像生成过程.png)

其中 $Z = F(c; \Theta)$ 代表了zero convolution模块， $\Theta_{z1}$ 和 $\Theta_{z2}$ 代表了前后两个zero convolution层的参数权重， $\Theta_{c}$ 则代表了ControlNet的参数权重。

由于训练开始前zero convolution模块的输出都为零，所以ControlNet未经训练时的初始输出为0：

$$\begin{cases} 
\mathcal{Z}\left(\boldsymbol{c};\Theta_{z1}\right) = 0 \\ 
\mathcal{F}\left(x + \mathcal{Z}\left(\boldsymbol{c};\Theta_{z1}\right);\Theta_{\mathrm{c}}\right) = \mathcal{F}\left(x;\Theta_{\mathrm{c}}\right) = \mathcal{F}(x;\Theta) \\
\mathcal{Z}\left(\mathcal{F}\left(x + \mathcal{Z}\left(\boldsymbol{c};\Theta_{z1}\right);\Theta_{\mathrm{c}}\right);\Theta_{z2}\right) = \mathcal{Z}\left(\mathcal{F}\left(x;\Theta_{\mathrm{c}}\right);\Theta_{z2}\right) = \mathbf{0} 
\end{cases}$$

由此可知，**在ControlNet微调训练初始阶段对Stable Diffusion/FLUX.1底模型权重是没有任何影响的，能让底模型原本的性能完整保存**，之后ControlNet的训练也只是在原Stable Diffusion/FLUX.1底模型基础上进行优化。

总的来说，**ControlNet的本质原理使得训练后的模型鲁棒性好，能够避免模型过拟合，并在特定条件场景下具有良好的泛化性，同时能够在小规模数据和消费级显卡上进行训练**。

<h2 id="45.ControlNet的损失函数是什么？">45.ControlNet的损失函数是什么？</h2>


<h2 id="46.ControlNet有哪些高阶用法？">46.ControlNet有哪些高阶用法？</h2>


<h2 id="47.ControlNet中Balanced、My-prompt-is-more-important、ControlNet-is-more-important三种模式的区别是什么？">47.ControlNet中Balanced、My prompt is more important、ControlNet is more important三种模式的区别是什么？</h2>


<h2 id="48.ControlNet-1.1与ControlNet相比，有哪些改进？">48.ControlNet 1.1与ControlNet相比，有哪些改进？</h2>

**ControlNet 1.1与ControlNet 1.0具有完全相同的模型架构。ControlNet 1.1主要是在ControlNet 1.0的基础上进行了优化训练，提高了鲁棒性和控制效果，同时发布了几个新的ControlNet模型。**

从ControlNet 1.1开始，ControlNet模型将使用标准的命名规则（SCNNR）来命名所有模型，这样我们在使用时也能更加方便与清晰。具体的命名规则如下图所示：

![ControlNet1.1模型命名规则.png](./imgs/ControlNet1.1模型命名规则.png)

ControlNet 1.1一共发布了14个模型（11个成品模型和3 个实验模型）：

```bash
control_v11p_sd15_canny
control_v11p_sd15_mlsd
control_v11f1p_sd15_depth
control_v11p_sd15_normalbae
control_v11p_sd15_seg
control_v11p_sd15_inpaint
control_v11p_sd15_lineart
control_v11p_sd15s2_lineart_anime
control_v11p_sd15_openpose
control_v11p_sd15_scribble
control_v11p_sd15_softedge
control_v11e_sd15_shuffle（实验模型）
control_v11e_sd15_ip2p（实验模型）
control_v11f1e_sd15_tile（实验模型）
```

<h2 id="49.介绍一下ControlNet-Canny条件控制的原理">49.介绍一下ControlNet Canny条件控制的原理</h2>


<h2 id="50.介绍一下ControlNet-Depth控制条件的原理">50.介绍一下ControlNet Depth控制条件的原理</h2>


<h2 id="51.介绍一下ControlNet-NormalMap控制条件的原理">51.介绍一下ControlNet NormalMap控制条件的原理</h2>


<h2 id="52.介绍一下ControlNet-OpenPose控制条件的原理">52.介绍一下ControlNet OpenPose控制条件的原理</h2>


<h2 id="53.介绍一下ControlNet-MLSD控制条件的原理">53.介绍一下ControlNet MLSD控制条件的原理</h2>


<h2 id="54.介绍一下ControlNet-Lineart控制条件的原理">54.介绍一下ControlNet Lineart控制条件的原理</h2>


<h2 id="55.介绍一下ControlNet-SoftEdge控制条件的原理">55.介绍一下ControlNet SoftEdge控制条件的原理</h2>


<h2 id="56.介绍一下ControlNet-Scribble/Sketch控制条件的原理">56.介绍一下ControlNet Scribble/Sketch控制条件的原理</h2>


<h2 id="57.介绍一下ControlNet-Segmentation控制条件的原理">57.介绍一下ControlNet Segmentation控制条件的原理</h2>


<h2 id="58.介绍一下ControlNet-Shuffle控制条件的原理">58.介绍一下ControlNet Shuffle控制条件的原理</h2>


<h2 id="59.介绍一下ControlNet-Tile/Blur控制条件的原理">59.介绍一下ControlNet Tile/Blur控制条件的原理</h2>


<h2 id="60.介绍一下Controlnet-Inpaint控制条件的原理">60.介绍一下Controlnet Inpaint控制条件的原理</h2>


<h2 id="61.介绍一下ControlNet-InstryctP2P控制条件的原理">61.介绍一下ControlNet InstryctP2P控制条件的原理</h2>


<h2 id="62.介绍一下一下ControlNet-Reference-only控制条件的原理">62.介绍一下一下ControlNet Reference-only控制条件的原理</h2>


<h2 id="63.介绍一下ControlNet-Recolor控制条件的原理">63.介绍一下ControlNet Recolor控制条件的原理</h2>


<h2 id="64.训练ControlNet模型的流程中有哪些关键参数？">64.训练ControlNet模型的流程中有哪些关键参数？</h2>


<h2 id="65.ControlNet模型的训练流程一般包含哪几部分核心内容？">65.ControlNet模型的训练流程一般包含哪几部分核心内容？</h2>

ControlNet系列模型的训练流程主要分成以下几个步骤：

1. 设计我们想要的额外控制条件：除了上面章节中讲到的控制条件，我们还可以根据实际需求自定义一些控制条件，从而使用ControlNet控制Stable Diffusion/FLUX.1朝着我们想要的细粒度方向生成内容。
2. 构建训练数据集：确定好额外控制条件后，我们就可以开始构建训练数据集了。ControlNet数据集中需要包含三个维度的信息：Ground Truth图片、作为控制条件（Conditional）的图片，以及对应的Caption标签。
3. 训练我们自己的ControlNet模型：训练数据集构建好后，我们就可以开始训练自己的ControlNet模型了，我们需要一个至少8G显存的GPU才能满足ControlNet模型的训练要求。

<h2 id="66.ControlNet有哪些万金油级应用案例？">66.ControlNet有哪些万金油级应用案例？</h2>


<h2 id="67.SD/SDXL/FLUX.1-ControlNet之间有哪些区别？">67.SD/SDXL/FLUX.1 ControlNet之间有哪些区别？</h2>


<h2 id="68.介绍一下Controlnet-Union的原理">68.介绍一下Controlnet-Union的原理</h2>

controlnet-union-sdxl-1.0模型的结构如下所示：

![controlnet-union-sdxl-1.0模型结构示意图](./imgs/controlnet-union-sdxl-1.0模型结构示意图.png)

Controlnet-union-sdxl-1.0模型模型的优化点：

1. **采用分桶训练技术**：对不同分辨率数据采用分桶训练策略，这样在推理时能够生成任意宽高比的高分辨率图像。
2. **海量高质量训练数据**：使用超过10M张高质量图像，数据集覆盖多样化的场景和内容。
3. **采用重新标注提示词**：使用CogVLM模型生成详细的Caption描述作为训练标签，使得模型具备优秀的提示词遵循能力。
4. **集成多种训练技巧**：包括但不限于数据增强、多目标损失函数、多分辨率训练等。
5. **参数效率高**：与原始ControlNet相比，参数量几乎未增加，网络参数和计算量无明显上升。
6. **支持多种控制条件**：兼容12种控制方式+5种高级编辑功能，每种条件的控制效果均不逊色于独立训练的ControlNet模型。
7. **支持多条件融合生成**：支持在推理时同时使用多条件的控制能力，多条件融合机制在训练中学习得到，无需手动设置超参数或设计提示词。
8. **兼容性强**：可与业界主流的SDXL模型、LoRA模型兼容使用。

controlnet-union-sdxl-1.0模型基于原始ControlNet架构，同时提出了**两个新模块Condition Transformer（条件变换器）和Control Encoder（控制编码器）**。

**在控制编码器中**，每个控制条件都被赋予一个特定的**控制类型标识符**。例如，OpenPose 对应标识符 (1, 0, 0, 0, 0, 0)，深度图对应 (0, 1, 0, 0, 0, 0)。**当存在多个条件时，例如同时使用 OpenPose 和深度图，其标识符将合并为 (1, 1, 0, 0, 0, 0)**。在控制编码器中，这些标识符通过正弦位置编码转换为嵌入向量，随后通过线性层将其投影至与时间嵌入相同的维度。**控制类型特征会与时间嵌入相加，从而在网络中传递不同控制类型的全局信息**。这一简洁设计有助于ControlNet区分各类控制条件，因为时间嵌入通常对整体网络具有广泛影响。无论是单一条件还是多条件组合，均对应唯一的控制类型标识符。

实际的控制类型标识符如下所示：

0 -- openpose
1 -- depth
2 -- thick line(scribble/hed/softedge/ted-512)
3 -- thin line(canny/mlsd/lineart/animelineart/ted-1280)
4 -- normal
5 -- segment

**在条件变换器中**，对ControlNet进行了扩展，使其能够同时处理多个控制输入。条件变换器的作用在于整合不同图像条件的特征。controlnet-union-sdxl-1.0模型的两大创新点在于：

1. 首先，不同条件共享同一条件控制编码器，从而使网络结构更为简洁与轻量。
2. 其次，引入了一个Transformer层，用于在原始图像特征与条件图像特征之间交换信息。同时并未直接采用Transformer的输出，而是利用其预测原始条件特征的调整量（即条件偏差）。**这种设计类似于 ResNet 的残差思想，实验表明该结构能显著提升模型性能**。

**同时针对多条件的同时控制，对ControlNet的条件编码器（Condition Encoder）也做了改进**。ControlNet原有的条件编码器由多个卷积层与Silu激活函数堆叠而成。**在保持其架构不变的基础上，controlnet-union-sdxl-1.0增加了卷积通道数，构建了一个更“宽”的编码器，这一改进显著提升了网络的表现能力**。原因在于，所有图像条件共享同一编码器，因此需要编码器具备更强的特征表示能力。原有结构对于单一条件可能足够，但在处理十余种条件时则显得力不从心。

**为了让controlnet-union-sdxl-1.0模型能够同时进行多条件的控制生成，官方设置了统一训练策略（Unified Training Strategy）进行多条件训练**。多条件训练有助于促进不同条件之间的融合，并增强模型的鲁棒性（因为单一条件所涵盖的知识有限）。


<h2 id="69.ControlNet是如何起作用的？">69.ControlNet是如何起作用的？</h2>

在以Stable Diffusion和FLUX.1为核心的AIGC图像生成过程中，想要ControlNet起作用，首先我们需要输入一张参考图，通过**预处理器** (Preprocessor)对输入参考图按一定的模式进行预处理，通常是使用传统的计算机视觉算法（如边缘检测、人体姿态估计、深度估计等）来从输入参考图中提取出纯粹的控制信息，也就是我们常说的**条件图像**(Conditioning Image)。

当然的，我们也可以不使用预处理功能，直接输入一张自己处理好的图片当作预处理图。下面是Rocky构建的ControlNet的条件图像处理流程图示，让大家能够更好的理解：

![ControlNet的条件图像处理流程](./imgs/ControlNet的条件图像处理流程.png)

接着条件图像信息通过ControlNet再注入到Stable Diffusion和FLUX.1中，再加上原本就直接注入到Stable Diffusion和FLUX.1中的文本信息和图像信息（可选，进行图生图任务），综合作用进行扩散过程，最终生成受条件信息控制的图像。

总的来说，ControlNet做的就是这样一件事：**它为扩散模型（如 Stable Diffusion/FLUX.1）提供一种额外的“约束”条件，引导AIGC大模型按照我们期望的构图、姿态或结构来生成图像，减少图像生成的随机**性。

为了大家方便的理解，**Rocky也制作了ControlNet推理的完整流程图**，大家可以直观的学习理解：

![完整的ControlNet模型推理流程](./imgs/完整的ControlNet模型推理流程.png)


<h2 id="70.ControlNet模型包含十几种控制功能可以归位几大类？">70.ControlNet模型包含十几种控制功能可以归位几大类？</h2>

Rocky认为这是一个非常重要的问题，ControlNet模型的各种控制功能非常多，我们需要进行归纳总结，才能更好的在AIGC时代中运用这些技术工具。

我们可以根据其处理信息的**类型**和**应用场景**归为几大类：

### 第一类：边缘与线条类
这类模型通过提取图像中的线条、轮廓或边缘信息来控制图像的结构和形状。它们通常用于精确的形状控制和线稿上色。

*   **Canny**：基于Canny边缘检测算法，提取图像中所有显著的边缘。
*   **MLSD**：专门用于检测建筑和室内设计中的**直线**，非常适合生成建筑草图、室内布局。
*   **Scribble**：将输入视为**涂鸦**或手绘草图，能够将非常粗略的线条转化为精致的图像。
*   **Soft Edge**：类似于Canny，但边缘更柔和、更粗，对自然图像（如动物、植物）的兼容性更好。
*   **Lineart**：专门用于提取**线稿**，尤其是从真实照片或艺术作品中提取，线条质量通常比Canny更高、更干净。

### 第二类：几何与3D信息类
这类模型使用从图像中推断出的3D信息（如深度、法线）来指导生成过程，从而控制物体的空间关系和立体感。

*   **Depth**：提取图像的**深度信息**，生成带有前景、中景、背景层次的图像，非常适合保持场景的立体感和空间一致性。
*   **Normal**：提取物体的**表面法线贴图**，它包含了物体表面细微的朝向信息，能生成光照和表面质感非常逼真的图像。

### 第三类：语义与内容信息类
这类模型使用更高层次的、经过抽象和理解的信息来控制生成内容，例如人体姿态、物体分割区域等。

*   **OpenPose**：检测图像中人物的**骨骼关键点**（姿势），可以精确控制生成人物的动作、姿态，甚至是手部动作和多人场景。
*   **Segmentation**：使用**语义分割图**，为图像中的不同部分（如天空、树木、人物、衣服）分配不同的颜色标签，从而对画面的每个区域进行像素级的精确控制。

### 第四类：风格与抽象信息类
这类模型不关注具体的形状或结构，而是关注图像的整体风格、颜色分布或纹理。

*   **Shuffle**：提取输入图像的**颜色分布和风格纹理**，将其应用到生成的新图像上，本质上是一种内容感知的风格迁移。
*   **Instruct Pix2Pix**：这是一个特殊模型，它不依赖于额外的控制图，而是直接接受**文字指令**来编辑图像。

### 第五类：特殊应用与重绘类
这类模型用于解决特定的图像生成或编辑任务。

*   **Tile**：用于**图像超分辨率**和**细节重绘**。它通过忽略输入图像的宏观结构，专注于局部纹理和细节，来对图像进行“放大并增强细节”的操作。
*   **Inpaint**：专门用于**局部重绘**。需要与Stable Diffusion的inpaint功能结合使用，通过提供蒙版区域和ControlNet的引导，在特定区域内进行高质量、与周围环境协调的重绘。

### 总结

| 类别 | 核心功能 | 包含模型 |
| :--- | :--- | :--- |
| **边缘与线条** | 控制形状、轮廓、结构 | Canny, MLSD, Scribble, Soft Edge, Lineart |
| **几何与3D** | 控制空间深度、立体感、表面朝向 | Depth, Normal |
| **语义与内容** | 控制人物姿态、物体分区 | OpenPose, Segmentation |
| **风格与抽象** | 控制颜色风格、纹理、根据指令编辑 | Shuffle, Instruct Pix2Pix |
| **特殊应用** | 图像放大、细节增强、局部重绘 | Tile, Inpaint |

这个分类方式可以帮助您更好地理解每个ControlNet模型的适用场景，并根据我们的创作需求快速选择合适的模型。在实际使用中，我们经常会将多个ControlNet组合使用，以达到更复杂和精确的控制效果。

---
