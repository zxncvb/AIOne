# 目录

## 第一章 主流AIGC图像生成编辑大模型高频考点

- [1.站在2026年的视角，AIGC图像生成领域未来的技术发展趋势是什么样的？](#1.站在2026年的视角，AIGC图像生成领域未来的技术发展趋势是什么样的？)
- [2.目前主流的AIGC图像生成大模型有哪些？](#2.目前主流的AIGC图像生成大模型有哪些？)
- [3.Imagen系列模型有什么跨周期的技术价值?](#3.Imagen系列模型有什么跨周期的技术价值?)
- [4.DALL-E 3有什么跨周期的技术价值？](#4.DALL-E-3有什么跨周期的技术价值？)
- [5.GPT-4o有哪些优秀的特点？](#5.GPT-4o有哪些优秀的特点？)
- [6.Midjourney系列模型迭代至今有哪些优秀的特点？](#6.Midjourney系列模型迭代至今有哪些优秀的特点？)
- [7.介绍一下Seedream系列模型的原理，每个版本都有哪些创新？](#7.介绍一下Seedream系列模型的原理，每个版本都有哪些创新？)
- [8.介绍一下HiDream-I1模型的原理](#8.介绍一下HiDream-I1模型的原理)
- [9.介绍一下Nano Banana Pro的特点](#9.介绍一下Nano-Banana-Pro的特点)
- [10.GLM-Image的创新点有哪些？](#10.GLM-Image的创新点有哪些？)
- [11.Qwen-Image系列的创新点有哪些？](#11.Qwen-Image系列的创新点有哪些？)
- [12.Z-Image系列的创新点有哪些？](#12.Z-Image系列的创新点有哪些？)


## 第一章 主流AIGC图像生成编辑大模型高频考点正文

<h2 id="1.站在2026年的视角，AIGC图像生成领域未来的技术发展趋势是什么样的？">1.站在2026年的视角，AIGC图像生成领域未来的技术发展趋势是什么样的？</h2>

Rocky认为，2026年是AIGC图像生成领域的“中场时刻”，很多原有的AIGC图像生成技术受到重大考验，2026年前的80%的AIGC技术成为历史长河中的数字尘埃，而剩下20%左右的AIGC技术则跨越周期继续繁荣。

同时，这些跨周期AIGC技术也在不断发展革新，与新技术、新趋势不断交融，形成全新的AIGC图像生成技术体系与核心。

Rocky在这里也总结了AIGC图像生成领域的技术未来发展趋势，希望能给大家带来帮助：

1. AIGC核心大模型的架构逐渐向自回归+扩散模型演变。
2. AIGC核心大模型的功能趋向图像生成、图像编辑、风格迁移、身份保持生成及多主体一致性等集综合功能于一身。
3. AIGC核心大模型趋向与演变成原生多模态大模型，这样的趋势十分优雅。
4. AIGC图像生成技术和对应的配套AIGC技术的迭代周期和迭代烈度前所未有，是传统深度学习时代无法比拟的，所以我们需要更好的对AIGC技术的核心价值与跨周期价值进行预判，减少沉没成本。


<h2 id="2.目前主流的AIGC图像生成大模型有哪些？">2.目前主流的AIGC图像生成大模型有哪些？</h2>

目前，AIGC时代主流的AIGC图像生成大模型包括：

1. Z-Image系列图像生成编辑大模型（Z-Image-Turbo、Z-Image、Z-Image-Omni-Base、Z-Image-Edit）
2. FLUX.2系列图像生成编辑大模型（pro、flex、dev、klein）
3. FLUX.1 Kontext和FLUX.1 Krea图像生成编辑大模型
4. FLUX.1系列图像生成大模型（pro、dev、schnell）
5. Stable Diffusion系列文生图大模型（1.x、2.x、XL、3、3.5）
6. Seedream系列文生图大模型（Seedream 2.0、3.0、4.0、4.5、5.0）
7. GLM-Image图像生成编辑大模型
8. Midjourney系列文生图大模型（V5、V6、V7）
9. 可图系列文生图大模型
10. Ideogram系列文生图大模型
11. DaLL·E系列文生图大模型（2-3）
12. PixArt系列文生图大模型（α、Σ）
13. Playground系列文生图大模型（v2.5、v3）
14. Imagen系列文生图大模型（1、2、3）
15. PixArt系列文生图大模型
16. 混元系列文生图大模型
17. NovelAI系列文生图大模型

- [点击查看详细答案](https://articles.zsxq.com/id_bkt6vb2dw655.html)

Rocky会根据AIGC时代的AI绘画大模型技术更新来持续补充完善详细答案和本答案！


<h2 id="3.Imagen系列模型有什么跨周期的技术价值?">3.Imagen系列模型有什么跨周期的技术价值?</h2>

**Imagen是AIGC时代AI绘画领域的第一个多阶段级联大模型，由一个Text Encoder（T5-XXL）、一个文生图 Pixel Diffusion、两个图生图超分Pixel Diffusion共同组成，让Rocky想起了传统深度学习时代的二阶段目标检测模型，这也说明多模型级联架构是跨周期的，是有价值的，是可以在AIGC时代继续成为算法解决方案构建的重要一招。**

![Imagen模型结构](./imgs/Imagen模型结构.png)

同时Imagen是AI绘画领域第一个使用大语料预训练语言模型T5-XXL作为Text Encoder的AI绘画大模型。论文中认为在文本编码器部分下功夫比在生成模型上下功夫效果要好，即使文本编码器部分的T5-XXL是纯文本语言模型，也比加大加深生成模型参数效果要好。

不过Imagen也有他的局限性，在扩散模型部分还是选用经典的64x64分辨率的U-Net结构。选择小模型可以缓解Diffusion迭代耗时太长，导致生成过慢的问题，生成小图像再超分确实是加速生成最直观的方法。但是也注定了无法生成比较复杂内容和空间关系的大图像。


<h2 id="4.DALL-E-3有什么跨周期的技术价值？">4.DALL-E 3有什么跨周期的技术价值？</h2>

技术报告链接：[dall-e-3.pdf](https://cdn.openai.com/papers/dall-e-3.pdf)

DALL-E 3是OpenAI于2023年9月推出的革命性AI图像生成系统，代表了文本到图像转换技术的重大飞跃。与DALL-E 2相比，它具有更强大的自然语言理解能力，能精确解读复杂、微妙的文本描述，包括抽象概念和多层次指令。生成的图像质量显著提升，呈现出更高分辨率、更丰富细节和更自然的光影效果。DALL-E 3彻底解决了前代产品处理文字的局限性，能在图像中准确呈现各种字体和文本布局。其与ChatGPT的创新集成使用户能通过对话式体验逐步完善创意，获得提示词优化建议。在艺术表现方面，它能更精准地模拟从古典油画到现代数字艺术的各种风格和媒介。安全性也得到加强，内置更严格的保护机制防止生成不当内容。用户可通过ChatGPT Plus订阅、OpenAI API或Microsoft的Bing Image Creator和Designer工具使用DALL-E 3，已广泛应用于广告、出版、产品设计、游戏开发和建筑可视化等专业领域，为创意工作流程带来前所未有的效率和可能性。

不同文本对比：

![image-20250407193948936](./imgs/dalle-3.png)


<h2 id="5.GPT-4o有哪些优秀的特点？">5.GPT-4o有哪些优秀的特点？</h2>

根据OpenAI的报告，GPT-4o模型现在集成了图像生成功能。这是一项重要的技术进步，但我们也应该保持客观的认识。

技术报告地址：[Introducing 4o Image Generation | OpenAI](https://openai.com/index/introducing-4o-image-generation/)

## 主要特点与功能

GPT-4o的图像生成功能有几个值得注意的特点：

1. **文本渲染能力**：模型能够在图像中准确呈现文本，这解决了之前AI生成图像中文字常常模糊不清的问题。
2. **多轮对话生成**：用户可以通过自然对话方式逐步调整图像，使创作过程更加灵活。
3. **指令遵循能力**：能够处理较为复杂的提示，按照用户要求生成包含多个元素的图像。
4. **上下文学习能力**：能分析用户上传的图片，并将这些元素整合到新生成的图像中。
5. **知识整合**：将文本理解和图像生成能力结合起来，例如能够将代码可视化或创建基于配方的图示。

## 实际局限性

然而，这项技术仍有局限性：

1. **裁剪问题**：长图像容易被过度裁剪，特别是底部部分。
2. **幻觉现象**：在低上下文提示时仍会生成虚构内容。
3. **复杂度限制**：处理超过10-20个概念的复杂图像时容易出错。
4. **非拉丁文字渲染不稳定**：多语言文本渲染存在困难。
5. **编辑精度不足**：精确编辑图像特定部分时可能导致其他部分也发生变化。

## 与专业工具的比较

重要的是，尽管GPT-4o的图像生成功能令人印象深刻，但它**目前无法替代专业的图像生成流程**。专业设计师和艺术家使用的工具(如Photoshop、Illustrator或专门的AI工具如Midjourney、Stable Diffusion)提供了更精细的控制和更高质量的输出。

对于需要精确控制细节、专业品质和特定风格一致性的商业项目，专业工具和工作流程仍然是不可替代的。GPT-4o的图像生成更适合快速原型设计、概念探索或非专业用途。

## 实用价值

GPT-4o的图像生成功能最大的价值在于其便捷性和多模态整合能力。它使普通用户能够在对话中轻松创建视觉内容，而不必切换到专门的图像生成工具。这对于日常交流、简单创意表达和教育用途特别有用。这是AI技术向前迈进的重要一步，但我们应该保持对其能力的客观认识，理解它是对专业图像创作工具的补充，而非替代。在可预见的未来，复杂的设计任务和专业视觉创作仍将需要专门的工具和人类专业知识。


<h2 id="6.Midjourney系列模型迭代至今有哪些优秀的特点？">6.Midjourney系列模型迭代至今有哪些优秀的特点？</h2>

Rocky认为Midjourney系列是AIGC时代AI绘画ToC产品的一个非常有价值的标杆，我们需要持续研究挖掘其价值与优秀特点：

1. 图像生成整体性能持续提升。
2. 图像细节持续提升，包括图像背景、内容层次、整体光影、人物结构、手部特征、皮肤质感、整体构图等。
3. 语义理解持续增强，生成的图像内容与输入提示词更加一致。
4. 文字渲染能力持续提升。
5. 更多辅助功能支持：超分、可控生成、人物一致性、风格参考等。
6. 用户易用性持续提升：用户输入更加简洁的提示词就能生成高质量的图片，更加符合用户的使用习惯。

同时，**Midjourney产品不单单只是一个模型这么简单，其背后是一套完整的AIGC算法解决方案**。

其核心由AIGC图像生成大模型构成，同时配备了一套完整的前处理与后处理算法。

在前处理阶段，将输入的提示词进行扩写润色，使其能够更好的指导图像的生成。

同时，有一个庞大的图像数据库与图像特征库用于和输入的提示词进行匹配，在提取与输入提示词最相似的图像后，作为文生图和图生图的基础图像，能够较好的提升图像生成的整体效果。

在后处理阶段，使用图像增强、图像超分等算法来获取高分辨率和高质量的图像。

<h2 id="7.介绍一下Seedream系列模型的原理，每个版本都有哪些创新？">7.介绍一下Seedream系列模型的原理，每个版本都有哪些创新？</h2>


<h2 id="8.介绍一下HiDream-I1模型的原理">8.介绍一下HiDream-I1模型的原理</h2>


<h2 id="9.介绍一下Nano-Banana-Pro的特点">9.介绍一下Nano Banana Pro的特点</h2>

Nano Banana Pro作为当前全球最强的原生多模态图像生成大模型，有如下的核心特点：

| 特点维度 | 核心提升 | 关键表现 |
| :--- | :--- | :--- |
| **图像质量与控制力** | 支持最高4K分辨率及多种自定义长宽比，提供前所未有的控制精度。 | 作品可直接用于专业印刷或社交媒体，解决了前代模型比例控制不佳的痛点。 |
| **世界知识与逻辑推理** | 深度融合 Gemini 3 Pro 的推理能力和谷歌搜索的实时知识库。 | 能生成符合现实物理逻辑和最新信息的图像，如实时天气信息图、精确的科普图解等。 |
| **文字渲染与多语言支持** | 实现了革命性的精准文本渲染，支持多语言生成和翻译。 | 中文排版清晰准确，可将图像中的文字一键翻译成另一种语言并保持原有版式。 |
| **多图像融合与一致性** | 可一次性融合多达14张参考图像，并保持最多5个人物的高度一致性。 | 极大地简化了漫画创作、系列产品设计、多角色海报制作等需要高度一致性的工作流程。 |
| **对话式编辑与上下文记忆** | 支持多轮对话编辑，能记住之前对话的上下文（如角色、设计风格）并进行精准修改。 | 用户可以通过自然语言指令（如“给主角换件红衣服”）持续优化图像，大幅提升创作效率。 |


总的来说，Nano Banana Pro 的强大之处在于它不仅仅是一个“画图工具”，更是一个具备深度思考能力和广阔世界知识的“视觉创造伙伴”。它通过提升图像质量、增强逻辑推理、解决文字渲染和角色一致性等核心难题，正在将AI图像生成技术推向一个全新的高度。


<h2 id="10.GLM-Image的创新点有哪些？">10.GLM-Image的创新点有哪些？</h2>

### 一、模型架构核心设计思想
1. **GLM-Image模型架构**  
   - 架构组成：自回归模块（Auto-regressive module）+ 扩散解码器（Diffusion decoder）的混合架构。  
   - 参数规模：自回归模块基于GLM-4-9B-0414初始化，含90亿参数；扩散解码器采用单流DiT（Diffusion Transformer）结构，含70亿参数（遵循CogView4设计）。  
   - 核心作用：自回归模块负责生成含低频语义信号的tokens，主导语义理解、布局规划与指令遵循；扩散解码器负责优化高频细节，恢复精细纹理、文本渲染精度与视觉保真度。

2. **混合设计的原因**  
   - 设计原因：解决纯扩散模型在复杂指令遵循、知识密集场景中“语义对齐不足”的短板，同时弥补纯自回归模型在高保真细节生成上的局限，融合两者优势（扩散模型的训练稳定性+自回归模型的语义理解能力）。  
   - 特征分工：自回归模块处理**低频语义信号**（如图像布局、物体关系、文本内容逻辑）；扩散解码器处理**高频细节信号**（如纹理质感、文字笔画精度、光影细节）。

3. **使用语义VQ分词策略的优势**  
   相比传统方案，语义-VQ（semantic-VQ）的核心优势的是：  
   - 语义相关性更强：传统VQVAE的视觉编码侧重重建完整性，但语义关联弱；1D向量（如DALL-E2）信息不完整，难以对应具体图像；而语义-VQ通过离散语义训练，token间相关性更高，更适配视觉生成的语义对齐需求。  
   - 训练收敛更优：相同码本规模下，语义-VQ的训练损失（~3）远低于VQVAE（~7），模型收敛速度更快、稳定性更强。  
   - 实操优化：GLM-Image采用XOmni的tokenizer方案，进一步提升token建模的语义关联性，为扩散解码器提供更可靠的语义基础。

4. **扩散解码器的backbone设计**  
   - backbone架构：单流DiT（Diffusion Transformer）架构（遵循CogView4设计）。  
   - 扩散调度策略：flow matching（流匹配）。  
   - 核心效果：flow matching确保高保真图像生成的训练稳定性，同时提升收敛效率，避免纯扩散模型在高频细节优化中出现的“纹理模糊”“文本畸变”问题。

5. **图像编辑中“细节保留”与“计算开销”的平衡方案**  
   - 技术方案：采用**block-causal attention（块因果注意力）** 机制，而非传统全注意力（如Qwen-Image-Edit）。  
   - 与全注意力的区别：全注意力需对参考图像与生成图像的所有token进行全局关联，计算开销随图像分辨率呈指数增长；block-causal attention通过kvcache（键值缓存）减少参考图像token的计算量，同时保留关键细节的关联性，在“细节保真度”与“计算效率”间实现平衡。
  
6. **自回归与扩散模型的优缺点融合**  
   | 模型类型       | 优点                  | 缺点                  | GLM-Image的融合方式                  |
   |----------------|-----------------------|-----------------------|---------------------------------------|
   | 自回归模型     | 语义理解强、指令遵循好 | 细节保真度低、训练不稳定 | 用其主导语义布局，规避细节生成短板    |
   | 扩散模型       | 细节保真高、训练稳定  | 语义对齐弱、知识密集场景表现差 | 用其优化高频细节，保留训练稳定性优势  |

7. **扩散解码器移除prompt输入**  
   - 移除原因：语义-VQ tokens已从自回归模块获得足够语义信息（如文本内容、布局逻辑），无需额外prompt输入重复提供语义，避免信息冗余。  
   - 核心影响：
     1. 减少计算开销：无需加载大参数文本编码器（如CLIP），降低内存占用与推理延迟；
     2. 提升效率：简化解码器输入流程，聚焦高频细节优化，不分散语义处理精力。

8. **语义-VQ tokens与VAE latent的融合方式**  
   - 融合方式：语义-VQ tokens经投影层转换后，与VAE latent表示沿**通道维度（channel dimension）** 拼接。  
   - 优势：保持输入序列长度不变，几乎无额外计算开销，同时兼顾语义信息（语义-VQ tokens）与底层特征（VAE latent），为细节生成提供完整支撑。

9. **图像编辑中双条件输入**  
   - 语义-VQ tokens：提供**低频语义信息**（如修改后的物体类别、文本内容、整体布局）；  
   - 参考图像VAE latents：提供**高频细节信息**（如参考图像的纹理、颜色、局部结构），确保编辑后不丢失原图像的关键细节。

### 二、训练与优化技术设计
1. **自回归模型部分的预训练策略**  
   - 基础模型：基于GLM-4-9B-0414初始化。  
   - 关键设计：  
     1. 冻结文本词嵌入层，避免预训练语义知识丢失；  
     2. 新增视觉词嵌入层，用于视觉token的投影转换；  
     3. 替换原LM头为视觉LM头，适配图像生成任务；  
     4. 采用MRoPE（Rotary Position Embedding）作为位置编码，适配文本-图像 interleaving（交织）场景（同时支持文本到图像、图像到图像生成）。

2. **渐进式生成策略**  
   - 高分辨率训练前，先生成低分辨率、小数量的token作为布局基础，再逐步优化高分辨率细节的生成策略。  
   - 设计原因：低分辨率阶段（256-token）采用光栅扫描生成时可控性良好，但高分辨率阶段（512px+）直接沿用该策略会导致布局紊乱、可控性下降，需通过“先布局后细节”的渐进式逻辑解决。  
   - 实现逻辑：  
     1. 对目标高分辨率图像下采样，生成256个相同宽高比的token，用于确定核心布局；  
     2. 增加该低分辨率token的训练权重，强化其对最终布局的引导作用；  
     3. 扩散解码器基于低分辨率语义token，逐步生成高分辨率细节。

3. **解耦的强化学习策略**  
   - 核心逻辑：后训练阶段通过“解耦奖励”分别优化自回归模块和扩散解码器两个模块，避免单一优化目标导致的“语义与细节失衡”。  
   - 优化算法：两个模块均采用GRPO（Generalized Policy Optimization）；扩散解码器额外适配flow-GRPO（GRPO的扩散模型变体，适配flow matching调度策略）。  
   - 优化目标差异：  
     - 自回归模块：聚焦**低频奖励**（语义一致性、美学评分），融合HPSv3（美学评分）、OCR（文本渲染精度）、VLM（语义正确性）三大奖励源；  
     - 扩散解码器：聚焦**高频奖励**（细节保真度、文本精度），融合LPIPS（感知纹理相似度）、OCR（文字笔画准确性）、手部专用评分模型（手部生成正确性）。

4. **引入提升中文文本渲染精度的辅助模型**  
   - 引入模型：轻量级Glyph-byT5模型。  
   - 作用机制：对需要渲染的文本区域进行**字符级编码**，生成glyph embeddings（字形嵌入），并与视觉嵌入沿序列维度拼接，强化解码器对中文笔画结构、排版逻辑的理解，解决中文渲染易畸变的痛点。

5. **训练分辨率阶段与token数量、输出分辨率的对应设计**  
   - 训练分辨率阶段：256px、512px、512px-1024px混合分辨率。  
   - token数量：采用XOmni tokenizer（16×压缩比），对应token数分别为256、1024、1024-4096。  
   - 输出分辨率：扩散解码器的上采样因子为32，最终输出分辨率范围为1024px-2048px（如256token→256×16=4096px²→1024px×4096px²/1024px=4096px？修正：原文明确“最终输出分辨率1024px-2048px”，核心对应训练阶段的混合分辨率缩放）。


### 四、核心功能与应用场景
1. **GLM-Image支持的核心任务**  
   - 基础任务：文本到图像生成（Text-to-Image）。  
   - 扩展任务：图像编辑（物体增减、局部修改）、风格迁移、身份保留生成（Identity-preserving generation）、多主体一致性生成（Multi-subject consistency）。

2. **知识密集型场景的应用价值**  
   文档案例（垃圾分类口诀、手冲咖啡公式、活字印刷步骤、水循环原理）体现核心价值：  
   - 精准表达复杂信息：支持多段文本、步骤化内容、专业公式的可视化（如1:15咖啡粉水比、活字印刷“刻字-烧制-排版”流程）；  
   - 语义逻辑不丢失：确保知识的准确性（如牛顿三棱镜实验中“紫光拐弯最大”的物理规律）、步骤的连贯性（如蛋糕制作的“打发-翻拌-冷藏”顺序）；  
   - 多模态融合：实现文本、图表、场景的一体化生成（如Citywalk地图+文字标注、生物发光鱼的属性说明）。

3. **精准图像编辑的功能与技术关键**  
   - 支持操作：物体增减、风格迁移、局部细节修改（如文字替换、背景调整）。  
   - 技术关键：  
     1. 双条件输入：扩散解码器同时接收语义-VQ tokens（语义逻辑）和参考图像VAE latents（高频细节），确保“修改不丢原细节”；  
     2. block-causal attention：通过kvcache减少计算开销，同时保留参考图像的关键细节（如纹理、物体轮廓）。

4. **多主体一致性生成的应用场景**  
   - 应用场景：群体人像生成、多物体协同场景（如“3个不同姿势的宇航员在月球表面”）、系列化内容创作（如同一品牌的不同产品图）。  
   - 技术保障：语义-VQ分词策略确保多主体的语义关联一致性，扩散解码器的细节优化确保同一主体的特征（如颜色、形状）不畸变，同时通过多分辨率训练强化全局协调性。


<h2 id="11.Qwen-Image系列的创新点有哪些？">11.Qwen-Image系列的创新点有哪些？</h2>


<h2 id="12.Z-Image系列的创新点有哪些？">12.Z-Image系列的创新点有哪些？</h2>


---
